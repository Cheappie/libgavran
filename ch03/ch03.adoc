== At the pages layer, building transactions

We have gotten to the point where we can read and write data to a file. It will take a while to understand why we didn't simply call `write` or `read` directly and call
it a day. We'll discuss that in detail when we implement transactions. Right now, I want to focus on how we are going to lay out the data on the file. 

Files are typically thought of as sequence of bytes. Consider the text file shown below. It shows a simple text file consisting of several lines. 

----
One
Three
Four
----

If I want to add `Two` to the file in the right location, what do I have to do? There is no easy way to add data in the middle of the file. I have to write out
what I want and then continue writing to the end of the file (extending it along the way). There actually _are_ ways to extend a file in such a manner, in turns
out. It isn't very useful for most scenarios, but I want to discuss that a bit so you'll understand how that works.

You can call `fallocate` with `FALLOC_FL_INSERT_RANGE` to insert a range of blocks in the middle of a file. That works because while the file system present
you with the illusion of a sequence of bytes, the reality is very different. Take a look at Figure 3, which shows the physical outline of the text file above.

.Physical layout of a file on disk
image::{img-src}/img01.png[]

Any file that is smaller than 512 bytes will fit on a single block one block (sometimes called sector). A block is 512B - 4KB in size, in most cases and you can
assume that it will be 4KB on pretty much any modern disk. And by modern, I'm talking anything anything made in the past _decade_. 

Pretend that the file is actually large enough to reside on multiple blogs. You can use the `filefrag` command to view the blocks on the file. This is _exactly_ 
what happens when we talk about fragmentation of files. The physical layout of the file is made up of blocks, which reside on a block device (the disk drive).
It is the job of the file system to map those blocks into a file and present us with the file stream abstraction. 

With `FALLOC_FL_INSERT_RANGE`, we can tell the file system that we want to add new blocks to the file, somewhere in the middle. That is a cheap operation, because
we aren't actually moving anything. We simply change the list of blocks that are assigned to the file and write new data to the block. You can see how that looks
on Figure 4.

.A file's physical layout after adding a block in the middle
image::{img-src}/img3-2.png[]

So we can add data cheaply in the middle of a file, although it requires somewhat unusual API calls. The problem is that we can do that only on block boundary. On 
most modern systems, that is 4KB in size. We can also only insert data in increments of 4KB, so this approach isn't generally usable.

At the same time, we are going to think about how we are going to manage the data we put into storage. We need to consider how we read and write data. As it 
turns out, there are quite a few common methods to handle this scenario:

* Append only - we always write at the end of the file, and the file format ensures that new values are found before old values. This has the benefit of being 
  simple to start with, since there is very little to manage, but you'll very quickly end up with most of your space being used by values that has been 
  overwritten. Getting rid of these old values require you to _compact_ the file, which may take twice as much space as the original file take and introduce
  a _lot_ of load on the I/O system.
* Fixed size records - in this model, we define the size of a record upfront (64 bytes, for example) and then we can treat the file as an array of those records.
  This is how many applications stored their data and it is a very simple method that is surprisingly powerful. It has the downside, of course, that you are 
  forced to pick a set size and use it for the life time of the data.
* Page model - the file is divided into pages (4KB - 4MB) in size that are treated as independent buffers. We are always working on a the page level, which is
  a buffer that is read and written to in an atomic fashion. In other words, we replicate exactly how the file system think of the world. This is similar to the 
  fixed size records option, but instead of storing fixed size records, we have fixed size pages and we are free to manage their internal structure as we see
  fit. The page size is also large enough that we don't usually need to 

=== Getting to grips with paging

For our storage engine, we are going to use the paging model. That allows the most flexibility and is the most common choice for storage engines. I'm not going to
go too deeply into the design choices, you might want to refer the https://www.databass.dev/[Database Internals] book for details. In particular, the terms you
are looking for are Log Structure Merge vs Page Structure. 

[TIP]
.Pages are a way to work with the data inside the storage engine
====
Our storage engine is going to divide the file into pages that are 8KB in side. Instead of thinking about the whole of a file as a stream of bytes, we are going
to cut it into pages and work on each one of them independently. This is a very common decision for databases since there are many benefits for this approach.
Modifying a single page solve the need to insert data in the middle of a file, we can overwrite the page as a whole. 

Pages are to a database as bytes are to a file. The basic building blocks, but they aren't sufficient. In order to actually make _sense_ of the system, we have
to layer additional features and data structures. I'm building Govran from the ground up and explaining each section independently. It all come together and
then we'll have a beautiful picture and gorgeous architecture.
====

The next question to ask is what will be the page size we'll select. The page size is of great important for the storage engine. That is the atomic unit on which
all operations are made. The page size must be a multiple of the file system block size. In practice, that means that it should be a multiple of 4KB these days. 
When building Voron (RavenDB's storage engine) we have run a whole bunch of benchmarks and the sweet spot for our needs was a page size that was 8KB. I'm going
to use that value again and maybe we'll play with the size when we get to writing benchmark code (which is still very far away).

The API that we wrote so far isn't really suitable for working with pages. That is intentional, we are now building another layer in the storage engine. On top
of our PAL code (which deals with files) we now have a paging layer. I'm going to need to jump ahead a little bit and declare a few things that will not make 
_any_ sense now. I'm going to define types for database and a transaction. You can see them in Listing 3.1.

[source]
.Listing 3.1 - db.h - The declarations of db_t and txn_t
----
include::./code/db.h[]
----

The reason that I'm defining them now is that there is going to be a strong association between the paging layer, which manages the pages on the file and the 
transaction layer. In fact, the only _way_ for us to work with pages at all is in the context of a transaction. In the hope of avoiding duplicate work, I'm 
going to forward declare these and move on. We'll flesh these down the line.

Listing 3.2 shows the API we'll have for dealing with pages.

[source]
.Listing 3.2 - paging.h - The initial paging API
----
include::./code/paging.h[]
----

In Listing 3.2, we define the `PAGE_SIZE` constant and the `page_t` struct. There isn't much there at this point, we simply have the page number and
the mapped address for the page. We can use that to get a particular page or to write a set of pages. Let's explore how we can use this API to implement the 
same read & write operation as we did in Listing 2.11. 
One caveat we have to take into account is that we _can't_ modify the result of the `pages_get` directly. This is because the memory is mapped as readonly. Attempts
to write to it will result in a segmentation fault. In order to deal with that, we'll copy the page to our own buffer and use the `pages_write` to update its content.

[TIP]
.Always work _with_ the hardware
====
One additional factor for using pages as the manner in which we manage the data in the file is that this means that all our I/O is now going to be page based.
This in turn means that drives such as SSD or NVMe are going to have easier time, because we are never going to issue a read or a write the crosses a page
boundary. This also tend to allow you to get better lifetime from your hardware, since you are reduce the amount of work it needs to do.
Writing on page boundary has been shown to increase the longevity of the hard drives and can also allow the disk to optimize the data access better.
====

We are working with pages, so we want to use page aligned memory, which usually means that we want to make use of `posix_memalign`. That function, however, has awkward
usage within our API, so we'll wrap it in `txn_allocate_page`, shown on Listing 3.3.

[source]
.Listing 3.3 - mem.c - Allocating memory in chunks of pages
----
include::./code/mem.c[tags=palmem_allocate_pages]
----

The `txn_allocate_page` is pretty bare boned, but that is sufficient for now. With that at hand, let's see how we can make use of the API to read and write from the file.

[source]
.Listing 3.3 - main.c - Reading and writing using the paging API
----
include::./code/main.c[]
----
<1> Changes from Listing 2.11 start here.
<2> We ask to get a (read only) page 0.
<3> We allocate a (mutable) page from memory (unrelated to the file).
<4> We copy the data from page 0 to the mutable page.
<5> We copy a string to mutable page.
<6> We write the modified page back to the file.
<7> Using memory mapped I/O, we read the values we just wrote to the file.

We get the page, copy it to our own buffer, modify our own copy and then write it back. This technique is called Copy On Write and it has some highly desirable
properties. For example, until I call `pages_write`, there has been no change to the file. That means that I can abort an operation midway through, free the 
copy of the memory I used and move on without need to write compensation logic to restore things to the way it was.

Listing 3.3 also present us with another problem. I'm likely not going to be able to keep track of everything in a system that need to modify more than a single page. 
We need some way to manage pages in a more seamless manner, to handle the Copy On Write, the writes of all the modified pages back to the file and to free all that 
memory.

For that matter, what happens if I need to modify the same page twice? Am I going to have multiple copies? What will end up in the file in the end?
All of that leads me to realize that we can't remain at the level of working with a single page, we need some higher scope to work with here. We need to implment
the concept of a database and a transaction.

=== A skeleton database

We aren't really ready for real transactions, because it will take time until it actually have the appropriate transactional properties, but that is the right term
for where we will be.

A transaction is a short lived object that is meant to do a specific operation. This is important because we don't expect a transaction to last very long or be very large.
I'm sure we'll get some of those, but we are going to optimize for small & short transactions. Long or big transactions will work, but they aren't our optmization traget.

Look at Listing 3.4, which shows the API we are going to expose for our users. 

[source]
.Listing 3.4 - db.h - Transaction API declaration for our storage engine
----
include::./code/db.h
----

The idea in Listing 3.4 is that we want to do _everything_ within the scope of a transaction. Note that if you want to get or modify a page, you must provide a 
valid transaction to do so. The functions are also named `txn_low_get_page`, the `low` part of the name indicates that they are low level interface. I don't 
intend for our users to _ever_ work at that level. These functions will be moved to an internal header and not exposed as we provide more features to callers. 

We now have a `txn_t` struct that we can use to represent the transaction, but the actual implementation is hidden away in the `txn_state_t`. We'll shortly 
see why that is a _very_ useful approach for what we want to do. We also introduced the concept of a `db_t`, which will hold the `file_handle_t` and any other 
resources that will be required.

[CAUTION]
.Paying attention to the man behind the screen
==== 
C is a great low level language, but I'm really feeling the lack of data types. I want to make sure that everything that goes into the engine 
is covered here, but I'm not sure how useful it will be to have you go through  a hash table implementation, for example.
 
I'm assuming that if you are reading this book, you probably are familiar with this enough so that should be of no interest to you. For now,
I want to keep putting all the code here, but I think that when I'll start using code beyond the standard library, I'm going to just point
you to that code and not put it here.
====

As you can imagine, the transaction API is going to be of enormous importance for our storage engine and as such, so we'll spend some time fleshing out the API.
This is where we start to put together the skeleton of Gavran. Let's take a look at the first stage, creating a database, shown in Listing 3.5.

[source]
.Listing 3.5 - db.c - Creating an database instance
----
include::./code/db.c[]
----

A `db_t` is a simple structure that holds a single opaque pointer. I'm using the same approach in `txn_t` as well and it means that I have both encapsulation as well
as a way to modify the state of the object without needing to change the value that holds it. We'll see how useful that is when we get to building transactions.

The `struct database_state` itself isn't very big at the moment. It contains the database options (currently only the minimum size of the database), the file handle
and the memory map result. As times goes by, we'll see more and more data being managed at the database level, but it is important to start with the proper structure
now, to avoid extra work later.

We check and validate the options, then we use our existing PAL file system API to create a file, set it to the right size and map it. With the use of `ensure` and
`try_defer`, the entire thing reads as if we had no error handling. Note that we use multiple `try_defer` statements to setup resource cleanup in the event of a 
failure. They are guaranteed to run in reverse order of definition.

In the next chapter, we are going to focus on `handle_newly_opened_database` and do a lot of work there, but for now, I want to focus on transactions.

=== Starting to implement transactions.

You already saw the external API to create a transaction in Listing 3.4, now we want to actually build it. Before we can do that, I want to go over what transactions _are_.
A transaction in the context of Gavran is a set of changes that are applied to the database. Until they are applied, they are not visible to any other transactions.

Before we dive into the details of how a transaction is implemented in code, I want to talk about what we are trying to _do_. The attachments that we'll build in this
chapter is mostly concerned with managing a list of modified pages. The idea is that you'll always go through the transaction to modify pages, it will manage them for
you and eventually commit them.

The overall structure can be seen in Figure 4. 

.The layout of a transaction with 2 modified pages
images::{img-src}/img3-3.png

Using the Copy On Write mode tha we discussed earlier, whenever we want to modify a page, we'll copy it to the side and make the modifications. Once all the changes 
in the transaction are completed, we'll persist them all at once. Consider the implications of such aa system. How would a rollback look in this case? 
Simply free the copies, no one else had any access to them, so nothing needs to be done. 

As for commit, once all the changes has been written to the file, we are done. We saw in the previous chapter that it takes significant effort to make sure that a
change to the file metadata is persisted. Changes to the file's data are by no means easier. We are going to deal with a single facet of the ACID properties in
this chapter, and atomic commit. 


.A note about multi threading
****
We don't have any. 

Right now, the code isn't going to be able to handle multiple threads.
The atomic property that we'll have is about sequential or interleaved transactions.
Gavran _will_ have all of the ACID properties, including safe usage under multi threaded code, mind you. 
****

Listing 3.6 shows the structure of a transaction as well as its initialization.

[source]
.Listing 3.6 - txn.c - Creating a transaction
----
include::./code/txn.c[tags=transaction_state]

include::./code/txn.c[tags=txn_create]
----


A `txn_t` is just a pointer to `txn_state_t`, which holds the details about the transactions as well as the start of a hash table to hold
the modified pages. We use the empty `entries` field to point past the end of the struct to the actual data. I'm doing it in this manner because
it means that I have to do less allocations, which greatly simplify the code.

You'll note that we are passing a `flags` parameter to the function, but not doing much of it. I'm currently ignoring that but we'll use that to
set various behaviors on the transaction, such as whatever it is read only or allowed to write.
In `txn_create`, we allocate enough space for the transaction state as well as 8 hash table entries. We'll see how they are used shortly.

[CAUTION]
.I gotta implement a hash table, argh!
====
I have to admit, given that I write databases for a living, I was highly amused by how reluctant I was to implement a hash table. I have wrote
a few, so it isn't a new task. I'm also doing that while implementing a _storage engine_, which is closely related. 

The good news here is that this hash table only need to support `put` and `get`, not `remove`. That makes the overall complexity much easier.

Given that the readers of this book are likely interested, I'm going to go for an open addressing with linear probing model. And one of the 
key features that I care about is avoiding the usual pointer caching that you'll typically see in hash tables. That is why the actual 
`entries` buffer is placed after the `txn_state_t`. Data locality. 
====

One of the key responsibilities of the transaction is managing the work of `txn_modify_page`, to handle that properly, we need to store the modified
pages in the transaction and be able to access them quickly.  Listing 3.7 has the details on how we manage that.

[source]
.Listing 3.7 - Modifying a page inside a transaction
----
include::./code/txn.c[tags=txn_modify_page]
----

The `txn_modify_page` function in Listing 3.7 will first check if we already asked to modify the page in this transaction. If so, that copy of 
the page will be returned to us. If not, we'll check that the page number if valid, allocate a copy of the page, copy the data to it and
then register it in the hash table. 

This is called Copy On Write (COW) and it will serve as the basis for implementing one of the core ACID properties, Isolation.

.Memory alignment
====
In Listing 3.5 you can see that I'm using `palmem_allocate_pages` function, that make use of `posix_memaligned` under the covers. I'm asking the system
to give me 8KB buffer with 8KB alignment.

This is a match to how the operating system manage pages in memory (on 4KB alignment). It means that for the rest of my system, I can safely assume that 
the page buffer is properly align for anything that I want to do. This is important for performance and correctness reasons, especially outside of the x64/x86 world.
====

=== Managing the transaction state with a hash table

The real work in `txn_modify_page` is inside the `lookup_entry_in_tx` and `allocate_entry_in_tx`. Let's see how these work, we'll start with
the `lookup_entry_in_tx` in Listing 3.8, where we are searching through the hash table for a match.

[source]
.Listing 3.8 - txn.c -Looking up a page in the transaction's hash table
----
include::./code/txn.c[tags=lookup_entry_in_tx]
----

I'm going to skip explaining https://en.wikipedia.org/wiki/Hash_table[how hash tables work], I assume that you either have a grasp on that
or is able to learn that from other sources. I'll mention that I'm using a hash table implementation strategy called 
https://en.wikipedia.org/wiki/Linear_probing[linear probing]. This refers to how the hash table handles collisions. Instead of creating a 
linked list of values on hash collision, we'll move to the next location in the array. That means that we are going to have good locality
of reference when doing the search, the entire hash table is typically a single allocation and the `get` and `put` implementation are
straightforward.

The downside of linear probing and open addressing in general is that the deletes tend to be far more complex to implement than an 
implementation that uses chaining. The good news is that we don't need to worry about that here, so the complexity is greatly reduced.

There are a few other things to consider for this case:

* The key for the hash is the page number, these are `uint64_t` values and it is very likely that you'll get consecutive values.
  As such, we don't try to be fancy with the hash function, we simply modulus the page number with the number of buckets in the 
  hash table and start the search there.
* The rule on collision goes like this. Whenever a put is attempted that collided, we'll move to the next available spot that 
  is free. In other words, the worst case scenario for this sort of hash table is having to scan through all the buckets. 
* This is where the load factor comes into play. Given that the page numbers are likely to cluster around specific values, the
  empty buckets that remain act as backstop to avoid iterating through all the buckets. 
* A `page_hash_entry_t` is 16 bytes in size of x64, which means that we can fit 4 of them on a cache line. A sequential reading
  through the buckets is likely to generate highly predictable access pattern which can be optimized by the compiler and CPU
  significantly. 

Now that we know how to get an entry from the hash table, let's look at the other side, how can we put one in? This is the more complex
operation because we may need to resize the hash table if the load factor grows big enough. The details are in Listing 3.9.

[source]
.Listing 3.9 - txn.c - Adding an entry to the hash table, potentially resizing it
----
----