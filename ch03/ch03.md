
# The paging layer

We have gotten to the point where we can read and write data to a file. It will take a while to understand why we didn't simply call `write` or `read` directly and call
it a day. We'll discuss that in detail when we implement transactions. Right now, I want to focus on how we are going to lay out the data on the file. 

Files are typically thought of as sequence of bytes. Consider the text file shown in Listing 3.1. It shows a simple text file consisting of several lines. 

```{caption="A multi line text file" .txt}
One
Three
Four
```

If I want to add `Two` to the file in the right location, what do I have to do? There is easy way to add data in the middle of the file. I have to write out
what I want and then continue writing to the end of the file (extending it along the way). There actually _are_ ways to extend a file in such a manner, in turns
out. It isn't very useful for most scenarios, but I want to discuss that a bit so you'll understand how that works.

You can call `fallocate` with `FALLOC_FL_INSERT_RANGE` to insert a range of blocks in the middle of a file. That works because while the file system present
you with the illusion of a sequence of bytes, the reality is very different. Take a look at Figure 3.1, which shows the phyical outline of the file in Listing 3.1

![Physical layout of a file on disk](./ch03/img01.png)

If you would look at an actual file with the size shown on Listing 3.1, you'll see that it is small enough that it all fits in one block (sometimes called sector).
Pretend that the file is actually large enough (typically, if it is greater than 16 KB, that should do it) to reside on multiple blogs. You can use the `filefrag`
command to view the blocks on the file. This is _exactly_ what happens when we talk about fragmentation of files. The physical layout of the file is made up of 
blocks, which reside on a block device (the disk drive). It is the job of the file system to map those blocks into a file and present us with the file stream 
abstraction. 

With `FALLOC_FL_INSERT_RANGE`, we can tell the file system that we want to add new blocks to the file, somewhere in the middle. That is a cheap operation, because
we aren't actually moving anything. We simply change the list of blocks that are assigned to the file and write new data to the block. You can see how that looks
on Figure 3.2.

![File physical layout after adding a block in the middle](./ch03/img02.png)

We can add data cheaply in the middle of a file, although it requires somewhat unsual API calls. The problem is that we can do that only on block boundary. On 
most modern systems, that is 4KB in size. We can also only insert data in increments of 4KB, so this approach isn't generally usable.

At the same time, we are going to think about how we are going to manage the data we put into storage. We need to consider how we write and read data. As it 
turns out, there are quite a few common methods to handle this scenario:

* Append only - we always write at the end of the file, and the file format ensures that new vlaues are found before old values. This has the benefit of being 
  simple to start with, since there is very little to manage, but you'll very quickly end up with most of your space being used by values that has been 
  overwritten. Getting rid of these old values require you to _compact_ the file, which may take twice as much space as the original file take and introduce
  a _lot_ of load on the I/O system.
* Fixed size records - in this model, we define the size of a record upfront (64 bytes, for example) and then we can treat the file as an array of those records.
  This is how many applications stored their data and it is a very simple method that is surprisingly powerful. It has the downside, of course, that you are 
  forced to pick a set size and use it for the life time of the data.
* Page model - the file is divided into pages (4KB - 4MB) in size that are treated as independent buffers. We are always working on a the page level, which is
  a buffer that is read and written to in an atomic fashion. In other words, we replicate exactly how the file system think of the world. This is simiar to the 
  fixed size records option, but instead of storing fixed size records, we have fixed size pages and we are free to manage their internal structure as we see
  fit.

## Getting to grips with paging

For our storage engine, we are going to use the paging model. That allows the most flexiblity and is the most common choice for storage engines. I'm not going to
go too deeply into the design choices, you might want to refer the Database Internals^[https://www.databass.dev/] book for details. In particular, the terms you
are looking for are Log Structure Merge vs Page Structure. 

The next question to ask is what will be the page size we'll select. The page size is of great important for the storage engine. That is the atomic unit on which
all operations are made. The page size must be a multiple of the file system block size. In practice, that means that it should be a multiple of 4KB these days. 
When building Voron (RavenDB's storage engine) we have run a whole bunch of benchmarks and the sweet spot for our needs was a page size that was 8KB. I'm going
to use that value again and maybe we'll play with the size when we get to writing benchmark code (which is still very far away).

The API that we wrote so far isn't really suitable for working with pages. That is intentional, we are now building another layer in the storage engine. On top
of our PAL code (which deals with files) we now have a paging layer. Listing 3.2 shows the API we'll deal with.

```{caption="Low level paging API declarations" .c}
#define PAGE_SIZE 8192
#define PAGE_ALIGNMENT 4096

_Static_assert(PAGE_SIZE && PAGE_SIZE % PAGE_ALIGNMENT == 0, 
	"PAGE_SIZE must be of a multiple of PAGE_ALIGNMENT");

typedef struct page {
	void* address;
	uint64_t page_num;
} page_t;

MUST_CHECK bool get_page(void* base_addr, 
	page_t* page);

MUST_CHECK bool write_pages(file_handle_t* handle,
	page_t* pages, size_t num_of_pages);
```

In Listing 3.2, we define the `PAGE_SIZE` constant and the `page_t` struct. There isn't much there at this point, right now, we simply have the page number and
the mapped address for the page. We can use that to get a particular page or to write a set of pages. Let's explore how we can use this API to implement the 
same read & write operation as we did in Listing 2.12. Because Listing 2.12 is fairly long, Listing 3.2 will start immediately after we map the file. 

One caveat we have to take into account is that we _can't_ modify the result of the `get_page` directly. This is because the memory is mapped as readonly. Attempts
to write to it will result in a segmentation fault. That means that Listing 3.3 is _much_ longer than the similar code in 2.12.

```{caption="Writing and reading from a db file using paging API" .c}
page_t page;
page.page_num = 0;
if(!get_page(addr, &page)){
  print_all_errors();
  return EIO;
}

void* modified;
if(!posix_memalign(&modified, PAGE_SIZE, PAGE_SIZE)){
  push_error(ENOMEM, "Cannot allocate memory for page");
  print_all_errors();
  return ENOMEM;
}
defer(free_p, modified);

memcpy(modified, page.address, PAGE_SIZE);

const char msg[] = "Hello Gavran";
memcpy(modified, msg, sizeof(msg));

if(!write_pages(handle, &page, 1)){
  print_all_errors();
  return EIO;
}

printf("%s\n", page.address);
```

We get the page, copy it to our own buffer, modify our own copy and then write it back. This technique is called copy on write and it has some highly desriable
properties. For example, until I call `write_pages`, there has been no change to the file. That means that I can abort an operation midway through, free the 
copy of the memory I used and move on without need to write compesnation logic to restore things to the way it was.

Listing 3.3 also present us with another problem. Where are we going to write the data? Right now, I'm using the first page, but obviously that isn't something
that we can really do for long. We need some way to manage pages. The following basic operations are required:

* `modify_page` - which will give us a _clone_ of the data in the page, so we won't have to write the copy code all the time.
* `allocate_page` - which will allocate a new page for our needs.
* `free_page` - mark that page as free (and avaiable for allocations).

We'll start with `modify_page`, because that present a significant challange. In particular, who is going to be the owner of this memory, and how are we going
to be able to work with it? If I call `modify_page` on the same page twice, I don't want to have another copy, for example. That all leads us to the realization
that we need a scope. 

## Starting to implement transactions

In databases, such a scope is typically called a transaction. I hesitate to call it so yet, because it will take time until it actually have the appropriate 
transactional properties, but that is the right term. A transaction is a short lived object that is meant to do a specific operation. This is important because
we don't expect a transaction to last very long or be very large. I'm sure we'll get some of those, but we are going to optimize for small & short transactions.
We'll still make sure that long and big transactions will work, of course.

Listing 3.4 show the new API that we'll work to implement in this chapter.

```{caption="Transaction API declaration for our storage engine" .c}
#define PAGE_SIZE 8192
#define PAGE_ALIGNMENT 4096

_Static_assert(PAGE_SIZE && PAGE_SIZE % PAGE_ALIGNMENT == 0, 
	"PAGE_SIZE must be of a multiple of PAGE_ALIGNMENT");


typedef struct page{
    uint64_t page_num;
	void* address;
} page_t;

typedef struct transaction_state txn_state_t;

typedef struct transaction {
    txn_state_t* state;
} txn_t;

MUST_CHECK bool create_transaction(file_handle_t* handle, 
		uint32_t flags, txn_t* tx);
MUST_CHECK bool commit_transaction(txn_t* tx);
MUST_CHECK bool close_transaction(txn_t* tx);

MUST_CHECK bool get_page(txn_t* tx, page_t* page);
MUST_CHECK bool modify_page(txn_t* tx, page_t* page);
MUST_CHECK bool allocate_page(txn_t tx, page_t* page);
MUST_CHECK bool free_page(txn_t* tx, page_t* page);

```

The idea in Listing 3.4 is that we want to do _everything_ within the scope of a transaction. Note also that we now have a `txn_t` struct that we can 
use to represent the transaction, but the actual implementation is hidden away in the `txn_state_t`. We'll shortly see why that is a _very_ useful
approach for what we want to do.

> **Pay attention to the man behind the screen**
> 
> C is a great low level language, but I'm really feeling the lack of data types. I want to make sure that everything that goes into the engine 
> is covered here, but I'm not sure how useful it will be to have you go through  a hash table implementation, for example.
> 
> I'm assuming that if you are reading this book, you probably are familiar with this enough so that should be of no interest to you. For now,
> I want to keep putting all the code here, but I think that when I'll start using code beyond the standard library, I'm going to just point
> you to that code and not put it here.

This API also require us to supply quite a bit of functionality in the transaction and that will only grow. In Listing 3.5 you can see
how the transaction is created. 

```{caption="Creating a new transaction" .c}
typedef struct page_hash_entry {
    uint64_t page_num;
    void* address;
} page_hash_entry_t;

struct transaction_state {
    file_handle_t* handle;
    void* address;
    size_t allocated_size;
    uint32_t flags;
    uint32_t modified_pages;
    page_hash_entry_t entries[];
}

bool create_transaction(file_handle_t* handle, uint32_t flags, txn_t* tx){
    assert_no_existing_errors();
    
    uint64_t size;
    if(!get_file_size(handle, &size)){
        mark_error();
        return false;
    }

    void* addr;
    if(!map_file(handle, 0, size, &addr)){
        mark_error();
        return false;
    }

    size_t initial_size = size(transaction_state_t) + sizeof(page_hash_entry_t) * 8;
    transaction_state_t* state = calloc(initial_size);
    if (!state){
       if(!unmap_file(addr, size))){
           mark_error();
       }
       push_error(ENOMEM, "Unable to allocate memory for transaction state");
       return false;
    }
    memset(state, 0, initial_size);
    state->allocated_size = initial_size;
    state->handle = handle;
    state->flags = flags;
    state->address = addr;

    tx->state = state;
    return true;
}
```

A `txn_t` is just a pointer to `transaction_state_t`, which holds the details about the transactions as well as the start of a hash table to hold
the modified pages. We use the empty `entries` field to point past the end of the struct to the actual data. I'm doing it in this manner because
it means that I have to do less allocations, which greatly simplify the code.

You'll note that we are passing a `flags` parameter to the function, but not doing much of it. I'm currently ignoring that but we'll use that to
set various behaviors on the transaction, such as whatever it is read only or allowed to write.

In `create_transaction`, we map the file (and that is probably not something that we should do on a per transaction basis). This is a good indication
that we need to also have a higher level scope here, that will outlive a tranasction. After mapping the file, we allocate enough space for the 
transaction state as well as 8 hash table entries. We'll see how they are used shortly.

> **I gotta implement a hash table, argh!**
>
> I have to admit, given that I write databases for a living, I was highly amused by how reluctant I was to implement a hash table. I have wrote
> a few, so it isn't a new task. I'm also doing that while implementing a _storage engine_, which is closely related. 
>
> The good news here is that this hash table only need to support `put` and `get`, not `remove`. That makes the overall complexity much easier.
>
> Given that the readers of this book are likely interested, I'm going to go for an open addressing with linear probing model. And one of the 
> key features that I care about is avoiding the usual pointer caching that you'll typically see in hash tables. That is why the actual 
> `entries` buffer is placed after the `txn_state_t`.

One of the key responsabilities of the transaction is managing the work of `modify_page`, to handle that properly, we need to store the modified
pages in the transaction. That is the reason for the extra space that we allocate for the transaction. Listing 3.6 has the details on how we 
manage that.

```{caption="The modify\\_page function will create a new copy of a page or reuse an existing one" .c}
bool modify_page(txn_t* tx, page_t* page) {
    assert_no_existing_errors();

    page_hash_entry_t* entry;
    if(lookup_entry_in_tx(tx->state, 
    		page->page_num, &entry)) {
        page->address = entry->address;
        return true;
    }

    uint64_t offset = page->page_num * PAGE_SIZE;
    if(offset + PAGE_SIZE > tx->state->file_size){
        push_error(ERANGE, 
        	"Requests page %lu is outside of "
        	"the bounds of the file (%lu), 
            page->page_num, tx->state->file_size);
        return false;
    }
    void* original = ((char*)tx->state->address + offset);
    void* modified;
    int rc = posix_memalign(&modified, PAGE_SIZE, 
    		PAGE_SIZE);
    if (rc){
        push_error(rc, 
        	"Unable to allocate memory for a COW page %lu", 
        	page->page_num);
        return false;
    }
    memcpy(modified, original, PAGE_SIZE);
    if(!allocate_entry_in_tx(&tx->state, page->page_num, 
    		&entry)){
        mark_error();
        free(modified);
        return false;
    }
    entry->address = modified;
    page->address = modified;
    return true;
}
```

The `modify_page` function in Listing 3.6 will first check if we already asked to modify the page in this transaction. If so, that copy of 
the page will be returned to us. If not, we'll check that the page number if valid, allocate a copy of the page, copy the data to it and
then register it in the hash table. 
This is called Copy On Write (COW) and it will serve as the basis for implementing one of the core ACID properties, Isolation.

> **Memory alignment**
>
> In Listing 3.5 you can see that I'm using `posix_memalign` method. I'm asking the system to give me 8KB buffer with 4KB alignment.
> This is a match to how the operating system manage pages in memory (on 4KB alignment). It means that for the rest of my system, 
> I can safely assume that the page buffer is properly align for anything that I want to do. This is imporant for performance
> and correctness reasons, especially outside of the x64/x86 world.

## Managing the transaction state with a hash table

The real work in `modify_page` is inside the `lookup_entry_in_tx` and `allocate_entry_in_tx`. Let's see how these work, we'll start with
the `lookup_entry_in_tx` in Listing 3.7, where we are searching through the hash table for a match.

```{caption="Searching the hash table for a match for the requested page" .c}
#define get_number_of_buckets(state) (\
		(state->allocated_size - sizeof(txn_state_t))\
		 / sizeof(page_hash_entry_t))

static bool lookup_entry_in_tx(txn_state_t* state, 
		uint64_t page_num, page_hash_entry_t** entry){

    size_t number_of_buckets = get_number_of_buckets(state);
    size_t starting_pos = (size_t)(page_num % number_of_buckets);
    // we use linear probing to find a value in case of collisions
    for(size_t i = 0; i < number_of_buckets; i++){
        size_t index = (i + starting_pos) % number_of_buckets;
        if(!state->entries[index].address){
            // empty value, so there is no match
            return false;
        }
        if(state->entries[index].page_num == page_num){
            *entry = &state->entries[index];
            return true;
        }
    }
    return false;
}
```

I'm going to skip explaining how hash tables work^[https://en.wikipedia.org/wiki/Hash_table], I assume that you either have a grasp on that
or is able to learn that from other sources. I'll mention that I'm using a hash table imlementation strategy called 
linear probing^[https://en.wikipedia.org/wiki/Linear_probing]. This refers to how the hash table handles collisions. Instead of creating a 
linked list of values on hash collision, we'll move to the next location in the array. That means that we are going to have good locality
of reference when doing the search, the entire hash table is typically a single allocation and the `get` and `put` implementation are
straightforward.

The downside of linear pribing and open addressing in general is that they deletes tend to be far more complex to implement than an 
implementation that uses chaining. The good news is that we don't need to worry about that here, so the complexity is greatly reduced.

There are a few other things to consider for this case:

* The key for the hash is the page number, these are `uint64_t` values and it is very likely that you'll get consecutive values.
  As such, we don't try to be fancy with the hash function, we simply moduls the page number with the number of buckets in the 
  hash table and start the search there.
* The rule on collision goes like this. Whenever a put is attempted that colided, we'll move to the next available spot that 
  is free. In other words, the worst case scenario for this sort of hash table is having to scan through all the buckets. 
* This is where the load factor comes into play. Given that the page numbers are likely to cluster around specific values, the
  empty buckets that remain act as backstop to avoid iterating through all the buckets. 
* A `page_hash_entry_t` is 16 bytes in size of x64, which means that we can fit 4 of them on a cache line. A sequential reading
  through the buckets is likely to generate highly predictable access pattern which can be optimized by the compiler and CPU
  significantly. 

Now that we know how to get an entry from the hash table, let's look at the other side, how can we put one in? This is the more complex
operation because we may need to resize the hash table if the load factor grows big enough. The details are in Listing 3.8.

```{caption="Adding an entry to the hash table, potentially resizing it" .c}
static bool allocate_entry_in_tx(txn_state_t** state_ptr,
		 uint64_t page_num, page_hash_entry_t** entry){

    txn_state_t* state = *state_ptr;
    size_t number_of_buckets = get_number_of_buckets(state);
    size_t starting_pos = (size_t)(page_num % number_of_buckets);
    // we use linear probing to find a value in case of collisions
    for(size_t i = 0; i < number_of_buckets; i++){
        size_t index = (i + starting_pos) % number_of_buckets;
        if(state->entries[index].page_num == page_num){
            push_error(EINVAL, "Attempted to allocate entry for "
            	"page %lu which already exist in the table", page_num);
            return false;
        }

        if(!state->entries[index].address){
            size_t max_pages = (number_of_buckets * 3/4);
            // check the load factor
            if(state->modified_pages+1 < max_pages){
                state->modified_pages++;
                state->entries[index].page_num = page_num;
                *entry = &state->entries[index];
                return true;    
            }
            switch(expand_hash_table(state_ptr, number_of_buckets)){
                case hash_resize_success:
                    // try again, now we'll have enough room
                    return allocate_entry_in_tx(state_ptr, 
                    	page_num, entry);
                case hash_resize_err_no_mem:
                    // we'll accept it here and just have higher
                    // load factor
                    break; 
                case hash_resize_err_failure:
                    push_error(EINVAL, "Failed to add page %lu to"
                    	" the transaction hash table",
                    	 page_num);
                    return false;
            }
        }
    }

     switch(expand_hash_table(state_ptr, number_of_buckets)){
        case hash_resize_success:
            // try again, now we'll have enough room
            return allocate_entry_in_tx(state_ptr, page_num, entry);
        case hash_resize_err_no_mem: 
            // we are at 100% capacity
            // can't recover, will generate an error now
            push_error(ENOMEM, "Can't allocate to add page %lu "
            	"to the transaction hash table", page_num);
            return false;
        case hash_resize_err_failure:
            push_error(EINVAL, "Failed to add page %lu to the"
            	" transaction hash table", page_num);
            return false;
    }
}
```

The code in Listing 3.8 starts out in the same manner as `lookup_entry_in_tx` does, by doing a scan on the buckets from the starting position based on the
modulus of the page number with the number of buckets. We are searching for one of two cases:

* A bucket that has the same page number, which we error on, because after the page has been allocated, there should never be any overwrites.
* An empty bucket, which we can reuse. The way I implemented this hash table, we will write to the first empty bucket. That means that if we find any
  empty bucket during a scan, we know that the page number we search can't be on a later bucket.

Once we found an empty slot of put the value in, we check the load factor on the table. If it is lower than 75%, we can assign the bucket to the page
and return successfully. But if we are over the 75% mark, we need to resize the hash table. This is done in `expand_hash_table`, which we will explore
shortly. 

There are three possible return values for `expand_hash_table`:

* Success, in which case we increased the size of the hash table and will recurse to add the page in the right location.
* Failure, the function has run into unrecoverable problem. 
* Unable to resize because of memory pressure. In this case, we have two separate behaviors. If there isn't enough memory, but there is still room in 
  the hash table we'll allow the load factor to grow as needed. We'll only raise an error when the load factor reach 100% and we cannot complete the 
  allocation at all.

The idea behind this behavior is to be robust to temporary conditions. We don't *have* to resize the hash table if there is memorry pressure right now.
We can increase the load factor and gain more time. Maybe the transaction will complete and we can throw away the entire table, for example.

> **Memory management in Linux**
> 
> By default, Linux will never fail a `malloc`. In other words, when calling `malloc`, you'll never get `NULL` back. If you care to know more about 
> this behavior, search for overcommit and the OOM killer^[https://lwn.net/Articles/317814/]. A good paper on why overcommit is required is 
> "A fork() in the road"^[https://www.microsoft.com/en-us/research/publication/a-fork-in-the-road/]. 
>
> In essence, too much software simply assume that all memory allocations will succeed. Even the Linux kernel has this an issue with kind of 
> expectation^[https://lwn.net/Articles/627419/]. 
>
> The problem with not failing `malloc` is that Linux cannot just magic some omre RAM, and even with swap, you'll eventually run out. The issue
> is how it _handles_ this scenario. And the way Linux work is that when you use too much memory, Linux will select a victim process and kill it
> to reutilize the memory it uses.
>
> That has serious implications for people building robust software. I can't catch a `SIGKILL` and ignore it, after all. But I could change
> my system behavior to handle memory allocation fialure. There are configuration flag that you can use to avoid this behavior, but they 
> have their own side effects and they are global. In other words, they impact the whole system. If you are running a single process, or
> as we are doing now, a library, that isn't going to be useful for you.
>
> The code I'm writing here assume that it is possible to get memory allocation failures and handle them appropriately. We are currently
> allocating directly from the system, but that is likely to change as the implementation proceeds. We'll start managing the memory much
> more closely and will be able to react properly to low memory events.

The last piece of the puzzle for the hash table is the `expand_hash_table` function, which is covered in Listing 3.9.

```{caption="Expanding the hash table" .c}
enum hash_resize_status {
    hash_resize_success,
    hash_resize_err_no_mem,
    hash_resize_err_failure,
};

static enum hash_resize_status expand_hash_table(
			txn_state_t** state_ptr, size_t number_of_buckets){
    size_t new_number_of_buckets = number_of_buckets*2;
    size_t new_size = sizeof(txn_state_t) + 
    	(new_number_of_buckets*sizeof(page_hash_entry_t));
    txn_state_t* state  = *state_ptr;
    txn_state_t* new_state = calloc(1, new_size);
    if (!new_state){
        // we are OOM, but we'll accept that and let the hash
        // table fill to higher capacity, caller may decide to 
        // error
        return hash_resize_err_no_mem;
    }
    memcpy(new_state, state, sizeof(txn_state_t));
    new_state->allocated_size = new_size;

    for(size_t i = 0; i < number_of_buckets; i++){
        if(!state->entries[i].address)
        	continue;

        size_t starting_pos = 
        	state->entries[i].page_num % new_number_of_buckets;
        bool located = false;
        for(size_t j = 0; j < new_number_of_buckets; j++){
            size_t index = 
            	(j + starting_pos) % new_number_of_buckets;
            if(!new_state->entries[index].address){ // empty
                new_state->entries[index] = state->entries[i];
                located = true;
                break;
            }
        }
        if(!located){
            push_error(EINVAL, 
            	"Failed to find spot for %lu"
            	" after hash table resize", 
            	state->entries[i].page_num);
            free(new_state);
            return hash_resize_err_failure;
        }
    }

    *state_ptr = new_state;// update caller's reference
    free(state);
    return hash_resize_success;
}
```

Listing 3.9 allocate a new bucket buffer, double the size of the old one (plus the size of the `txn_state_t`). If we fail this allocation, we let the 
caller know about this and they can decide how to handle it. You saw the two behaviors we have for this error mode in Listing 3.7.

We then copy the `txn_state_t` and then copy all the existing values from the existing table to the new one, placing them in their new location. 
I'm handling impossible scenario as well (`located` being `false`), just to be on the safe side. The key here is that there really isn't much that is
interesting here. This function does the work, and that is it.

There is a catch here. We allocate a _new_ buffer for the hash table and the transaction state. But the caller points to the old one. That is why we use
the `txn_t` that holds a `state` field. The caller can keep using the same value, but we replace the internal state of the system. There are other
important properties that we result from this behavior:

* The memory is always allocated as a single buffer, aiding locality.
* Doubling the size of the hash table means that we'll very quickly reach the appropriate size.
* When we'll start managing memory directly, instead of using `malloc`, we'll be able to reuse this buffer across transaction calls. Meaning that allocations
  will only need to happen on the _first_ transaction.


## Committing and closing the transaction

Now that we have a way to keep track of the modified pages in the transaction, the next stage is to figure out how to commit those changes to disk. It turns out 
that most of the work was already done for us. Listing 3.10 shows how to "commit" a transaction.

I'm using commit in quotes here because this is not respecting any of the transaction properties, but we are laying down the foundation for actual transactions.


```{caption="Committing the modified pages to disk" .c}
bool commit_transaction(txn_t* tx){
    txn_state_t* state = tx->state;
    size_t number_of_buckets = get_number_of_buckets(state);
    
    for(size_t i = 0; i< number_of_buckets; i++){
        if(!state->entries[i].address)
            continue;

        if(!write_file(state->handle, 
            state->entries[i].page_num * PAGE_SIZE, 
            state->entries[i].address, PAGE_SIZE)) {
                push_error(EIO, "Unable to write"
                	" page %lu", state->entries[i].page_num);
            return false;
        }
        free(state->entries[i].address);
        state->entries[i].address = 0;
    }
    return true;
}
```

There really isn't much to do here, to be honest. We scan through the table of modified pages and write them to the file using the `write_file` function
we looked at in the previous chapter. If there is an error, we return, and that is about it. Note that we don't have any attempt to for durablity, 
atomicity, etc. 

After the transaction is committed, we can close it. Or, we can close the transaction without committing it to as a rollback. Listing 3.11 shows the
relevant code.

```{caption="Closing the transaction and freeing resources" .c}
 bool close_transaction(txn_t* tx) {
     if(!tx->state)
        return true; // probably double close?
    txn_state_t* state = tx->state;
    size_t number_of_buckets = get_number_of_buckets(state);
    bool result = true;
    for(size_t i = 0; i< number_of_buckets; i++){
        if(!state->entries[i].address)
            continue;

        free(state->entries[i].address);
        state->entries[i].address = 0;
    }

   if(!unmap_file(state->address, state->file_size)){
        mark_error();
        result = false;
    }

    free(tx->state);

    tx->state = 0;
    return result;
 }
 ```

Closing the transaction in Listing 3.11 means that we iterate over the modified pages and free them, we release the file mapping and free the transaction
state. And then we are done. 

A more sophisticated system will allow us to reuse the allocated memory from one transaction to the next, but I'm reserving those kind of behavior for the
future. Get it working, get it working _right_ and only then get it working fast.

## Using the transaction API

The last thing that we'll do in this chapter is put everything together. Listing 3.12 shows how we can now write Listing 3.2 using the new API.
It is a bit longer than the previous version, but mostly because of all the error handling. As soon as our API will stabilize, I hope to be able
to start calling this from Python, where the errors will be handled via exception and the whole of Listing 3.12 is handled in 8 lines of code in 
Python. In fact, you can see that as Listing 3.13. 

```{caption="Writing and reading using the transaction API" .c}
txn_t tx;
if(!create_transaction(handle, 0, &tx)){
  print_all_errors();
  return EIO;
}
defer(close_transction_p, &tx);

page_t page;
page.page_num = 0;
if(!modify_page(&tx, &page)){
  print_all_errors();
  return EIO;
}

const char msg[] = "Hello Gavran";
memcpy(page.address, msg, sizeof(msg));

if(!commit_transaction(&tx)){
  print_all_errors();
  return EIO;
}

if(!close_transaction(&tx)){
  print_all_errors();
  return EIO;
}

if(!create_transaction(handle, 0, &tx)){
  print_all_errors();
  return EIO;
}

if(!get_page(&tx,&page)){
  print_all_errors();
  return EIO;
}

printf("%s\n", page.address);

// tx is closed the defer
```

In Listing 3.12, you'll note that we are using _two_ transactions. One to do the writes and one for the reads. We actually got a very limited form
of isolation. Until the transaction is completed, another transaction will not see any of its changes.

```{caption="Writing and reading using Python API" .py}
with garvan.create_transaction(handle) as tx:
	p = tx.modify_page(0)
	msg = b"Hello Gavran"
	ctypes.memmove(p, msg, len(msg))
	tx.commit()

with garvan.create_transaction(handle) as tx:
	p = tx.get_page(0)
	print(p.raw)
```

We started this chpater talking about the kind of APIs that we were missing, `modify_page`, `allocate_page` and `free_page`. We end this chapter having 
dealt only with `modify_page`. This is because to implement `allocate_page` and `free_page`, we need to have more structure in how we are working with
the data. We'll discuss that in detail in the next chapter.