== Dynamic file growth

We are in Chapter 10 and we still have a fairly big limit in Gavran. We have to define, upfront, the size of the data file. Once we hit the limit of the file size, we
will start getting `ENOSPC` errors when we try to allocate new pages. We are currently using a default of 1MB file, which means that we have merely 128 pages to work 
with. That is... not ideal. Let's see how we can fix that.

.Considerations for file size growth
****
It turns out that there is a _lot_ of things to consider when thinking about how Gavran should increase its file size. These range from technical considerations 
to social ones. 

For example, one easy option is to specify what would be the maximum size of the file. We can specify a high value, such as 10 GB or a 100 GB and be done with it.
That is how LMDB works, for example. That has a limit in the sense that you must restart the DB when the maximum size is hit. Another issue is how you allocate
disk space. 

LMDB uses a neat trick. On Linux it use `mmap()` with a size that is much larger than the actual file size, then when it need more pages, it will `write()` to the end of 
the file, extending it. That will allow the new file to be accessed via the original map. That has the disadvantage of asking the file system for disk space
one page at a time, which is not efficient. Allocating the whole file at once, on the other hand, cause users to complain.
****

=== Increasing the size of the data file

Gavran is going to have the following behavior regarding extending the file:

* We will allow to increase the file on the fly.
* We will pre-allocate disk space in advance.

These two requirements are contradictory to one another, but we can make them work. Instead of allocating the file one page at a time, we are going to use
the following formula to increase file size:

* Increase by 10% of the existing file size, rounded to the next power of two.
* A minimum of 1 MB and a maximum of 1GB.

The logic behind this is that we want to maintain reasonable file size as we grow, while balancing how much we need to grow the file. Growing the file is 
an expensive operation. We need to increase the file size itself, which means that the file system needs to allocate it size on disk. We are intentionally
asking for larger increases, so the file system can allocate us continuous ranges as much as possible.
Then there is the work that Gavran needs to do, remapping of the memory and setting up the new free space accounting. 

On the one hand, we want to reduce the number of times that we grow the file, but we also have to take into account the fact that users expect the file 
to be as small as possible. <<db_find_next_db_size>> shows the logic behind increasing the file size.

[source]
[[db_find_next_db_size]]
.`db.c` - Computing the next size fo the file when it is needed
----
include::./code/db.c[tags=db_find_next_db_size]
----

We'll increase the file by 10% each time, rounded to power of two, except when we are near a power of two of the file. Then we'll prefer that. This is mostly
for my peace of mind, because I like seeing round numbers in the file size. It doesn't have any meaningful difference from a technical perspective.

We'll currently fail with `ENOSPC` in the `txn_allocate_page()`, which we can't find enough free pages. Let's see what we need to do in order to increase the
file size. I modified the end of `txn_allocate_page()` to attempt to increase the size of the file, as shown in <<txn_allocate_page_10>>.

[source]
[[txn_allocate_page_10]]
.`txn.alloc.c` - Increasing the size of the file if we have no more room available
-----
include::./code/txn.alloc.c[tags=txn_allocate_page]

  // redacted
include::./code/txn.alloc.c[tags=txn_allocate_page_end]
-----

In other words, we'll try to increase the size by calling `db_try_increase_file_size()`, if we are successful, we'll recurse into `txn_allocate_page()` again 
to perform the actual allocation. If the file extension flopped, we'll return `ENOSPC` as we previously did. 

The fact that we need to handle file size growth lead to a bunch of complication for the code. We currently have a single `span_t` instance that 
is kept on the `global_state` field in both `db_state_t` and `txn_state_t`. This was done in preparation of this moment. 
We haven't explicitly stated it, but there is a great importance in managing the state of the system in particular place and order. While the `span_t`
was the same for the duration of the database lifetime, that caused no issues. But now we need to modify it, and that brings a whole set of trouble.

.The responsibilities of components in Gavran
****
The `db_state_t` holds the global state of the system, generally things that either never change (like the file we use) for the duration of the database
instance or modified and used only by the current write transaction.

The `txn_state_t` holds the state for a particular transaction. If it is a read transaction, it is a frozen state, which cannot be altered. We intend to 
allow the use of a single state from multiple concurrent threads, so that is a hard requirement. 
For the write transaction, we allow to modify the state, but we take into account that the transaction may be rolled back. So any change that we do in the
write transaction cannot be made public to the rest of the system until the transaction commits.

That means that we have a cycle. There is some state that is modified by the write transaction, and on commit becomes the state of the whole system. It then 
become the (frozen) state of the read transaction that is created from the committed write transaction.
****

Before we get into exactly why the `span_t` being modified is problematic, let's take one step back. _Why_ do we need to modify it in the first place?
Take a look at <<extending-file>>, where we see a file that was expanded.

[[extending-file]]
.A file whose size was expanded, with the old and new memory mapping
image::extending-file.png[]

In <<extending-file>>, we have two memory mapping for the same file. The first one (on the top) maps the first 1MB of the file. Then we expanded the file and need
to have access to the newly allocated space, how do we get it? We map the whole file _again_. That is the second map (on the bottom). The first 1 MB of
the file is _shared_ between those two mappings. There are two virtual address ranges that will point to the same place. That will be the same physical memory page 
if the page is resident in memory. If the page isn't in memory, then a page fault on _either_ map will setup _both_ addresses to point to the same page when it is
accessed.

[TIP]
.Can't we map the new data at the end of the old range?
====
Linux and Windows both have APIs that allow you to ask to map a file at a particular address. We can use that to map the new portions of the file at the 
end of the old mapping, effectively extending it. In fact, Linux has a dedicated system call for this `mremap()`. The problem is that this may fail, and we
need to write the code to handle the scenario when there isn't enough virtual memory available at the current location to expand the file. 

Since we need to write this code anyway, it reduces the complexity for us to just have one way of expanding the memory, by remapping it to another location 
in the new size. We waste a bit of virtual address space, but we can usually treat that as "large enough that we don't care" anyway.
====

Virtual memory is a wonderful thing, we map the same range in the file twice, and changes that happened there will be reflected in both. A good post 
on the matter in https://devblogs.microsoft.com/oldnewthing/20031007-00/?p=42263[Raymond Chen talking about Stupid memory-mapping tricks]. The ability
to retain the old mapping is important, we have read transactions that are actively working against that old mapping. If we were to drop the memory
mapping, they would suddenly try to access invalid memory. That leads to the crux of our problem. With the `span_t` no longer being static, we need
to think about how to manage the new state of the system.

The idea behind `db_global_state_t` is that a read transaction has a frozen view of the world, including all the state that we have on the system. 
That includes the memory map that was effective at the time of the transaction. Let's take a look at <<txn_state_t_10>> to refresh out memory about how
the `txn_state_t` struct looks like.

[source]
[[txn_state_t_10]]
.`gavran/db.h` - The `txn_state_t` struct, the heart of all transactions
----
include::../include/gavran/db.h[tags=txn_state_t]
include::../include/gavran/db.h[tags=cleanup_callback_t]
----

Take note in <<txn_state_t_10>> to the `on_forget` and `on_rollback` fields. What are those? The `cleanup_act_t` is shown
on <<txn_state_t_10>>  as well. This is used to hold the required state for cleanups, which is something that we now need to take into account.

Consider the case of trying to create a new mapping for the file size increase as part of a transaction. And then that transaction fails for some other
reason. Because the transaction did not commit, we must discard all its state, including the new file mapping. That is the purpose of the `on_rollback` 
callback field, to allow to invoke cleanup code if the transaction has been rolled back.

.What about rolling back changes to the file size?
****
One thing that we are _not_ going to do is to roll back a change to the file size. If a transaction has extended the size of the file, we'll not truncate
it again. The reason for this is that there is no real need to do so. We aren't actually relying on the size of the data file for anything, we have the
`number_of_pages` value in the file header for that. And it is likely that we'll need to extend the file shortly anyway, so no need to do extra work
in this regard.
****

Conversely, when we have successfully committed a transaction after increasing the file size, we need to `unmap()` the _old_ mapping. We can't do that 
immediately, because there are older read transactions that may be using it. We are going to reuse the same cleanup concept we have for writing for the
disk. If there are no transactions that are old enough to want to look at old pages, we can write them to disk. In the same vein, we can wait until all
the active transactions are newer than the current write transaction and then close the old mapping. All the current transactions are now looking only
at the new one. That is the purpose of the `on_forget` callback back.

With all of this background in place, let's look at the code to actually increase the file size, shown in <<db_try_increase_file_size>>.

[source]
[[db_try_increase_file_size]]
.`db.c` - Increasing the data file size
----
include::./code/db.c[tags=db_try_increase_file_size]
----

We compute the new size of the file and map it again. Note that there is a small dance here with the error handling to make sure that in all cases,
we'll cleanup the resources if there was an error.
After we map the file we use `try_defer` to ensure that a failure to register the rollback will not cause us to leak the new memory map. We register
a deletion of the _existing_ memory map when the transaction is forgotten. When no one is using this transaction or any older transaction, as we saw
in the previous chapter. 

[IMPORTANT]
.The free space bitmap also need some space
====
In <<db_try_increase_file_size>>, you can see a call to `db_new_size_can_fit_free_space_bitmap()`, this is meant to handle a very specific edge case. 
When we increase the size of the file, we also need to update the free space bitmap. Usually we can do that by marking the bits in the bitmap, but if 
the size of the file grew beyond the capacity of the bitmap, we need to move it. And we have to assume that there is _no_ available free space in the
existing file for the bitmap. 

We limit our file growth to 1 GB in size, but that can cause us issues. If the size of the _free space bitmap_ is more than 1 GB and we need to move 
it, we may run into a case where we increase the file and then fail because there is not enough free space for the new free space bitmap. Given a 
maximum size of 1 GB, it means that the file would need to have over 64 TB in size before we'll actually have to deal with it. But I'm not planning
on having limits that I don't absolutely have to have. Therefor, `db_new_size_can_fit_free_space_bitmap()` is here to ensure that any growth in the
file will have enough space to accommodate the free space bitmap. That is also why the call to `db_try_increase_file_size()` is immediately followed
by a recursion into `txn_allocate_page()`. It is possible (although unlikely) that we'll need to increase the file _twice_ before we can satisfy the
allocation request. First to move the free space bitmap and then to allocate the actual pages requested.
====

<<txn_register_cleanup_action>> shows how we handle the registration on the transaction. 

[source]
[[txn_register_cleanup_action]]
.`txn.c` - Registering callbacks on the transaction
----
include::./code/txn.c[tags=txn_register_cleanup_action]
----

There are a couple of things that are very important to note here. First, we maintain a _copy_ of the state that was passed to us, instead of maintainin a
reference to it. That means that we can discard the value after we call the register function. Second, we use a single allocation to hold the new
`cleanup_act_t` and its state. These two behaviors simplify memory handling significantly, both inside the registration / invocation code 
paths and for the callers. You can see how they are being used in <<txn_commit_10>>.

[source]
[[txn_commit_10]]
.`txn.c` - Freeing rollback callback state on commit
----
include::./code/txn.c[tags=txn_commit]
----
<1> New code, when we update the mapping we work on after increasing the file size, we do that only on the local transaction. On commit, we publish this to 
    the rest of the system. In this case, the `global_state` on the `default_read_tx` and the `db_state_t`. Note that this is safe to do even for running
    transactions. The mapping overlap and you can access the same memory using both the old and new mapping to get the same values.
<2> New code, when we successfully committed the transaction, we can free the rollback state immediately.

<<txn_commit_10>> shows what happens when we successfully committed. We can immediately free all the memory around rollback callbacks. Since rollback is now
no longer possible. There is still the issue with the `on_forget` callback. That is called from the `txn_free_single_tx_state()` function, showing in 
<<txn_free_single_tx_state_10>>. 

[source]
[[txn_free_single_tx_state_10]]
.`txn.c` - Executing `on_forget` callback when the transaction is being discarded
----
include::./code/txn.c[tags=txn_free_single_tx_state]
----
<1> The new code, just before we free the transaction, call all the registered callbacks to ensure that the entire state is cleaned up.

[CAUTION]
.The identity function of a page is its `page_num`, nothing else!
****
Gavran makes the following promises with regards to pages and their memory. 

* Once you accessed a page, you can continue to access the page using the same memory address for the duration of the transaction.
* It is _invalid_ to store that pointer after the transaction has closed. 
* Two calls to `txn_get_page()` may return _different_ pointers in the same transaction. 

The last one is interesting, let's see how that can happen. We start a transaction and then call `txn_get_page(2) -> 0xF68000` for the duration
of the transaction, we'll always be able to use `0xF68000` to get to the page. However, we allocate a new page and the data file grow, resulting
in new mapping. We call `txn_get_page(2) -> 0xB8439000` and we get a _different_ pointer. This is because each one of the calls was served from
a different mapping. 

In this case, both `0xF68000` and `0xB8439000` are going to be _identical_ in content. They are pointing to the same physical memory location. 

This isn't a problem in practice, because we observe the exact same thing when we use `txn_modify_page()`, because of Copy on Write, we get a 
different memory from the one that we got from the `txn_get_page()` call. Note that inside a single transaction, all calls to `txn_modify_page()`
will return the _same_ pointer. Indeed, `txn_modify_page()` will simply return the previous value when called on a page for the second time.
****

Finally, there is the `txn_close()` function, which we had to modify to take into account the new callbacks. As a reminder, when we close a transaction that
hasn't been committed, this is considered a rollback, there is no explicit rollback mechanism in Gavran. You can see the changes we made to `txn_close()` in
<<txn_close_10>>.

[source]
[[txn_close_10]]
.`txn.c` - Handling rollback of transactions in `txn_close()`
----
include::./code/txn.c[tags=txn_close]
----

In <<txn_close_10>> there is a careful dance in the case of rollback. We need to invoke _just_ the `on_rollback` callback, to discard the new memory map and
_not_ invoke the `on_forget` callback (which should be invoked only for committed transactions). Because `txn_free_single_tx()` will call the `on_forget` callback,
we free all the `on_forget` callbacks before we call to `txn_free_single_tx()`.
The reason we don't want to call `on_forget` in the case of a rolled back transaction is that it will remove the _existing_ memory map, which will leave
us hanging badly.

[IMPORTANT]
.Transactions apply their state on commit, or rollback atomically
****
One of the design principles of Gavran is that the transaction is the unit of all changes in the system. We use that to modify the data file safely. We
also use that to modify the internal structure of Gavran itself. Using this model of pushing all changes through the transaction and accepting them via
the commit simplify a great deal of state management inside of Gavran.
****

The fact that all the modifications to the system state are done on an isolated copy of the `global_state` is a huge relief. We don't need to 
carefully orchestrate modifying things, we can do everything locally, on a copy, and only publish this as part of the commit. If you are finding
similarities in the approach to the Copy on Write system we use for page modifications, that is by no means an accident. Being able to reuse 
concepts and having a single line of thinking simplify the code base significantly.

Just increasing the file size and mapping the memory again isn't enough, I'm afraid to say. We also need to make sure that the free space bitmap
knows about the new size, that the file header is updated, etc. This is handled in `db_finalize_file_size_increase()`, called as the last step
in  `db_try_increase_file_size()`. You can see its behavior in <<db_finalize_file_size_increase>>.

[source]
[[db_finalize_file_size_increase]]
.`db.c` - After increasing the actual file size, we need to update our internal structures about the file size and free space usage.
----
include::./code/db.c[tags=db_finalize_file_size_increase]
----

In the common case, we can simply mark the newly available pages in the `db_increase_free_space_bitmap()` function. Each free space bitmap page can
mark 512 MB of data, so growing the free space bitmap should be a rare event, most of the time, we just need to mark the new pages as free and that
would be it. We'll see how we deal with growing the free space bitmap in `db_move_free_space_bitmap()` in a bit.

The rest of `db_finalize_file_size_increase()` is just setting the new `number_of_pages` on the page header on the first metadata entry of the file, 
and that is that. The changes are made to modified pages, which go through the usual Copy on Write, being written to the WAL, copied to the data
file when there are no read transactions that may observe this, etc. In other words, this part is also part of the transaction, like the `global_state`
behavior, but it is done using the usual mechanism that we have setup, no need for any special behaviors here.

The behavior is much more involved when we need to actually move the free space bitmap. This is shown in <<db_move_free_space_bitmap>>. What it _does_ in essence is
to create a new bitmap in memory, with the new size, based on the old bitmap. It uses the new bitmap to find the new location of the bitmap and then
move the bitmap there. See the notes in the code for the full details.

[source]
[[db_move_free_space_bitmap]]
.`db.c` - Moving the free space bitmap to a new location when it is unable to fit in its current location.
----
include::./code/db.c[tags=db_move_free_space_bitmap]
----
<1> Get the new size of the bitmap in pages, based on the new file size. We actually use _more_ space than is needed, to reduce the number of times
that we need to move the free space bitmap.
<2> Allocate _in memory_ buffer for the bitmap. 
<3> Copy the old bitmap and then mark the rest of the bitmap as busy.
<4> Set the newly available space as free in the new bitmap.
<5> Search _the new bitmap_ for available space for the new bitmap. This may be in the new file range or in the old one. We ensure that the new space
    will have at least enough space for the free space bitmap in `db_new_size_can_fit_free_space_bitmap()`.
<6> Mark the new free space bitmap pages as busy.
<7> Modify the bitmap pages and copy the new bitmap to them.
<8> Update the metadata of the new free space bitmap.
<9> Update the transaction's state to point to the new free space bitmap.
<10> Free the pages of the old free space bitmap, we are not freeing them in the _new_ bitmap, because we already set `free_space_bitmap_start` in the 
     transaction's `global_state.header` so that will already use the new bitmap.

<<db_move_free_space_bitmap>> is the most complex piece in the data file growth, it ensures that the free space bitmap moves without issue and it has to do quite a lot
to get things working. Probably the most tricky part is that we copy the bitmap to memory, including the newly available pages and then use that in 
memory bitmap to find the location of the bitmap. This is a bit recursive, but it ends up working quite nicely in the end. 

I'm actually copying the new bitmap twice, once to an in memory buffer where I'll run my checks and once from the in memory buffer to the page buffer 
held by the transaction.  At small sizes, it won't matter. But as the size of the data file grows, so would the free space. At 512 GB, if you'll recall,
the free space bitmap would take 64 MB. It is possible to avoid this cost by making the code more complex, but that is probably not worth it. 
At 512GB, the free space bitmap would not be 64 MB, we'll actually reserve _70_ MB for that. Meaning that the next time we'll need to move it will be when
we grow the file by another 48 GB. That far apart, it isn't really worth it to try to optimize the double memory copy.

.What did all of this get us?
****
We can now extend the size of the file on the fly, while transactions are still running. The internal metadata will be updated and if needed changed to
reflect the new change. We also take care to reduce those costs by allocating a bit more than is required when we increase the size of the free space 
bitmap. 

There is some complexity involved, the rest of the architecture and behavior remained the same and we even have better understanding of how these sort of 
constraints are going to impact our system in the future. 

The fact that the read transactions are immutable and that we need to keep their state around until they are no longer being used is going to be _huge_ when
we start to implement multi threading support. And the fact that we are spending so much time at the lowest levels of the system means that we don't need to
worry about these details at higher levels. We'll see how all of those play out in the next part of the book.
****

=== Increasing the size of the WAL

We spent a lot of time working on extending the size of the data file, but what about the size of the WAL? We currently define the size of the WAL on 
startup based on the `db_options_t.wal_size` value. What happens if we aren't able to clear the WAL quickly enough and we reach the end?
You might expect that we'll get an error, something like the `ENOSPC` that we got when reaching the limits of the data file before we fixed that, but 
that isn't the case.

[TIP]
.Avoiding unnecessary limits
====
It is very common in databases to have values such as `max_buffer_size` or the like. I don't like these because they are usually set in advance are 
are not related to the actual resources available on the machine. If I'm writing to disk, but I have 2 TB of hard disk free, I don't really
care to stop after 512MB because that is the size of the WAL configuration. 

Such values are bad if the common reaction when you find them is to just bump them upward. I would much rather have configuration that guide us,
not one that blocks us. Remember that principle when it comes to designing your system configuration. 
====

As it is currently written, we're using `pwrite()` to write to the WAL, and we are going to increment the position to which we write on each call. 
If we get to the end of the file, the operating system will simply increase the size of the file. The problem with doing this is that we haven't
updated the parent directory to make sure that the metadata change on the WAL file will be persisted. So this sort of works, but it isn't safe.
There is also the chance of hitting the limits of the disk, which means that you may get a `ENOSPC` error at any time. I would much rather do 
things in a more structured manner. 

Just like the manner in which we increase the size of the data file, we can detect and increase the size of the WAL. However, that leads to an 
interesting question, how should we actually handle the WAL? Right now we have a single file and we reset it occasionally. That isn't 
necessarily the best way to handle things. We could create a _new_ file when we reach the limits of the WAL. That is how Voron works, for 
example, as well as RocksDB and many other databases. 

Using multiple files means that we can more easily handle the WAL reset that we are currently managing by resetting the position we write to the 
file. We'll simply use a new file and we can use the file _name_ as an indication of sequence. So we'll have `db-00001.wal`, `db-00002.wal`, etc.
That works, but it has a few caveats. First, allocating new files all the time requires that you'll create them and have the file system allocate
space for them. That isn't ideal. 

We can keep these files around, reusing the allocated disk space. Again, this is what Voron is doing, but it takes an effort. We also need to rename the
files whenever we create a new WAL. That requires I/O operation for the rename as well as updating the parent directory. We also need to keep track
of what WAL file we are currently using, what files we can reuse, etc. 

The big advantage of having a single file is the simplicity of implementation. But it has a fatal issue. Consider the timeline shown in <<endless-tx-in-wal>>.

[[endless-tx-in-wal]]
.A time line of concurrent read and write transactions
image::endless-tx-in-wal.png[]

In order to reset the WAL, we need to have _no_ active transactions. If at any given point we have active transactions, we'll never be able to 
reset the WAL and it will grow without bound. When using multiple files, we can easily handle that because at some point the current transaction
will be beyond the oldest WAL file, which means that it can be deleted or repurposed. That is not possible to do when you have a single file.

So we need to change our WAL to use multiple files. Let's see what changes are required to make this happen... The first thing to realize is 
that working with files in C simply sucks. My first attempt to get a listing of the relevant files from the directory (files with format of
`<database-name>-<number>.wal` went on for over a 125 lines! Instead of trying to use multiple dynamic files, I decided to use a static set 
of files. We'll have the following files in a Gavran database. 

* `db/phones` - The actual database file.
* `db/phone-a.wal` - The first WAL file.
* `db/phone-b.wal` - The second WAL file.

All three of them together are going to compose our database. Note that the first & second designation are arbitrary. There isn't any intrinsic
order in the use of the WAL files.

[TIP]
.Avoiding esoteric features
====
Linux actually includes support for `FALLOC_FL_COLLAPSE_RANGE`, which does exactly what we want. We can give it a range at the start of the 
file and it will truncate it from the _start_. That is just perfect for our situation, but it is sadly really bad idea to actually make use
of such a feature. Why is that?

`FALLOC_FL_COLLAPSE_RANGE` exists only on Linux, it has no equivalent on Windows or macOS. It is also limited to specific file systems. And
one of the key things that I have learned is that people do all _sort_ of crazy things with their systems. You may be running Gavran on 
Linux on top of ext4 (supporting `FALLOC_FL_COLLAPSE_RANGE`) but you may be running this in a docker instance running on Windows, in which 
case the volume you use is actually using CIFS (*not* supporting `FALLOC_FL_COLLAPSE_RANGE`). Or you have a user that put a Gavran database on
a USB stick (usually formatted with FAT32, with no supporting `FALLOC_FL_COLLAPSE_RANGE`). 

It pays to be _really_ conservative with features that we seek to rely on at the file system level. There is a _lot_ of variance between the
various systems and it is easy to get yourself snagged by those details.
====

In `wal_recover()` we are using `wal_recovery_operation_t` and a whole bunch of functions to iterate through the WAL. Up until now, it didn't
really make sense to have all of this complexity. Now it should be clearer why I went this route. This allows us to abstract how we iterate 
over the data in the WAL. We provide functions to get the next valid transaction as well as the next range, which may be an old transaction, 
The key is that the transaction in question may come from _either_ WAL files, and the higher level code isn't really going to care about that.

Having a single file for the WAL is a simple to get started, but it isn't the only reason I chose it. There are some have some important properties 
that we get by having a single file, we don't keep messing with the file system, by allocating
new files and deleting them as we create new WAL files. That can be surprisingly expensive. Another consideration is that it is common for users
to want to move split the data and the WAL to different locations. Why would a user want to do something like that? Well, consider the _physical_
architecture that they may have.

If the user have two disks attached to a machine, they may want to have the data reside on one disk and the WAL on another. The requirements that
we place on the WAL and the data files are _very_ different. The WAL is all about sequential writes and is almost never read (only on recovery)
while the data file is about random reads and writes, buffered by memory. However, the data file writes are _not_ critical for us. We are currently
in single threaded mode, but in the future, they will be done in a background thread. The WAL performance, on the other hand, is critical, because
we cannot complete a transaction until the write to the WAL has successfully completed. 

Given these requirements, it is common to have a big and somewhat slow disk for the data file, which is mostly backed by memory anyway and a smaller
but faster disk for the WAL. Separating the data disk and WAL disk also gives us better throughput overall, since we don't have competition on the 
same resource.

[TIP]
.How do you configure multiple disk layout?
====
Multiple disk layout for a database is both an advanced option and something that database administrators are well familiar with and will try to
apply. It is also a PITA to define in code, because there are so many places that you need to reference it and the root cause of many bugs because
the code didn't account for the "WAL is in another location" issue. 

Instead, Gavran takes the position that the user can run a `ln -s /db/phones-a.wal /mnt/fast/wals/phones.wal` and everything will work as expected
because we are carefully _not_ doing anything to prevent that scenario when we are opening the WAL file.

This means that we have to have _predictable_ file names for the WAL file(s), or use a directory for them. 
====

I would much rather have a single file, but we can't have that and allow for continuously opened read transactions, as we saw in <<endless-tx-in-wal>>. Creating
many WAL files has its issues, so we'll avoid it as well. Instead, we'll use double buffering approach. Gavran will have _two_ WAL files. For the
database `/db/phones` we'll have:

* `/db/phones-a.wal`
* `/db/phones-b.wal`

These are going to be used as the backing store for the WAL, and we'll switch between them as needed. Because we can switch between them, we can 
clear the one that isn't being used when there are active transactions and then switch to it, and so on. You can see how that would work in <<two-wal-files>>.

[[two-wal-files]]
.Two WAL files used to store the transaction history of a database
image::two-wal-files.png[]

<<two-wal-files>> shows the two files and the transactions that are stored in them. Let's consider the recovery process from such a state.

* We open both files and test the first transaction on both of them.
* We start with the file with the _oldest_ transaction id.
* We run through the file normally, applying the diffs from the transaction.
* When we are done with the valid transactions, we validate the end of the file, as we did before.
* When we are done with the first file, we do the same on the second one. Again with validation at the end.

In the case of <<two-wal-files>>, we'll have the following steps:

* Compare tx 14 from the `B` file to tx 17 from `A` file and decide to use `B` first.
* Read transactions 14, 15 and 16 from the `B` file.
* Read transaction 12 from the file, discard it as old (previously written).
* Switch to `A` file.
* Read transactions 17, 18 from the `A` file.
* Discard transaction 8 from `A` file because it is old.
* Discard garbage data at the end of `A` file.

==== Recovery with dual WAL files

How would the checkpoint process work with two files, then? Previously, we could only do a real checkpoint when there were no other active transactions.
Now, we can do a checkpoint when the oldest active transaction is beyond the oldest WAL file. We'll `fsync()` the data file and then write a zero page
at the beginning of the oldest WAL file. That ensures that we'll no need to apply any of the data in the WAL file. We'll still need to 
consider the _other_ file, of course, but that is already handled by the steps above.

Let's see how that is actually going to work. I'm going to start by implementing the recovery changes. Because we have an abstraction over the iteration
of transactions data, we won't need to change the `wal_recover` function. We'll start with looking into the data structures that are used during recovery.
<<wal_state_t_10>> has the details.

[source]
[[wal_state_t_10]]
.`gavran/db.h` - The data structures we use to manage the WAL
----
include::../include/gavran/db.h[tags=wal_state_t]
----

In <<wal_state_t_10>> you can see the `wal_state_t`, which is held by `db_state_t`. The `wal_state_t` has an array of two `wal_file_state_t` and the 
`current_append_file_index` field that selects the one that is currently in use. When calling `wal_append()` we'll always write to the WAL file indicated
by `current_append_file_index`. We'll switch between the WAL files after each checkpoint, because that is when the _other_ file has been cleared of all
previous transactions and is able to accept new ones at the start of the file.

During the recovery, however, we use a different structure, the `wal_recovery_operation_`, shown in <<wal_recovery_operation>>.

[source]
[[wal_recovery_operation]]
.`wal.c` - The data structures we use to recover the WAL
----
include::./code/wal.c[tags=wal_recovery_operation]
----

There are a few differences between `wal_recovery_operation_t` and the `wal_state_t`. In particular, the `files` array in the `wal_recovery_operation_t` is
_not_ necessarily in the same order as the one in `wal_state_t`. For the `wal_state_t`, the order is that the `A` file is first and `B` file is second.
For the `struct wal_recovery_operation`, the order is dependant on the value of the earliest transaction recorded.
The `current_recovery_file_index` is also different from `current_append_file_index`. That should be clear from the naming, but I'm making sure to point it
out explicitly. 

The first step on the recovery process with dual WAL files is to select which one of them come before the other. This is done in the `wal_init_recover_state()`
function, shown in <<wal_init_recover_state_10>>.

[source]
[[wal_init_recover_state_10]]
`wal.c` - Initialize the state of the recovery process and select which file we'll process first
----
include::./code/wal.c[tags=wal_init_recover_state]
----

There is a significant amount of code in `wal_init_recover_state()`, but that is mostly to handle all possible options. We start by checking the first
transaction in each file and then ordering them based on the earliest transaction id. We need to handle the case of `tx_id` being `0`, which indicate
that there are no valid transactions in the file, which is the indicator that the WAL file was reset.
We do that by only using one element in the `files` array and setting things up so the recovery process will only consider that file. 

Of particular importance is the setting of `current_append_file_index` and `current_recovery_file_index`. These will always be inverse of one another.
The `current_recovery_file_index` is the first file that we'll start the recover from and `current_append_file_index` is the file that the next call
to `wal_append()` will write to.

After the setup in `wal_init_recover_state()`, let's take a look at how we iterate transactions. The `wal_recover()` function no longer deals with the 
mechanics of iteration, that is handled by `wal_next_valid_transaction()` which knows how to merge the valid transactions across the different files.
The code for that is shown in <<wal_next_valid_transaction_10>>.

[source]
[[wal_next_valid_transaction_10]]
.`wal.c` - Finding the next valid transaction (among multiple files)
----
include::./code/wal.c[tags=wal_next_valid_transaction]
----
<1> We now need to check to the end of the file, to see if there are valid transactions after an invalid one (meaning, disk corruption). 
    We now run it here, because it needs to run on _both_ files.
<2> If we are at the second file, we are done and can stop the iteration, there are no more possible transactions.
<3> There is still a file that we need to run through, let's setup the required fields to start iterating there.
<4> Recursively call ourselves so we'll do validation on the next file transaction, etc.

The rules we setup for the WAL say that if there is an invalid transaction, after that transaction there may be only:

* Invalid data of any kind.
* Transactions that are valid (have proper hash), but have a transaction id that is lower than the last valid transaction id.

We validate this by scanning through the rest of the file one page at a page (transactions are always written on page boundary).
If we find a valid (but old) transaction, we can abort the scan because we know that we didn't stop the recovery process due to a data corruption issue. 
If we find a valid transaction that has a transaction id that is larger than the highest valid transaction id we have, we know that there is a problem. 
At this point, we run into something that broke the chain, there is a missing transaction that we can't apply. That is a cause for alarm and will abort the
recovery process with an error. We saw the details of how this work in the previous chapter and there has been no change in this regard.

Such steps may seem paranoid at time, but there is a very important rule that we have is: 
https://docs.microsoft.com/en-us/archive/blogs/larryosterman/one-in-a-million-is-next-tuesday["One in a million is next Tuesday"]. Even the 
most unlikely of things are _bound_ to happen, and we have to write the code to handle them. This particular validation check was written when Voron
gained the ability to reuse WAL files (Voron maintains a pool of them, in a more complex format, rather than just the two we have here). We have to 
rely on the file system and the hardware to maintain data integrity, but we have bitten enough times to be very cautious about it. 

The final part of the recovery process is handled by `wal_complete_recovery`, where we update the database's in memory data structures. 
This is shown in Listing <<wal_complete_recovery>>.

[source]
[[wal_complete_recovery]]
.`wal.c` - Completing the recovery process by updating the in memory state of the database.
----
include::./code/wal.c[tags=wal_complete_recovery]
----

The `wal_complete_recovery()` manages the last few details of setting up the in memory state of the database after the WAL recover, from here
on, it is the database that will complete its own setup. 

===== Implementing checkpoint with dual WAL files

The checkpoint behavior of Gavran has changed with the introduction of dual WAL files. Now, instead of waiting until we reached half the WAL
and then checkpoint the file, we'll have a slightly more sophisticated behavior, shown in Listing 10.20.
The `(cur_file_index + 1) & 1` in <<wal_will_checkpoint>> is used to refer to the _other_ WAL file, regardless of what the current one it. This basically
implements overflow addition in the range [0,1]. 

[source]
[[wal_will_checkpoint]]
.`wal.c` - Deciding when to checkpoint when using double buffering between the files
----
include::./code/wal.c[tags=wal_will_checkpoint]
----

The `wal_will_checkpoint()` function will return `true` under the following conditions: 

* The _current_ WAL file is over half full.
* The other WAL file latest transaction is old enough to ensure that no one is looking at the data from it (meaning we wrote it to disk). 

Note that this may also trigger for the _current_ file, if there haven't been enough transactions to fill both WAL files before a checkpoint can occur. 
You can see how this works in <<wal_checkpoint_10>>.

[source]
[[wal_checkpoint_10]]
.`wal.c` - On checkpoint, either reset both WAL files (if possible) or reset one and switch future transactions to append to it.
----
include::./code/wal.c[tags=wal_checkpoint]
----

[TIP]
.Why bother resetting the WAL files early?
====
We are going into some trouble to allow the code to reset both WAL files. In fact, unless there are read transactions that prevent it, the
behavior in <<wal_checkpoint_10>> will make sure that we have just a single WAL file in use. Why bother?

Having to read through the WAL files can add significantly to the startup overhead. By being eager to clear the WAL file when we `fsync()` 
the data, we make sure that our startup times is reduced.
====

I'm currently using a fairly simplistic metric of half the WAL file being full is enough to trigger an `fsync()`. Other strategies may involve
time (say, do a checkpoint every N seconds) or size (checkpoint every 4 MB). I'm assuming that the WAL size is going to be reasonably small,
based on actual needs. A WAL size that is 16MB in size will cause us to do a checkpoint every 8MB of WAL writes. That is likely to be a good
number, but we'll have to wait for benchmarking to confirm it.

[CAUTION]
.What happens if we can't checkpoint?
====
A read transaction being open will prevent us from being able to write the data to disk. That in turn means that we can't checkpoint. New
writes will accumulate and go to the current WAL file. We'll increase the WAL file as needed, to accommodate the requirement. Eventually,
the read transaction will close and we'll be able to proceed with the checkpoint process. At this point, we'll also truncate the WAL files
back to their maximum size.
====

If the WAL file is too large, we'll need to truncate it, you can see that in use inside <<wal_reset_file>>.

[source]
[[wal_reset_file]]
.`wal.c` - Resetting a WAL file on checkpoint
----
include::./code/wal.c[tags=wal_reset_file]
----
<1> New code, if the WAL file is too big, we'll trim it to the WAL size when the checkpoint allows it. 

In `wal_reset_file()` we are writing a page full of zeros as the first page of the file. That ensures that we won't consider this file as
able to hold valid transactions. If the file size is bigger than the configured values, we'll truncate the file. This might be a good place
to add more refined behavior such as waiting a cycle or two to see if the extra space is actually needed. For now, I"m going with the simplest
option.

===== Appending to the WAL

Appending to the WAL using dual WAL files didn't change all that much. We just need to be careful not to exceed the size of the file, 
most of the work of preparing the transaction to `wal_prepare_txn_buffer` (diffing the pages, compressing, building the hash, etc). That allows
us to focus solely on what `wal_append` is doing, shown in <<wal_append_10>>. 

[source]
[[wal_append_10]]
.`wal.c` -  Appending a transaction to the current WAL file
----
include::./code/wal.c[tags=wal_append]
----
<1> New code here, we select the current file to append to, check if we need to increase the size of the file and then write to the file.

In `wal_append()` we generate the hash for the transaction and then check if there is space in the current WAL file and increase the
size if needed. It _always_ write to the current WAL file. Switching between them is the responsibility of `wal_checkpoint()`. 

This turned out to be a much longer chapter than expected. We have learned how to extend the size of the data file safely and how to build double
buffering for the WAL file to handle continuously running transactions. 

=== Unit Tests

I actually run into a problem with the unit tests here. In Chapter 5, we tested that we'll get `ENOSPC` if we allocate beyond the size limit. We 
now have the ability to extend the file, so we actually got a test failure because of this issue. I added a `maximum_size` option to the 
`database_option_t` structure to allow us to make such a limit. 

.Why not change the test?
****
There are actually reasonable scenarios where you might want to have an enforced size limit. For example, assume that you have many databases
being used, one for each user, and you want to ensure that there is some fairness in using the disk space among all of these. LevelDB, for example,
was originally intended for use in Chrome, and there you have a size limit per website for how much you can store. That is a use case for this feature
and it took very little code (add the field, and add a check on `db_try_increase_file_size`), so I implemented it rather than change the test.
****


[source,python]
.Listing 10.23 - Unit test growing the size of the file and the WAL double buffering mode
----
include::../pyapi/tests10.py[]
----

////