% Building Storage Engine from Scratch
% Oren Eini
% Hibernating Rhinos

---
header-includes:
    - \lstset{breaklines=true}
    - \lstset{basicstyle=\small\ttfamily}
    - \lstset{extendedchars=true}
    - \lstset{tabsize=2}
    - \lstset{columns=fixed}
    - \lstset{showstringspaces=false}
...

# Introduction

My name is Oren Eini, and I have been working with databases for over 15 years. For the past decade or so, I have been
involved in the creation of [RavenDB^[https://ravendb.net), a distributed document database. I have also been 
involved in building other systems of similar nature such as queuing infrastructure, a distributed hash table and the like. 

It turns out that at the bottom of the stack, you need a _good_ way to store data to disk. I have looked into many ways
to handle this, from simply writing XML or JSON file to a folder to utilizing existing software and libraries such as
SQLite, LMDB, Esent, LevelDB and RocksDB. For quite some time, it has been my hobby to read through any storage engine
that I came across. And there have been many of those.

This is a book that is going to cover how to build a database, from the ground up. We aren't going to be talking about
the appropriate schema or what constraints to use. We are going to build the low level infrastructure that you need in
order bo _build_ databases. In particular, I want to walk through all the details of building a production quality 
storage engine with full ACID^[Atomic, consistent, isolated, durable] properties and real transactions.

> **Assumptions about the reader**
>
> I'm writing this book for fun, because it kept bouncing in my head and I really need to get it out so I can sleep.
> The target audience are developers who have at least a passing with system level programming and the C language.
> I'm not going to find the most esoteric pieces of C to use, but you are expected to understand the C, native 
> memeory, data structures and the like. 
>
> I'll reference other resources at times for additional materials, but I'm going to focus on the storage
> engine rather than reiterating existing knowledge.

I have written detailed reviews into many of them. The following list contains some of those review, note that they are
usually spread over to multiple posts in a series. I went through the code base and wrote my notes as they occurced to me.

* CouchDB^[https://ayende.com/blog/posts/series/3607/reading-erlang] - Document database written in Erlang.
* LMDB^[https://ayende.com/blog/posts/series/162754/reviewing-lightning-memory-mapped-database-library]- Low level 
  storage engine library written in C.
* LevelDB^[https://ayende.com/blog/posts/series/161410/reviewing-leveldb] - Low level storage engine library from
  Google. The basis for RocksDB from Facebook. 
* FASTER^[https://ayende.com/blog/posts/series/184225-A/reviewing-faster] - Low level storage engine from Microsoft.
* LemonGraph^[https://ayende.com/blog/posts/series/184066-C/reading-the-nsas-codebase] - Graph database by the NSA.
* Noise^[https://ayende.com/blog/posts/series/178497/reviewing-noise-search-engine] - Full text search engine in Rust.
* Resin^[https://ayende.com/blog/posts/series/178945/reviewing-resin] - Document database written in C#.
* Sled^[https://ayende.com/blog/posts/series/186753-A/reviewing-sled] - Low level storage engine written in Rust.

I've also written my own storage engine library, based on the things that I have learned from these reviews, experience
in the field and the needs of RavenDB itself. This storage engine is called Voron, we started working on that in 2013
and has switched all of RavenDB to it completely by late 2015. Voron has been powering all RavenDB production systems 
for years, with very little trouble.

Voron is written in C# was built to be _high_ performance and has been able to achieve impressive benchmark numbers. 
We have also built Voron to serve RavenDB and adapted RavenDB to best utilize Voron. That has been a very benefitical 
relationship for us because it allowed us to make certain that the operations we needed had as little cost as possible. 

## Why this book?

I have written extensively about database building in the past. Again, each of these links is to a series of posts that describe
certain aspects of building a database. 

* The Guts n' Glory of Database Internals^[https://ayende.com/blog/posts/series/174337/the-guts-n-glory-of-database-internals] -
  A series of posts that walks through the challenges involved in building a proper database and recommendations on how to deal
  with them.
* Database build 101^[https://ayende.com/blog/posts/series/175041/database-building-101] - Series of posts detailing how to build
  a graph database. 
* Voron internals^[https://ayende.com/blog/posts/series/175073/voron-internals] - How Voron works behind the curtain.
* Low level Voron optimizations^[https://ayende.com/blog/posts/series/176961/low-level-voron-optimizations] - Discuss some
  of the tricks we use to get Voron to be faster.

All of my work in Open Source. You can find RavenDB's code at our GitHub repository^[https://github.com/ravendb/ravendb] and the 
Voron project is the `src/Voron` directory there.

I wanted to write this book to not only present my (and my team's) work but to also have walk you through how you can
build a storage engine library from nothing. Rather than just throw code over the wall, or have stuff that is used only for RavenDB,
I wanted _explain_ things and tell the complete story. I'm also using this project as a way to test out some interesting expriments
that came to mind do proper research into additional avenues and in general build something cool.

I hope that you'll find this book interesting, it has certainly been interesting to write it. This isn't meant to be a theoretical
book, however. I'm going to walk through actual code, explain the reasoning behind what I'm doing, the alternatives for the choices
I made as implemented by other products and the implications of those choices. 

I'm going to implement the storage engine in C, since that will make it the most broadly applicable, and because I enjoy writing code
in C. I'm usally writing C# code, so this is a great change of pace for me. 

For more theoretical reading, I would recommend:

* Database Internals^[https://www.databass.dev/] - Discuss implementation details of database, both storage related and in the context
  of distributed sytems. I found it facinating to read and I highly recommend it. It puts you in the role of the implementor and go over
  very important details in an approachable manner.
* Designing data-intensive applications^[https://dataintensive.net/] - Go over similar ground as Database Internals, but it does so from
  a completely different perspective, the user. This is a great book to understand how to make _use_ of this knowledge. I think that these
  books completement each other quite nicely.

I'm going to be talking about the road that was taken in this book. Whenever there is a design choice to make, I'm going to call it out
and reference some of the other options, but I'm going to focus on the decisions that _were_ made, instead of exploring all the various
options.

## What is a storage engine? 

A storage engine in the context of this book is a library or API that are used to store data at a very low level. It is the basic
building block of databases, queuing sytems, distributed architecture and more. You'll usually use a database, rather than a storage
engine directly, because a database would typically offer more capabilities.

For example, LemonGraph, mentioned above, is a graph database that is using LMDB as its storage engine. RavenDB is a document database
that is using Voron as its storage engine. You can use a stroage engine directly, of course, if you need to have access to the lowest
level of the system. RocksDB and LevelDB, for example, are very commonly used as backing store for data for micro services, and LMDB
is also very well suited for this purpose.

Voron, RavenDB's own storage engine, is written in C#, and as such is typically not something that you can just use inside any application.
However, if you are running on .NET or CoreCLR, you'll be able to make use of it directly. 

Storage engines are typically low level, offering primitive operations that you can build on. One of the reasons for this book is that 
if you understand the undelrying primitives, and more importantly, why they exist, you can design far better systems.

A storage engine stores data. The last staement may sounds like tautology, but it is important. The core operations for a storage engine
are:

* `put(key, val)`
* `val get(key)`
* `bool del(key)`
* `iterator iterate();`

Different storage engines implement these in different fashions, providing guarantees about the data and how it is persisted. You _could_
write a storage engine that would simply store the each value as a file in a directory. That _works_ and you have turned the file system
into your storage engine. That has issues, however. File systems tend to do poorly with a lot of small values and there are non trivial
complexities regarding concurrency and atomicity of the data.

The goal of a storage engine is to take ownership of all those details and let the application focus on doing its own thing. Quite often
you'll see applications choosing and using a particular storage engine for the additional behavior it provides. Secondary indexes, data
model that lends itself to merging, etc. 

## What storage engine will we build?

I have written quite a few storage engines at this point. Voron is obviously one of them, but I have also written storage engines modeled 
around LevelDB and Lucene as well as various tailored solutions for specific cases. Depending on the scope of the problem, there is no end 
to the amount of work you can put into a storage engine. There are always more to add and things to improve. A whole team has been working 
on and with Voron for over half a decade and we still have quite a list of featuers that we want to add, for example.

To avoid such slippage in the book, I'm going to define the goals for the storage engine we'll be building up front. I have chosen the name
Gavran (Raven, Voron and Gavran are all the same word, in different languages), because typing "storage engine" all the time is quite tiring.

* Built in C, as it is meant to be embedded in other processes. Should compile with `-Weverything -Werror` (enable _all_ warnings and treat
  them as errors), pass Valgrind properly, etc.
* Transactional and concurrent, you can have real ACID transactions with concurrent transactions that allow readers to continue where there
  are writes without blocking and vice versa.
* Readable and usable, this is hard to define exactly, I admit. I *care* about the API and the readability of the code, to the point where 
  I'll jump through hoops to get more readable and understandable code.
* Give error handling. I'm spoiled, I like my errors to tell me exactly what is going on and what to do about fixing it. That can be hard
  to do properly in C, so we'll have to tackle this as well. And yes, this is important enough to be in the baseline acceptance criteria.
* Performant, it should have high enough perfromance that it will not be an issue. My intent is to get it to be placed around the top of 
  storage engine benchmarks.
* Zero copy, should make it possible to get the data without copying from the database buffers to the application buffers.
* Cross platform, should be able to run on Linux, Windows, ARM devices, Android, iOS, etc. Should run on 32 bits and 64 bits.
* Support complex data structures, such as trees and maps in addition to the usual `get`, `set`, `del` operations.
* Small, I can't commit to a number of lines of code, but I want it *small*. To compare, LevelDB is about 20,000 lines of code and LMDB
  is just over 10,000 lines of code. I aim to be somewhere in the middle. 

That is quite a list of features, but these are the _minimum_ requirements, as far as I'm concerned. My hope is that I can make the journey
there interesting along the way.

## Structure of this book

Building a storage engine is a non trivial task. There are many interlocked pieces that depend on one another. In order to avoid confusion
we are going to be building the engine in stages, adding a single aspect at time. It means that we are going to have to build somethings 
multiple times as we add additional functionality. By the same token, it means that you can see the layers of the engine as it is built.

We'll start each chapter with the a code sample and then discuss what is required to implement it before getting to the actual implementation.
C is a great language for system programming, but for tests and ease of iterating, it isn't ideal. We'll also build a Python wrapper for our
C API which we'll use to test and work with the engine.

I'm going to try to show all the code that builds the engine in the book. You should be able to sit down and type it all and get a working 
storage engine. I'm assuming that you are going to be reading this on an electronic device, not a physical medium, you can probably make things
easier on yourself by using the code from the book's GitHub repository^[https://github.com/ayende/libgavran].

## Structure of the code

I find that it is best to have a well define code contract in a project, and this is no exception. Here are the rules for the code:

* The core API is written in C11 (_not_ C++) and must compile with `-Weverything -Werror` enabled.
* Functions and argument names will use `snake_case` formatting. 
* All integers should have their width specified, use `uint32_t` or `int64_t` instead of `unsigned long long`. The use of `size_t` is permitted, however.
* To the greatest extent possible, Gavran should avoid allocating memory. This isn't truly possible, but it is surprising how far we can push it.

## Error handling

Error handling is a major pain point in C, mostly because you are mostly left on your own. In particular, the use of integer return codes for error handling
loses a _lot_ of context that can be very important. Therefor, Gavran will have the following error handling strategy.

* All functions that can fail must return `bool`, such failures are expected to be rare. A `false` result indicates a failure.
* All such return values _must_ be checked. We'll use `[[nodiscard]]` to ensure this.
* A function that cannot fail can use the return value. 
* The actual error (code, message, location, etc) will be reported using the `push_error()` function, which is covered later in this chapter.
* Callers are expected to check the `bool` return value and then call `get_errors()` to get the actual errors that happened. 
* The errors are actually stored in a thread local buffer, so multi thread usage will not cause an issue.

The idea is fairly basic, we want to ensure that we get _good_ errors, as if we are running in a managed langauge. You can see the API that we're
trying to get in Listing 1.1. The code itself doesn't matter much, I want you to pay attention to the structural elements. In particular, 
`MUST_CHECK`, `push_error()` and `mark_error()`. 

```{caption="Usage of the error API to properly capture error context" .c}
static  MUST_CHECK  bool dance() {
   return true;
}

static MUST_CHECK bool sing(const char* song){
   push_error(ENOTSUP, 
    "Can't sing '%s', I can't recall the lyrics", song);
   return false;
}

static MUST_CHECK bool action(){
   if(!sing("Fernando")){
      mark_error();
      return false;
   }
   if(!dance()){
      mark_error();
      return false;
   }
   return true;
}

int main () {
   if(!action())
      print_all_errors();
   return 0;
}
```

Executing the code in Listing 1.1 will give us the output shown in Listing 1.2. It should be familiar, because this is the track trace of the error.
This API uses `push_error()` to push an error that happened, along with its context, as you can see in the `sing` function. We use `mark_error()` if
we got an error and there is no additional context to add (but we want to record the stack trace). The result is an API that is no more tedious than
the usual C error handling, but give us _good_ errors.

```{caption="The output of the code in Listing 1.1"}
sing()   - src/main.c:14  -  95 - Can't sing 'Fernando',
                                  I can't recall the lyrics
action() - src/main.c:20  -  95 - ...
```

> **Skippable section**
>
> The rest of the section goes in depth into the error handling strategy. You already know how to _use_ it, so if you want to jump directly into
> the storage engine internals, feel free to skip this part. I want to go over every line of code, which means that I also have to cover the 
> uninteresting (yet crucial) parts. More importantly, I want to discuss my reasoning and approach in these areas, because they end up having a
> major impact on the project as a whole.

The `MUST_CHECK` macro is defined to ensure that the compiler will issue a warning if we ignore the result of the method, ensuring that we always 
handle the exceptional case. 

The internal error handling API is defined in `error.h` file, and is recorded in full in Listing 1.3.

```{caption="The error handling API exposed by error.h" .c}
#pragma once

#include <stdint.h>

#define MUST_CHECK __attribute__((warn_unused_result))

#define mark_error() push_error_again(\
    __FILE__, __LINE__, __func__)
#define push_error(code, format, ...) push_error_internal(\
      __FILE__, __LINE__, __func__, code, format, ##__VA_ARGS__)

#define assert_no_existing_errors() \
   if (get_errors_count()){ \
      push_error(EINVAL, "Cannot call %s when there are unnoticed errors", __func__); \
   } 

__attribute__((__format__ (__printf__, 5, 6)))
void  push_error_internal(const char* file, uint32_t line, 
        const char *func, int32_t code, const char* format, ...);

void  push_error_again(const char* file, 
        uint32_t line, const char *func);

void  print_all_errors(void);

void  clear_errors(void);

const char** 
    get_errors_messages(size_t* number_of_errors);

int*  get_errors_codes(size_t* number_of_errors);
size_t get_errors_count(void);

```

As you can see, `MARK_CHECK` is defined using a compiler attribute. These attribute is supported by GCC and Clang, but one of my state goals is to be
able to run this on cross platform. For the time being, I'm focusing solely on getting the project up and running on Linux using Clang. I'll port it 
first to GCC and then to MSCV on Windows later in this book.

The `mark_error()` and `push_error()` functions are actually macros, with use the `__FILE__`, `__LINE__` and `__func__` builtins to provide additional
context for our "stack trace". The `##__VA_ARGS__` definition is actually GCC specific to allow variadic macros. This actually cause a warning in 
`-Weverything` by Clang and the one warning that I have ignored for the time being. 

> **Forcing error handling**
>
> Beyond just using `MUST_CHECK`, we also have the `assert_no_existing_errors` macro which will can use to check if the caller has previously
> ignored an error. If that is the case, we'll refuse to go on and error immediately. 
> That has the effect of ensuring that errors are handled, because it will very visibly break stuff if you don't. 

The rest of the API if fairly similar, we provide functions to push an error (with formatting), accessing the errors and then clearing them. You'll 
note that unlike most C APIs, that we are missing quite a bit here. Namely, where is the memory handling? In order to reduce as much as possible the
complexity of the system, I have chosen to avoid memory allocations entirely in the error API. Let's see how this is achieved. Listing 1.4 has 
the key details. 

```{caption="Error handling implementation, thread local state for the win" .c}
#include <stdio.h>
#include <string.h>
#include "errors.h" 

#define MAX_ERRORS 64
#define MAX_ERRORS_MSG_BUFFER 2048

_Thread_local static char _messages_buffer[MAX_ERRORS_MSG_BUFFER];
_Thread_local static const char* _errors_messages_buffer[MAX_ERRORS];
_Thread_local static int _errors_messages_codes[MAX_ERRORS];

_Thread_local static size_t _errors_count;
_Thread_local static size_t _errors_buffer_len;
_Thread_local static uint32_t _out_of_memory;
```

We define quite a bit of values as thread local state. The idea is that instead of allocating the memory at the time of an error, we'll allocate the memory 
at the time we create the thread. This is done for us automatically, so by the time we get to actually recording an error, we don't have to fear most failure
modes.

The `_messages_buffer` is a 2KB buffer that is used to store the actual messages for the errors, while the `_errors_messages_buffer` is an array that stores
the relevant message for each of recorded array. In other words, the `_errors_messages_buffer` will always point to a location inside `_messages_buffer`. 
The `_errors_messages_codes`, in turn, is a parallal array of the actual error codes that we got. 

The `_errors_count` value counts the number of errors that are currently recorded while `_errors_buffer_len` counts how much of `_messages_buffer` is in use. 
The size of the buffer is 2KB and the maximum number of errors we can hold is 64. The `_out_of_memory` flag is set if we have too many errors, at which point
we'll start discarding them.

If you'll look at the API in Listing 1.3, you can see that aside from `push_error_internal` and `push_error_again`, all the other functions we offer are about
reading the errors or resetting them. The usual method you'll use is to read the errors and do something with them, then clear them all using `clear_errors()`.
A shorthand for printing to the terminal is provided with `print_all_errors()`.

Let's take a look at the heart of this API, the `push_error()` function, shown in Listing 1.5. The code is a bit dense, but the idea is that we format the
provided value into the `_messages_buffer` and it the result to `_errors_messages_buffer` and `_errors_messages_codes`. Most of this function is just making
sure tha things align properly and that there is enough memory for the final message we generate.

```{caption="The push\\_error implementation" .c}
__attribute__((__format__ (__printf__, 5, 6)))
void push_error_internal(const char* file, uint32_t line, 
    const char *func, int32_t code, const char* format, ...) {
    if(_errors_count + 1 >= MAX_ERRORS)
    {
        // we have no space any longer for errors, ignoring 
        _out_of_memory |= 1;
        return;
    }

    size_t index = _errors_count++;

    _errors_messages_codes[index] = code;

    char* msg = (_messages_buffer + _errors_buffer_len);
    
    size_t avail = MAX_ERRORS_MSG_BUFFER - _errors_buffer_len;
    int chars = snprintf(msg, avail, "%s()", func);
    chars += snprintf(msg + chars, avail - (size_t)chars, 
        "%-*c - %s:%i", 18 - chars,' ', file, line);

    // safe to call immediately, if OOM, will write 0 bytes
    chars += snprintf(msg + chars, avail - (size_t)chars, 
        "%*c - %3i - ", 40 - chars, ' ', code);
    if((size_t)chars == avail){
        goto oom;
    }

  va_list ap; 
  va_start(ap, format);
  chars += vsnprintf(msg + chars, 
        avail - (size_t)chars, format, ap); 
  va_end(ap);

    if ((size_t)chars == avail) {
    goto oom;
    }
    else{
        _errors_buffer_len += (size_t)chars + 1; 
        _errors_messages_buffer[index] = msg;
    }
    return;
oom:
    _out_of_memory |= 2;
    _errors_messages_buffer[index] = 0;
}
```

The code in Listing 1.5 isn't doing any memory allocations. All the writes are happening to memory that has been pre-allocated. That said, the caller may
pass a format string that is expensive to compute. We are assuming that these errors are going to be rare, similar to how we threat exception in managed
languages. 

Now that you have seen how the core of the error API, let's see the rest of the API. Listing 1.6 completes the picture.

```{caption=""}
void push_error_again(const char* file, 
      uint32_t line, const char *func){
    if(!_errors_count)
        return; // no error

    // reuse the previous error code
    push_error_internal(file, line, func, 
      _errors_messages_codes[_errors_count-1], "...");
}

const char** get_errors_messages(size_t* number_of_errors){
     *number_of_errors = _errors_count;
     return (const char**)_errors_messages_buffer;
}

int* get_errors_codes(size_t* number_of_errors){
    *number_of_errors = _errors_count;
     return _errors_messages_codes;
}


void print_all_errors(void) {
    for(size_t i = 0; i < _errors_count; i++){
      printf("%s\n", _errors_messages_buffer[i]);
    }

    if(_out_of_memory){
        const char* msg = 
            "Too many errors, "
            "additional errors were discarded";
        printf("%s (%d)\n", msg, -(int32_t)_out_of_memory);
    }
    clear_errors();
}

void clear_errors(void){
    _out_of_memory = 0;
    memset(_errors_messages_codes, 0, 
        sizeof(int32_t*) * _errors_count);
    memset(_errors_messages_buffer, 0, 
        sizeof(char*) * _errors_count);
    memset(_messages_buffer, 0, 
        _errors_buffer_len);
    _errors_buffer_len = 0;
    _errors_count = 0;
}
```

There really isn't much there, once you see what is going on in `push_error()`, the rest is mostly just expose the details. Even so, this is a very powerful
API, since it allows us to have good error handling with little ceremony. 

You may find it strange that the very first thing that I did when starting a storage engine is to build error handling code, but that is a foundation that 
will serve us for the rest of the project. The fact that we can have good ways to report error can save us _weeks_ of troubleshooting time. And with this
in place, we can now move to our very first real task, working with files. 