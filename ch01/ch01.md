% Building a Storage Engine from Scratch
% Oren Eini
% Hibernating Rhinos

---
header-includes:
    - \lstset{breaklines=true}
    - \lstset{basicstyle=\small\ttfamily}
    - \lstset{extendedchars=true}
    - \lstset{tabsize=2}
    - \lstset{columns=fixed}
    - \lstset{showstringspaces=false}
...

# Introduction

My name is Oren Eini, and I have been working with databases for over 15 years. For the past decade or so, I have been
involved in the creation of [RavenDB^[https://ravendb.net), a distributed document database. I have also been 
involved in building other systems of similar nature such as queuing infrastructure, a distributed hash table and the like. 

It turns out that at the bottom of the stack, you need a _good_ way to store data to disk. I have looked into many ways
to handle this, from simply writing XML or JSON file to a folder to utilizing existing software and libraries such as
SQLite, LMDB, Esent, LevelDB and RocksDB. For quite some time, it has been my hobby to read through any storage engine
that I came across. And there have been many of those.

This is a book that is going to cover how to build a database, from the ground up. We aren't going to be talking about
the appropriate schema or what constraints to use. We are going to build the low level infrastructure that you need in
order bo _build_ databases. In particular, I want to walk through all the details of building a production quality 
storage engine with full ACID^[Atomic, consistent, isolated, durable] properties and real transactions.

> **Assumptions about the reader**
>
> I'm writing this book for fun, because it kept bouncing in my head and I really need to get it out so I can sleep.
> The target audience are developers who have at least a passing with system level programming and the C language.
> I'm not going to find the most esoteric pieces of C to use, but you are expected to understand the C, native 
> memeory, data structures and the like. 
>
> I'll reference other resources at times for additional materials, but I'm going to focus on the storage
> engine rather than reiterating existing knowledge.

I have written detailed reviews into many of them. The following list contains some of those review, note that they are
usually spread over to multiple posts in a series. I went through the code base and wrote my notes as they occurced to me.

* CouchDB^[https://ayende.com/blog/posts/series/3607/reading-erlang] - Document database written in Erlang.
* LMDB^[https://ayende.com/blog/posts/series/162754/reviewing-lightning-memory-mapped-database-library]- Low level 
  storage engine library written in C.
* LevelDB^[https://ayende.com/blog/posts/series/161410/reviewing-leveldb] - Low level storage engine library from
  Google. The basis for RocksDB from Facebook. 
* FASTER^[https://ayende.com/blog/posts/series/184225-A/reviewing-faster] - Low level storage engine from Microsoft.
* LemonGraph^[https://ayende.com/blog/posts/series/184066-C/reading-the-nsas-codebase] - Graph database by the NSA.
* Noise^[https://ayende.com/blog/posts/series/178497/reviewing-noise-search-engine] - Full text search engine in Rust.
* Resin^[https://ayende.com/blog/posts/series/178945/reviewing-resin] - Document database written in C#.
* Sled^[https://ayende.com/blog/posts/series/186753-A/reviewing-sled] - Low level storage engine written in Rust.

I've also written my own storage engine library, based on the things that I have learned from these reviews, experience
in the field and the needs of RavenDB itself. This storage engine is called Voron, we started working on that in 2013
and has switched all of RavenDB to it completely by late 2015. Voron has been powering all RavenDB production systems 
for years, with very little trouble.

Voron is written in C# was built to be _high_ performance and has been able to achieve impressive benchmark numbers. 
We have also built Voron to serve RavenDB and adapted RavenDB to best utilize Voron. That has been a very benefitical 
relationship for us because it allowed us to make certain that the operations we needed had as little cost as possible. 

## Why this book?

I have written extensively about database building in the past. Again, each of these links is to a series of posts that describe
certain aspects of building a database. 

* The Guts n' Glory of Database Internals^[https://ayende.com/blog/posts/series/174337/the-guts-n-glory-of-database-internals] -
  A series of posts that walks through the challenges involved in building a proper database and recommendations on how to deal
  with them.
* Database build 101^[https://ayende.com/blog/posts/series/175041/database-building-101] - Series of posts detailing how to build
  a graph database. 
* Voron internals^[https://ayende.com/blog/posts/series/175073/voron-internals] - How Voron works behind the curtain.
* Low level Voron optimizations^[https://ayende.com/blog/posts/series/176961/low-level-voron-optimizations] - Discuss some
  of the tricks we use to get Voron to be faster.

All of my work in Open Source. You can find RavenDB's code at our GitHub repository^[https://github.com/ravendb/ravendb] and the 
Voron project is the `src/Voron` directory there.

I wanted to write this book to not only present my (and my team's) work but to also have walk you through how you can
build a storage engine library from nothing. Rather than just throw code over the wall, or have stuff that is used only for RavenDB,
I wanted _explain_ things and tell the complete story. I'm also using this project as a way to test out some interesting expriments
that came to mind do proper research into additional avenues and in general build something cool.

I hope that you'll find this book interesting, it has certainly been interesting to write it. This isn't meant to be a theoretical
book, however. I'm going to walk through actual code, explain the reasoning behind what I'm doing, the alternatives for the choices
I made as implemented by other products and the implications of those choices. 

I'm going to implement the storage engine in C, since that will make it the most broadly applicable, and because I enjoy writing code
in C. I'm usally writing C# code, so this is a great change of pace for me. 

For more theoretical reading, I would recommend:

* Database Internals^[https://www.databass.dev/] - Discuss implementation details of database, both storage related and in the context
  of distributed sytems. I found it facinating to read and I highly recommend it. It puts you in the role of the implementor and go over
  very important details in an approachable manner.
* Designing data-intensive applications^[https://dataintensive.net/] - Go over similar ground as Database Internals, but it does so from
  a completely different perspective, the user. This is a great book to understand how to make _use_ of this knowledge. I think that these
  books completement each other quite nicely.

I'm going to be talking about the road that was taken in this book. Whenever there is a design choice to make, I'm going to call it out
and reference some of the other options, but I'm going to focus on the decisions that _were_ made, instead of exploring all the various
options.

## What is a storage engine? 

A storage engine in the context of this book is a library or API that are used to store data at a very low level. It is the basic
building block of databases, queuing sytems, distributed architecture and more. You'll usually use a database, rather than a storage
engine directly, because a database would typically offer more capabilities.

For example, LemonGraph, mentioned above, is a graph database that is using LMDB as its storage engine. RavenDB is a document database
that is using Voron as its storage engine. You can use a stroage engine directly, of course, if you need to have access to the lowest
level of the system. RocksDB and LevelDB, for example, are very commonly used as backing store for data for micro services, and LMDB
is also very well suited for this purpose.

Voron, RavenDB's own storage engine, is written in C#, and as such is typically not something that you can just use inside any application.
However, if you are running on .NET or CoreCLR, you'll be able to make use of it directly. 

Storage engines are typically low level, offering primitive operations that you can build on. One of the reasons for this book is that 
if you understand the undelrying primitives, and more importantly, why they exist, you can design far better systems.

A storage engine stores data. The last staement may sounds like tautology, but it is important. The core operations for a storage engine
are:

* `put(key, val)`
* `val get(key)`
* `bool del(key)`
* `iterator iterate();`

Different storage engines implement these in different fashions, providing guarantees about the data and how it is persisted. You _could_
write a storage engine that would simply store the each value as a file in a directory. That _works_ and you have turned the file system
into your storage engine. That has issues, however. File systems tend to do poorly with a lot of small values and there are non trivial
complexities regarding concurrency and atomicity of the data.

The goal of a storage engine is to take ownership of all those details and let the application focus on doing its own thing. Quite often
you'll see applications choosing and using a particular storage engine for the additional behavior it provides. Secondary indexes, data
model that lends itself to merging, etc. 

## What storage engine will we build?

I have written quite a few storage engines at this point. Voron is obviously one of them, but I have also written storage engines modeled 
around LevelDB and Lucene as well as various tailored solutions for specific cases. Depending on the scope of the problem, there is no end 
to the amount of work you can put into a storage engine. There are always more to add and things to improve. A whole team has been working 
on and with Voron for over half a decade and we still have quite a list of featuers that we want to add, for example.

To avoid such slippage in the book, I'm going to define the goals for the storage engine we'll be building up front. I have chosen the name
Gavran (Raven, Voron and Gavran are all the same word, in different languages), because typing "storage engine" all the time is quite tiring.
Gavran is also a word in Hebrew that can be roughtly translated as either very strong and powerful or has overcame challanges, so that was 
a nice bonus.

* Built in C, as it is meant to be embedded in other processes. Should compile with `-Weverything -Werror` (enable _all_ warnings and treat
  them as errors), pass Valgrind properly, etc.
* Transactional and concurrent, you can have real ACID transactions with concurrent transactions that allow readers to continue where there
  are writes without blocking and vice versa.
* Readable and usable, this is hard to define exactly, I admit. I *care* about the API and the readability of the code, to the point where 
  I'll jump through hoops to get more readable and understandable code.
* Give error handling. I'm spoiled, I like my errors to tell me exactly what is going on and what to do about fixing it. That can be hard
  to do properly in C, so we'll have to tackle this as well. And yes, this is important enough to be in the baseline acceptance criteria.
* Performant, it should have high enough perfromance that it will not be an issue. My intent is to get it to be placed around the top of 
  storage engine benchmarks.
* Zero copy, should make it possible to get the data without copying from the database buffers to the application buffers.
* Cross platform, should be able to run on Linux, Windows, ARM devices, Android, iOS, etc. Should run on 32 bits and 64 bits.
* Support complex data structures, such as trees and maps in addition to the usual `get`, `set`, `del` operations.
* Small, I can't commit to a number of lines of code, but I want it *small*. To compare, LevelDB is about 20,000 lines of code and LMDB
  is just over 10,000 lines of code. I aim to be somewhere in the middle. 

That is quite a list of features, but these are the _minimum_ requirements, as far as I'm concerned. My hope is that I can make the journey
there interesting along the way.

## Structure of this book

Building a storage engine is a non trivial task. There are many interlocked pieces that depend on one another. In order to avoid confusion
we are going to be building the engine in stages, adding a single aspect at time. It means that we are going to have to build somethings 
multiple times as we add additional functionality. By the same token, it means that you can see the layers of the engine as it is built.
We'll close each chapter with unit tests showing the work that was done.

We'll start each chapter with the a new feature to build, show the API that is required to build it and sample and then discuss what is required 
and the choices that were made at the API level. Then we'll dive to the actual implementation details. There is going to be a _lot_ of (annotated)
code in this book, as well as deep dives into some aspects of building a storage engine.

I'm going to try to show all the code that builds the engine in the book. You should be able to sit down and type it all and get a working 
storage engine. I'm assuming that you are going to be reading this on an electronic device, not a physical medium, you can probably make things
easier on yourself by using the code from the book's GitHub repository^[https://github.com/ayende/libgavran].

## Structure of the code

I find that it is best to have a well define code contract in a project, and this is no exception. Here are the rules for the code:

* The core API is written in C11 (_not_ C++) and must compile with `-Weverything -Werror` enabled. Note that while we want _operating system_ portability
  there is no hard requirement for _compiler_ portability. I'm going to use some features that are available for GCC and Clang, but not MSVC and likely 
  not in other C compilers^[These features are going to significantly impact the code quality and there are workarounds for other compilers, I'm just not
  going to bother with these. Building the storage engine is large enough task as it is.].
* Functions and argument names will use `snake_case` formatting. 
* All integers should have their width specified, use `uint32_t` or `int64_t` instead of `unsigned long long`. The use of `size_t` is permitted, however.
* To the greatest extent possible, Gavran should avoid allocating memory. This isn't truly possible, but it is surprising how far we can push it.
* Pointers should be using `restrict` whenever possible.
* Functions using more than a few arguments should take a `struct`, instead.
* The code is automatically formatted using `clang-formatter` with the Mozilla's style code and column limit set to 70 (so it will fit properly in the book).
* Naming convention for the function is `<component>_<action>_<object>`. So we'll have `platform_sync_file` or `pager_allocate_page`.
* The storage engine is going to be defined in layers, which each layer leaning on the previous one and isolating it. We'll avoid chatter that cross layer
  boundary.

One of the greatest weakness of C is in its error handling and the capabilities you have for managing resources. 
That is left entirely to the developer to manage. And while _technically_ you can write bug free code, it is a _hard_ task.

I spent most of my professional career working in managed languages, mostly C# and .NET. That means that I look at the sheer amount of ceremony to write
proper code in C and I cringe. In order to enable both good coding practices and maintain my sanity, I have defined a set of baseline infrastucture that
is going to help me write better and safer code. 

I want to first illustrate that with the sample code in Listing 1.1 and then discuss exactly what is going on here. In Listing 1.1 we have a function that
reads 4 bytes from a file and convert these bytes into a string format. There are a bunch of things that can go wrong in this kind of code. We may fail 
to open the file, or allocate memory. We may fail to read from the file, or not read enough, etc. We need to remember to always close the file descriptor
and we should always free the allocated buffer on error, but not if the function completed successfully.

That is a _lot_ of state to manage, and it makes it easy to make mistake. Listing 1.1 shows a fucntion that has to deal coordination across merely two
resources. If we have to deal with more, the situation worsen. Take a look at the code and then we'll break down everything that is new here.

```{caption="Using defer macro for deterministic cleanup of resources" .c}
static result_t
read_all(int fd, size_t size, void* buffer)
{
  size_t read_bytes = 0;
  while (read_bytes < size) {
    ssize_t cur =
      read(fd, (char*)buffer + read_bytes, size - read_bytes);

    if (cur <= 0) {
      failed(errno,
             "Failed to read requested data",
             with(read_bytes, "%zu"),
             with(cur, "%zd"),
             with(size, "%zu"));
    }
    read_bytes += (size_t)cur;
  }
  success();
}

static result_t
read_int_to_str_buffer(const char* path, char** buffer)
{
  int fd = open(path, O_RDONLY, 0);
  if (fd == -1) {
    failed(errno, "Unable to open file", with(path, "%s"));
  }
  defer(close, &fd);

  char* tmp_buf = malloc(128);
  ensure(tmp_buf, msg("Unable to allocate buffer"));
  size_t cancel_defer;
  try_defer(free, buffer, cancel_defer);

  int val;
  ensure(read_all(fd, sizeof(int), &val), with(path, "%s"));

  int chars = sprintf(tmp_buf, "%d", val);
  *buffer = realloc(tmp_buf, (size_t)chars + 1);
  ensure(*buffer, msg("Failed to decrease the buffer size?!"));

  cancel_defer = 1;
  success();
}
```

If we call `read_int_to_str_buffer("/path/to/directory", &buffer);` function (note that we use a directory, not a file) we'll have the following sequence of
events:

* The directory is opened.
* The memory is allocated.
* The read fails.
* Everything gets cleaned up.
* Caller can call `errors_print_all()` to get the information shown in Listing 1.2.

```{caption="Error output from running read_int_to_str_buffer on a directory" .c}
read_all()                - src/main.c:33  -  21 (Is a directory      ) - Failed to read requested data, read_bytes = 0, cur = -1, size = 4
read_int_to_str_buffer()  - src/main.c:55  -  22 (Invalid argument    ) - read_all(fd, sizeof(int), &val), path = test
```

Listing 1.1 shows a _lot_ of stuff that won't show up in a C tutorial.

Error handling:
* `result_t`
* `failed` and `with`
* `ensure` and `msg`
* `success`

Resource management:

* `defer` 
* `try_defer`

Together, these form the basis of error handling and resource management strategy for the upcoming storage engine. Let's see what each of them does and then
we'll discuss how they all fit together. I'll discuss how they are _implemented_ in the next section.

You'll note that we get a _lot_ of information here. This is _almost_ a stack trace and the richness of exceptions, without any of the costs associated with it.
We are also able to print out the current state at the time of the error, which can be _invaluable_ for debugging and troubleshooting.
Let's get familiar with the tools we have to provide this.

The `result_t` type is the return type for any function that may fail. It is an opaque type that is set to zero on failure and non zero of success. Using C
usual rules, we can use boolean operations on this value to test for success / fail. A key factor about `result_t` is that it is a value that _must_ be observed. 
In other words, ignoring this value will result in a compiler error using our settings. 

The `failed`, `with` and `msg` macros are used to report errors. A call to `failed` will exit the function with a fail status and the parameters to `failed` 
will be used to generate an error message. It is expected that multiple methods will fail in a sequence, and the infrastructure is setup to handle this.
You can use `msg` to provide some additional information and `with` to provide more context, such as local parameters. This is how we can see how much the 
`read_all` was asked to scan, for example.

The `success` macro is the other side, it exists from a function that has completed its job successfully. The `ensure` macro, on the other hand, will test the
provided expression and will exit with an error from the current function if the provided expression is false. 
You'll notice that in the `read_int_to_str_buffer` we do almost no explicit error checking, this is done for us by the `ensure`. The one case where we do have
to test a value explicitly is when we are dealing with system code, when we are checking if the `open` was successful, then calling `failed` with the error.

For this reason, we are going to bury the system calls and other things that can fail as deeply as possible, so we'll be able to use the `ensure` macro to reduce
the amount of explicit error handling we have to go through.

This style of error handling is problematic in C, because we can't just _exit_ a function, we have to do cleanup first. In pretty much any C project of any 
complexity you'll see `goto` used for this aproach, to try to centralized error handling in the function. This resource management strategy is brittle and hard
to use. Instead, we offer `defer` and `try_defer`. 

These are based on `__attribute__((__cleanup__))` extension for both Clang and GCC. This is a way to instruct the compiler to call a method when a variable
goes out of scope. We use this approach to ensure that we have proper cleanup. For example, making sure that regardless of how we exit the `read_int_to_str_buffer`
function, the file will be properly closed.
In `read_int_to_str_buffer` there is another resource usage pattern that we need to deal with. The buffer we allocate to hold the string must be released in the 
function fails, but must _not_ be release if the function is successful. 

In order to handle these, we have `try_defer`, it accepts an additional argument, of a variable that it will monitor when called. If that variable is set, we'll
cancel the deferral call. In other words, we'll be able to cancel the deferrable command in the successful path, but until that point, we'll have automatic 
cleanup. 

These two approaches together leads to a much cleaner codebase. And now, let's see how I actually built those. 

## Infrastructure implementation

I'll start with the `defer` commands, because they are the smaller of the two. Most they just give good syntax over the compiler behavior. You can see how they
are implmented in Listing 1.3.

```{caption="Implementing the defer macros" .c}
struct cancel_defer {
  void *target;
  size_t *cancelled;
};

void defer_close(struct cancel_defer *cd);

static inline void defer_free(struct cancel_defer *cd) {
  if (cd->cancelled && *cd->cancelled)
    return;
  void *p = *(void **)cd->target;
  free(p);
}

#define try_defer(func, var, cancel_marker)                                    \
  struct cancel_defer _##__LINE__                                              \
      __attribute__((__cleanup__(defer_##func))) = {                           \
          .target = var, .cancelled = &cancel_marker};                         \
  (void)_##__LINE__;

#define defer(func, var)                                                       \
  struct cancel_defer __DEFER__##__LINE__                                      \
      __attribute__((__cleanup__(defer_##func))) = {.target = var};            \
  (void)__DEFER__##__LINE__;
```

Listing 1.3 shows what goes on behind the scenes. We setup a call to `defer_<function>`, which must be provided by the caller. In practice, this is usually
not an issue, the number of unique calls is low, so adding this once for a few data types is very easy. Then we just rely on the compiler to do its magic.
The last line of the macros is meant to avoid a warning about unused varaible, since I'm compliing will all warnings enabled.

> **Skippable section**
>
> The rest of the section goes in depth into the error handling strategy. You already know how to _use_ it, so if you want to jump directly into
> the storage engine internals, feel free to skip this part. I want to go over every line of code, which means that I also have to cover the 
> uninteresting (yet crucial) parts. More importantly, I want to discuss my reasoning and approach in these areas, because they end up having a
> major impact on the project as a whole.

Moving on to the error handling, we actually have a few levels inside the error handling that we need to consider. There are a few functions that you can
use to _report_ errors:

* `errors_push` will create a new error and `errors_append_message` will add additional context to it.
* `errors_assert_empty` can check if we were called with errors that haven't been observed yet. In this case, we immediately return an error to avoid
  error propogation. 
* The functions `errors_get_count`, `errors_get_codes`, `errors_get_messages` will all inspect the existing errors if you need to do something programatically
  to them. 
* The function `errors_clear` marks them as observed and `errors_print_all` will print them errors to the console and clear them.

On top of these, several macros and being able to rely on `defer` gives us the right behavior. The rest of the work is done mostly by macros, so let's see them in Listing 1.4.

```{caption="The Error API declaration" .c}
#pragma once

#include <stdbool.h>
#include <stdint.h>
#include <stdlib.h>

typedef struct operation_result op_result_t;

#define result_t __attribute__((warn_unused_result)) op_result_t*

#define failed(CODE, MSG, ...)                                       \
  push_error(CODE, MSG);                                             \
  __VA_ARGS__;                                                       \
  return (op_result_t*)(void*)0;

#define failure(CODE, MSG, ...)                                      \
  (push_error_internal(__FILE__, __LINE__, __func__, CODE, MSG),     \
   ##__VA_ARGS__,                                                    \
   (op_result_t*)(void*)0)

#define ensure(CALL, ...)                                            \
  if (!CALL) {                                                       \
    return failure(EINVAL, #CALL, ##__VA_ARGS__);                    \
  }

#define msg(MSG) errors_append_message(MSG)

#define with(EXPR, FORMAT)                                           \
  errors_append_message(", " #EXPR " = " FORMAT, EXPR)

#define success() return (op_result_t*)(void*)1

#define errors_push(CODE, FMT_STR, ...)                              \
  errors_push_new(                                                   \
    __FILE__, __LINE__, __func__, CODE, FMT_STR, ##__VA_ARGS__);

#define errors_assert_empty()                                        \
  if (get_errors_count()) {                                          \
    push_error(EINVAL,                                               \
               "Cannot call %s when there are unnoticed errors",     \
               __func__);                                            \
  }

__attribute__((__format__(__printf__, 5, 6))) op_result_t*
errors_push_new(const char* file,
                uint32_t line,
                const char* func,
                int32_t code,
                const char* format,
                ...);

__attribute__((__format__(__printf__, 1, 2))) op_result_t*
errors_append_message(const char* format, ...);

void
errors_print_all(void);

void
errors_clear(void);

const char**
errors_get_messages(size_t* number_of_errors);

int*
errors_get_codes(size_t* number_of_errors);

size_t
errors_get_count(void);

```

A lot is going on in Listing 1.4, allow me to go through this one at a time. We start by forward declaring `op_result_t`. We don't actually _have_ such 
a struct. We'll be using `result_t`, which is a pointer to an `op_result_t` with the `warn_unused_result` enabled. Because we created a new type, the 
compiler will warn us about trying to assign it to other data types and the `warn_unused_result` will make sure taht we can't ignore it.

The `failed` and `ensure` macros are variadic. They are meant to be used with the `with` and `msg` macros. The idea is that we'll first push a new
error and then be able to provide more context to it. In the case of `ensure`, if the expression is true, there is no cost and nothing is actually
being run.

The `errors_push` macro just call to `errors_push_new` with context using`__FILE__`, `__LINE__` and `__func__` builtins to provide additional
context for our "stack trace". The rest of the functions aren't anything special. The `errors_push` macro is also variadic and uses a GCC extention 
to work (`##__VA_ARGS__`). 
That generates a warning in Clang, so I had to suppress it.

> **Forcing error handling**
>
> Beyond just using `result_t`, we also have the `errors_assert_empty` macro which will can use to check if the caller has previously
> ignored an error. If that is the case, we'll refuse to go on and error immediately. 
> That has the effect of ensuring that errors are handled, because it will very visibly break stuff if you don't. 

You'll note that unlike most C APIs, that we are missing quite a bit here. Namely, where is the memory handling? In order to reduce as much as possible the
complexity of the system, I have chosen to avoid memory allocations entirely in the error API. Let's see how this is achieved. Listing 1.4 has 
the key details. 

```{caption="Error handling implementation, thread local state for the win" .c}
#include "errors.h"
#include <assert.h>
#include <stdio.h>
#include <string.h>

#define MAX_ERRORS 64
#define MAX_ERRORS_MSG_BUFFER 2048

_Thread_local static char _messages_buffer[MAX_ERRORS_MSG_BUFFER];
_Thread_local static const char* _errors_messages_buffer[MAX_ERRORS];
_Thread_local static int _errors_messages_codes[MAX_ERRORS];

_Thread_local static size_t _errors_count;
_Thread_local static size_t _errors_buffer_len;
_Thread_local static uint32_t _out_of_memory;
```

We define quite a bit of values as thread local state. The idea is that instead of allocating the memory at the time of an error, we'll allocate the memory 
at the time we create the thread. This is done for us automatically, so by the time we get to actually recording an error, we don't have to fear most failure
modes.

The `_messages_buffer` is a 2KB buffer that is used to store the actual messages for the errors, while the `_errors_messages_buffer` is an array that stores
the relevant message for each of recorded array. In other words, the `_errors_messages_buffer` entires will always point to a location inside `_messages_buffer`. 
The `_errors_messages_codes`, in turn, is a parallal array of the actual error codes that we got. This in memory layout is shown in Figure 1.1.

![The actual error messages are stored in a thread loca buffer that is pre-allocated](/ch01/img01.png)

The `_errors_count` value counts the number of errors that are currently recorded while `_errors_buffer_len` counts how much of `_messages_buffer` is in use. 
The size of the buffer is 2KB and the maximum number of errors we can hold is 64. The `_out_of_memory` flag is set if we have too many errors, at which point
we'll start discarding them. I find that very unlikley to actually happen. Very deep stack traces are unusual in C so I don't expect will need even that much.

If you'll look at the API in Listing 1.3, you can see that aside from `errors_push_new` and `errors_append_message`, all the other functions we offer are about
reading the errors or clearing them. The usual method you'll use the `ensure` / `failed` macros to bubble the errors to the callers until we get sometwhere where
we can make a decision about the issue. The caller will need to read the errors and make a determination and then clear the error state and prepare for the next operation.
A shorthand for printing to the terminal is provided with `print_all_errors()`.

Let's take a look at the heart of this API, the `errors_push_new()` function, shown in Listing 1.5. The code is a bit dense, but the idea is that we format the
provided value into the `_messages_buffer` and it the result to `_errors_messages_buffer` and `_errors_messages_codes`. Most of this function is just making
sure tha things align properly and that there is enough memory for the final message we generate.

```{caption="The errors\\_push\\_new implementation" .c}
__attribute__((__format__(__printf__, 5, 6))) op_result_t*
errors_push_new(const char* file,
                uint32_t line,
                const char* func,
                int32_t code,
                const char* format,
                ...)
{
  if (_errors_count + 1 >= MAX_ERRORS) {
    // we have no space any longer for errors, ignoring
    _out_of_memory |= 1;
    return 0;
  }

  size_t index = _errors_count++;

  _errors_messages_codes[index] = code;

  char* msg = (_messages_buffer + _errors_buffer_len);

  size_t avail = MAX_ERRORS_MSG_BUFFER - _errors_buffer_len;
  int chars = snprintf(msg, avail, "%s()", func);
  chars += snprintf(msg + chars,
                    avail - (size_t)chars,
                    "%-*c - %s:%i",
                    25 - chars,
                    ' ',
                    file,
                    line);
  // safe to call immediately, if OOM, will write 0 bytes
  char stack_buffer[128];
  char* err_core_str = strerror_r(code, stack_buffer, 128);

  chars += snprintf(msg + chars,
                    avail - (size_t)chars,
                    "%*c - %3i (%-20s) - ",
                    40 - chars,
                    ' ',
                    code,
                    err_core_str);
  if ((size_t)chars == avail) {
    goto oom;
  }

  va_list ap;
  va_start(ap, format);
  chars += vsnprintf(msg + chars, avail - (size_t)chars, format, ap);
  va_end(ap);

  if ((size_t)chars == avail) {
    goto oom;
  } else {
    _errors_buffer_len += (size_t)chars + 1;
    _errors_messages_buffer[index] = msg;
  }
  return 0;
oom:
  _out_of_memory |= 2;
  _errors_messages_buffer[index] = 0;
  return 0;
}
```

The code in Listing 1.5 isn't doing _any_ memory allocations. All the writes are happening to memory that has been pre-allocated. That said, the caller may
pass a format string that is expensive to compute. We are assuming that these errors are going to be rare, similar to how we threat exception in managed
languages. The design of the system attempts to ensure that error handling should be robust and that having good error reporting doesn't come at the expense
of performance.

Now that you have seen how the core of the error API, let's see the rest of the API. Listing 1.6 completes the picture.

```{caption=""}
op_result_t*
errors_append_message(const char* format, ...)
{
  // should always be called with an error
  assert(_errors_count && _errors_buffer_len);
  char* msg = (_messages_buffer + _errors_buffer_len) - 1;
  size_t avail = MAX_ERRORS_MSG_BUFFER - _errors_buffer_len;

  va_list ap;
  va_start(ap, format);
  int chars = vsnprintf(msg, avail, format, ap);
  va_end(ap);
  if ((size_t)chars == avail) {
    *msg = 0; // undo possible overwrite of null terminator
    _out_of_memory |= 2;
  }
  _errors_buffer_len += (size_t)chars;
  return 0; // simply to allow it to be used in comma operator
}


const char**
get_errors_messages(size_t* number_of_errors)
{
  *number_of_errors = _errors_count;
  return (const char**)_errors_messages_buffer;
}
int*
get_errors_codes(size_t* number_of_errors)
{
  *number_of_errors = _errors_count;
  return _errors_messages_codes;
}

void
print_all_errors(void)
{
  for (size_t i = 0; i < _errors_count; i++) {
    printf("%s\n", _errors_messages_buffer[i]);
  }

  if (_out_of_memory) {
    const char* msg = "Too many errors, "
                      "additional errors were discarded";
    printf("%s (%d)\n", msg, -(int32_t)_out_of_memory);
  }
  clear_errors();
}

void
clear_errors(void)
{
  _out_of_memory = 0;
  memset(_errors_messages_codes, 0, sizeof(int32_t*) * _errors_count);
  memset(_errors_messages_buffer, 0, sizeof(char*) * _errors_count);
  memset(_messages_buffer, 0, _errors_buffer_len);
  _errors_buffer_len = 0;
  _errors_count = 0;
}

size_t
get_errors_count()
{
  return _errors_count;
}
```

There really isn't much there, once you see what is going on in `errors_push_new()`, the rest is mostly just expose the details. Even so, this is a very powerful
API, since it allows us to have good error handling with little ceremony. The `errors_append_message` and `errors_push_new` return a `op_result_t*` because they
might be used via the comma operator. That is a done in the `failure` macro, which is currently only called internally. 

You may find it strange that the very first thing that I did when starting a storage engine is to build error handling code, but that is a foundation that 
will serve us for the rest of the project. The fact that we can have good ways to report error can save us _weeks_ of troubleshooting time. And with this
in place, we can now move to our very first real task, working with files. 

## Testing the error API

I intend to close each chapter with a set of unit tests, verifying the functionality of the system that we built so far. Testing C code is a pain, especially
when we get to the more complex scenarios. In order to handle this, I intent to write the tests in Python, where it is much easier. Along the way, that will
serve as a way to ensure that we are able to make proper use of the code in an embedded mode.

