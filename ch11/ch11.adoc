== Pages validation & encryption

You might have caught on by now to the fact that I _don't_ trust the disk. A catastrophic disk failure doesn't happen all that often in normal day to day life, but errors are surprisingly
common. Backblaze is an interesting case study in this regard, because they https://www.backblaze.com/b2/hard-drive-test-data.html[routinely publish failure rates of hard drives]. Some
of which can reach an astonishingly high 2% failure rate in a single year. 

You can expect to have ECC (Error Correcting Codes) even on consumer drives today, and the nature of SSD drives means that they can recover from many minor errors on the fly without any
user visible effects. That isn't a fool proof plan and you can get into situations where https://www.backblaze.com/blog/how-reliable-are-ssds/[a read or write will simply fail]. I'm not
worried too much about _that_, to be honest. Hard errors in reading the data from disks are fine. We can't do better than the underlying hardware will allow us, after all. 

https://research.cs.wisc.edu/adsl/Publications/corruption-fast08.pdf[An Analysis of Data Corruption in the Storage Stack] from NetApp outlines some of the issues that you face at the
hardware level. They found over 400,000(!) failures in a period of under 4 years, 30,000 of them bypassed all attempts to check them and got to the hard of user's software. 
Then there is also the possibility of kernel bugs [https://topic.alibabacloud.com/a/fix-ext4-log-jbd2-bug-ext4-file-system-has-the-following-bug_8_8_31190436.html], this is an English
translation of a Chinese discussion on an EXT4 bug that caused data corruption after (very) heavy writes. 
Even something as silly as having the wrong cables can cause https://changelog.complete.org/archives/9769-silent-data-corruption-is-real[data corruption issues]. 
As you can imagine, the topic is quite interesting and there is a lot of 
https://perspectives.mvdirona.com/2012/02/observations-on-errors-corrections-trust-of-dependent-systems/[information available] on the topic. Including many academic studies. 
The https://queue.acm.org/detail.cfm?id=1866298[Keeping Bits Safe: How Hard Can It Be?] does a good job of summarizing the main challenges and issues.

If you haven't dropped this book and started to verify your backup routines, you can clearly see why there is a need to verify the data from the disk at the application level. Gavran
is already doing some of that, we are using Blake2B to validate the data on the WAL when we read it. This ensures that the recovery data operates on known good data. 
What about the data file? The data there is just a subject to the vagaries of hardware and software failures as the WAL. We also need to validate this information, no? The question is how?

=== Hashing data on commit

We validate the WAL using Blake2B hash, and I see no reason to change that for the data file. If you'll recall, the structure of the page metadata is explicitly setup to we reserved 32 bytes
for additional uses. That is specifically for this purpose. The `page_metadata_t` is shown in Listing 11.1.

[source]
.Listing 11.1 - impl.h - The new structure of the `page_metadata_t`, ready for hashing implementation
----
include::./code/impl.h[tags=page_metadata_t]
----
<1> Used for encryption (shown later in this chapter). The `nonce` is 12 bytes and `mac` is 16 bytes.
<2> Used to hold the hash of a page, using Blake2B, the size of the `page_hash` is 32 bytes.

The idea is that we'll hash each page when we commit the transaction and then we'll validate them on read. Let's look at what we need to do in order to hash the pages. There is one location
that we can do this on, the `txn_commit` function. I added a call to `txn_hash_pages` just before we call `wal_append`. At this point, there are no more changes possible in the transaction
and we can compute the hash for the pages. This is shown in Listing 11.2.

[source]
.Listing 11.2 - txn.c - Compute cryptographic hash for each of the pages in the transaction
----
include::./code/txn.c[tags=txn_hash_pages]
----
<1> Allocate a buffer to hold all the modified pages, we are going to be modifying (metadata) pages while we are computing the hashes, so we can't iterate over the hash directly.
<2> Copy the modified page numbers to a stable location, which won't change as we call `txn_modify_metadata` on the values.
<3> Call `txn_modify_metadata` on each of the modified pages, but compute the hash of only the pages that aren't metadata pages.
<4> Compute the hash on all the metadata pages in the transaction.

There are a few interesting things going on in `txn_hash_pages`. We iterate over all the modified pages in the transaction, but we can't use the hash table directly for that. The problem 
is that we need to modify the metadata of pages (because that is where we'll store the hash). Modifying the metadata page may change the hash table that holds the modified pages, so we
can't simply iterate on it. Instead, we allocate a buffer to hold the modified pages and copy the values from the hash table to the `modified_pages` buffer.

The next step is to run through the `modified_pages` and call `txn_modify_metadata` on each one of them. We have a separate process for metadata pages than normal pages, but we must call
`txn_modify_metadata` on all the pages that are in the transaction. A metadata page may or may not have been modified by the transaction, but by calling `txn_modify_metadata` on all the 
pages, we ensure that all the metadata pages that are relevant to the transaction are going to be in the transaction's modified pages hash table.

We call `txn_hash_page` on all the non metadata pages, but why do we need to treat the metadata pages differently? Normal pages have their hash stored in the metadata page, but for metadata
pages, the hash is stored _inside_ the page itself. Another issue is that as we are running through the pages, we update their metadata pages, so we must process the metadata pages after
we have finished processing all of the regular pages.

The last loop in `txn_hash_pages` iterate over the hash table again, why not use the `modified_pages` in this case? Because we may have _new_ modified metadata pages that were added to the
hash table because of the call to `txn_modify_metadata`. We handle metadata hashing in a slightly differently than the way we handle normal pages, let's look at 
Listing 11.3 to see the details of how this works.

[source]
.Listing 11.3 - txn.c - Hashing of a single page or metadata page
----
include::./code/txn.c[tags=txn_hash_page]
----

The code in `txn_hash_page` is straightforward, get the modified page from the hash table and compute the hash of the page data on the `page_hash` of the relevant metadata entry.
The only interesting thing here is that we are playing with how to do hashing for the metadata pages.

.Hashing usage for metadata pages
image::{img-src}/fig22.png[]

We can't hash the whole metadata page, because the `page_hash` for the metadata page actually resides in the metadata page. Instead, we use a small trick. The metadata entry for the 
metadata page itself is always the first item in the metadata page, and we know that its size is 64 bytes. The `page_hash` for the metadata page will then sit on byte positions 32 - 63
in the metadata page. Running the hash on the byte range 64 - 8192 would means that we could detect any changes in the page, but not in the first 32 bytes. 

We can use a more complex hashing scheme, as we have done in Listing 9.1, but we don't have to. Given that we need to include the first 32 bytes of the metadata page in the hash, we 
can simply use these as a _keyed_ hash. In order to produce the same output, both the first 32 bytes and the other 8,128 bytes would have to be the same. That is the only thing that 
we actually care about, and it reduce the amount of code we have to write significantly.

We are computing the hashes for all of the modified pages before we write them to the WAL, which means that the hash changes will get to the WAL as everything does. I love this approach
because it is simply layering behaviors on top of existing behaviors, in orthogonal manner. 

=== Validating the WAL changes

What about _verifying_ the hashes? After all, that is pretty much what they we created them for, no? The first place that we'll verify the hashes is after the WAL recover is complete. 
We just applied a whole bunch of diffs to the system, let's make sure that all the pages that we touched are valid according to their hash.

I've added a call to `wal_validate_recovered_pages` after the call to `wal_complete_recovery` in `wal_recover`. This is meant to be a last check to make sure that everything we wrote is
fine and working. Listing 11.4 shows how we do the actual validation. We register all the modified pages in the `modified_pages` hash table, which allows us to then scan through 
all those modified pages and validate their hashes. You can see how that works in Listing 11.4.

[source]
.Listing 11.4 - txn.c - Verifying the hashes for all the pages touched by the WAL recovery
----
include::./code/wal.c[tags=wal_validate_recovered_pages]
----
<1> Hash the page into a local buffer.
<2> Compare the expected hash with the actual hash, if they are identical, we are done.
<3> If they _aren't_ identical, we need to check if the expected has is all zeroes and that the page is also all zeroes.
<4> We tried everything, but there is a discrepancy between the actual hash and the expected hash, report the error.

.What about freed pages?
====
A freed page is one that was modified and then freed. From the point of view of the WAL and the lowest levels of Gavran, there is no different between a freed page and a modified
one. A freed page is simply one that has its free space bitmap cleared and its value was set to zeroes. As such, it will have a _valid_ hash for a zeroed buffer.
====

The actual validation is done by `txn_validate_page_hash`, which gets the page to validate and the expected hash for the page. You can see that the actual hashing is done by 
`txn_hash_page`, but there is a lot more work to be done in this function. Aside from actual data corruption, we may have a case of hash mismatch if the page is empty and the
expected hash for the page is zeroed. That indicates that this is a page that was never touched. 

With these changes, we now validate, using a cryptographic hash, that the data recovered from the WAL is the data that we meant to write to it. 

[TIP]
.Cryptographic hashing also helps catch bugs
====
The _reason_ that we have implemented hashing of the pages and transactions is that we want to be safe from on disk errors. A _really_ nice side affect of this feature is that we
can now catch early all sort of errors and surprises. It can be frustrating to realize that you are chasing a potential data corruption bug only to realize that you forgot to pass
the size in an edge case and are comparing to a null buffer, for example. But it is much better to get a clear an obvious error than have that edge case bite you two years down the
line. 

I'm speaking from experience here, storage engines are incredibly complicated beasts as the grow and the number of interactions inside the engine is very high. Being able to validate
your work as you change things in very useful.
====

=== Verifying the data on disk

Is it enough to validate the page modifications on recover? You might want to take things further. Listing 11.5 shows the options we have on the table.

[source]
.Listing 11.5 - db.h - Additional configuration options for the database
----
include::./code/db.h[tags=database_page_validation_options]
----

I've added a new `enum database_page_validation_options` option that we can use to control how often we'll validate the hashes on the pages. Here are what the options mean:

* `page_validation_none` - Only do validation of the pages that were modified during WAL recovery. Don't validate the pages beyond this.
* `page_validation_once` - Validate each page the first time that we read it from disk, but not afterward. This one will be the new default.
* `page_validation_always` - Validate each time the page is read.

The first option, `page_validation_none`, is what we have right now. We wrote the code for validating the WAL changes, but nothing else. For `page_validation_once`, the situation
is more interesting. We need to track what pages we already validated. I decided to use the simplest possible approach and define a bitmap of checked pages. To handle that I
added a couple of fields to the `db_state_t` structure:

* `uint64_t \*first_read_bitmap;`
* `uint64_t original_number_of_pages;`

I'm initializing these at the database creation, which is shown in Listing 11.6. 

[source]
.Listing 11.6 - db.c - Initiation of the bitmap used to track what pages we already verified the hash for.
----
include::./code/db.c[tags=db_setup_page_validation]
----

The idea is simple, with `page_validation_once`, we want to validate a page on the first time we read it from disk. Once it is read, we assume that it is fine and won't validate it 
again. We only need to validate the pages that are on disk when we start up, not ones that we added during the run of the database. That simplify things greatly because we don't need
to modify the size of the bitmap. 

[CAUTION]
.What is the logic behind validating only once?
====
Given that we are worried about data corruption, why wouldn't we want to validate the data at all times? Surely that would be safer, no?

This is a bit of a complex question, mostly because we don't _know_ when we read the data from disk. We are using memory mapped I/O, and even after we write to the disk, we may be
served the data from the RAM, not the disk. That means that it is entirely possible for us to have bad data on disk but valid data on RAM, with no way to tell. Blake2B is considered
to be a very fast cryptographic hash, it can reach nearly 1GB/sec in processing speed. But that is still an additional cost that we may no want to pay if it won't materially help us
improve the safety of the system.

At some point, we have to admit that we got to trust the hardware, if only a little bit. The `page_validation_once` option strikes the appropriate balance between paranoia and willful
ignorance. 
====

The actual validation function is `txn_ensure_page_is_valid`, which is called from `txn_modify_page` and `txn_get_page`. Listing 11.7 shows how it is implemented.

[source]
.Listing 11.7- txn.c - Validating that a page match its expected hash
----
include::./code/txn.c[tags=db_setup_page_validation]
----
<1> Check if we even need to do validation
<2> If the validation mode is `page_validation_once`, check if the database was initialized, the page we are checking wasn't created during this run (so wasn't on disk when we started)
and that we haven't already checked it.
<3> Do the actual validation of the page's hash.
<4> Mark the page as already checked, so we won't need to check it again under `page_validation_once` rules.

This completes the task of verifying that the data on disk wasn't changed behind our back. We use a cryptographic hash function to ensure that even a single bit flip will cause us to 
detect and alert on the issue. In fact, we have quite a lot of hashing going on:

* Hashing of the transaction buffers, which ensure that we can detect invalid transaction on disk. 
* Hashing of the pages during transaction commit, which is then validated:
** After WAL recovery.
** During normal operations, on first read from disk (or in each transaction, depending on your configuration).

Now that we have hashing worked out, I want to look at another cryptographic feature, encryption of the data.

=== Encrypting the data on disk

You might have noticed in Listing 11.5 that we now have a `encryption_key` field on the `database_options_t` struct. If this is set to a not null value, we will encrypt the on disk.

.Transparent data encryption
====
We are going to implement a feature called TDE (transparent data encryption). In this model, the data is _always_ encrypted. On disk an in memory, for the WAL and for the data file.
The only time that we hold the data in its decrypted form is when we have an active transaction that to hold it. From the point of view of the higher level APIs, there it no change
in the behavior of the system, it can work with the data as usual.

Whoever that get a hold of the data files, on the other hand, must get the key in order to make sense of what is going on here. We are going to use AES GCM algorithm to encrypt the
data. This is an algorithm with 256 bits key and very high degree of security. 

That said, note that the security of the system is dependent on the _whole_ system. How you store the key, for example, is quite important, but it is out of scope for Gavran.
====

The first step we'll take is to encrypt the data that goes into the WAL. This is easiest because we have a well defined point in which we can do so.