== Improving the Write Ahead Log

In the pervious chapter, we built a Write Ahead Log and made sure that it is written to the disk in a durable manner. We implemented recovery on startup as well
as a checkpoint process, clearing the WAL when we know that old transactions are safely stored in the data file. The implementation we have in the previous chapter
_works_, but it was meant primarily to provide you with the _concept_ of the Write Ahead Log and how it perform its role.

There is a _lot_ that is still missing from the implementation and I intend to make this chapter a journey into completing those holes in the WAL.

=== Verifying transaction integrity

A transaction is considered committed if it was written to the WAL successfully. On startup, we'll recover any change that were made in this transaction and ensure
that we have no data loss. What will happen if the write to the WAL ended prematurely? Let's say that we have a transaction that modified 10 pages, in our current
implementation, we'll need 88KB in the log to record the transaction (8KB for the transaction header, which is page aligned and then 80KB for the modified pages). 
If we crashed midway through the way, and only the first 40KB made it into the WAL, what will happen?

.What guarantees we require from the hardware?
****
The SQLite project has https://www.sqlite.org/atomiccommit.html[excellent documentation] on their assumptions of what will be the minimum behavior of the hardware
they need in order to run. Gavran has slightly different requirements, however:

* We assume that a write to the disk on page boundary will only impact the page or pages that it is touching. In other words, a write to a page cannot have any
  effect on the data on another page. 
* We assume that a write to the disk is non atomic and may fail midway, including when we are using `O_DIRECT | O_DSYNC` or any equivalent. Such failures, however,
  are constrained in their behavior. After a failed write, each byte in the page that was written to will be either:
** The value of that byte _before_ the write.
** The value of that byte _after_ the write. 
** There is no requirement that a range of bytes will change in an atomic manner, each byte may be set to its previous or new value and Gavran will operate correctly.
* In particular, we assume that a write to a page doesn't have to modify all the bytes, but any byte in the page must be either the old value or the new value. There
  is no scenario where the bytes on the page are zeroed or set to random values because of a failed write.
* A _successful_ write to the disk with `O_DIRECT | O_DSYNC` will ensure that the data is stored in a persistent medium, resilient to power failures.
* A _successful_ call to `fsync()` will ensure that all the writes made to the file reside in a persistent medium, resilient to power failures.

These assumptions are actually _weaker_ than those made by SQLite, if you'll note. There is no assumption of atomic writes on a sector or a particular order imposed on 
the write order to the disk. To my knowledge, all the disks that you are likely to encounter will behave according to these assumptions. The only exceptions of some
cheap consumer disks that don't implement `fsync()` or FUA (Force Unit Access, what `O_DIRECT | O_DSYNC` is translated into) properly.
****

Right now, if we have a failed write, it is very likely copy empty or random pages into the data file, causing data corruption. This is because we aren't verifying the _integrity_ of the 
transactions. In order to handle that, we'll need to compute a hash of the transaction before writing it and validate it during recovery. In the previous chapter
I mentioned that the choices I have are between `xxHash64` (a non cryptographic fast hash) and `BLAKE2b` (a cryptographic hash). The `xxHash64` is much faster, but
I'm going to use `Blake2B` for now. 

[TIP]
.Why `BLAKE2b` over `xxHash64`?
====
Either one will serve us well. The `BLAKE2b` requires 32 bytes compared to 8 bytes for `xxHash64`, but I'm not concerned about the size. We have plenty of space
in the transaction header. So why `BLAKE2b`? I could argue about safety, but I have no threat model that require me to handle attackers that can modify the data
on disk.

We are trying to protect ourselves either from partial writes or corruption on disk. Either option will be detected just as well by `xxHash64`. The answer is 
simple, until I have a performance trace that says that `BLAKE2b` is causing performance issues, it is easier for me to just use it rather than `xxHash64`. 
The `libsodium` package, which I used to get the `BLAKE2b` implementation is easy to setup and consume. For `xxHash64` I had to include some headers and modify
the build process. We are also going to need `libsodium` anyway in chapter 11, so we might as well use it now.
====

The first step along the way to verifying the transaction integrity is to add hashing in the `wal_append`. You can the changes required in <<wal_append_09>>.

[source]
[[wal_append_09]]
.`wal.c` - Hashing the transaction's data using `crypto_generichash()` (which uses `BLAKE2b`).
```
include::./code/wal.c[tags=wal_append]
```
<1> The new code, hashing the data in the transaction and using the `hash_blake2b` field at the beginning of the transaction to store it.

The `hash_blake2b` field is the first field in the `wal_txn_t` structure. This is done using the `crypto_generichash()` function from `libsodium` and will 
run over the entire transaction buffer (sans the first 32 bytes, where the hash value is written to). 

You can see that the actual amount of code we need to compute the hash is minimal, this is because the entire transaction is a single continuous buffer. You'll
recall that avoided using the `pwritev()` in the previous chapter, even though it allows us to save on copying of the data. One of the reasons we skipped on the
vectored writes is the simplicity for operations such as hashing of the transaction buffer. We could compute the hash on the separate pages, but it takes more code
and is somewhat more complex. We'll see more reasons for why we want to use a single buffer shortly. 

Validation of the transaction integrity is done by `wal_validate_transaction()`, shown in <<wal_validate_transaction>>.

[source]
[[wal_validate_transaction]]
.`wal.c` - Verifying the integrity of the transaction has as part of the database recovery
```
include::./code/wal.c[tags=wal_validate_transaction]
```

Validating the transaction is as easy as computing the initial hash. If we fail to validate the hash, we consider this to be a broken transaction and abort the recovery process.
That means that a partial write will be consider as noop and will have no effect, exactly as we want it to be. If you'll recall, the fact that we reuse the file after we
reset it means that we may read a valid transaction, only to have the next page come from the middle of a previous transaction. Now we can rest assured that such a case will
not be an issue for us. We validate using a cryptographic hash function, so there is no chance to get a value that would accidentally validate as a valid transaction.

Note that we have a call here to `wal_decompress_transaction()`, we'll look at that in more details later in this chapter. For now, you can assume that this just sets the 
`txn_p` pointer to the `tx` value.

With those two modifications, we have proper validation of transaction written to the disk. That wasn't bad at _all_. 

=== Validating future transactions

Now that we can verify the transaction integrity, we need to consider an interesting scenario. What happen if we fail to verify a transaction, but there
are valid transactions _beyond_ that invalid transaction? Consider the scenario shown in <<wal-corruption>>.

[[wal-corruption]]
.Invalid transaction (tx 5) followed by a valid transaction (tx 6)
image::wal-corruption.png[]

In <<wal-corruption>> we have transaction 5, which has a bad hash value. Notice that its size (the surrounding rectangle) goes into the next transaction. In our current code,
we'll run through the WAL, find that transaction 5 cannot pass verification and stop. That is fine, if this was an uncommitted transaction. However, the
situation in <<wal-corruption>> is not an uncommitted transaction, partially written to disk. It is a data corruption error in the middle of the WAL. It corrupted 
transaction 5, but transaction 6 is still there.

There isn't much that we can _do_ about such a scenario, mind. If we can't verify the hash, it is not possible to copy the pages and move on. We need to 
_report_ this scenario, however, and not allow the system to proceed and erase committed transactions.

[IMPORTANT]
.Is failing recovery a viable options? It is the _only_ option!
====
In a case such as <<wal-corruption>>, we have three options. We can attempt to write the transaction pages to the disk and hope for the best. That is likely going
to cause us data corruption and _strange_ errors down the line. Let's avoid that. We can abort the recovery process on the first transaction that failed
to verify. 

That sounds like a good idea, but it has two separate problems. The first is that we have transactions committed after the corrupted transaction that will
now be (hopefully) rolled back. That is actually the best case scenario. Remember that recovery isn't the only thing that writes to the data file. We are
also writing to the data file on an ongoing basis. It is entirely possible that some of the pages from the corrupted transaction (or later ones) were saved
to the data file, while other haven't. That leads back to data corruption and... _challenging_ debugging sessions.

If we detect such an issue and abort, we can at least have the user do something meaningful. Like restore from backup, which is probably the only safe option
that is left, after we found that we have disk corruption on the WAL.
====

We verify that the WAL has no valid transactions in the end in `wal_next_valid_transaction()`, you can see the changes in <<wal_next_valid_transaction_09>>.

[source]
[[wal_next_valid_transaction_09]]
.`wal.c` - After we get an invalid transaction, error if we have _valid_ transactions that we are going to have to skip
```
include::./code/wal.c[tags=wal_next_valid_transaction]
```
<1> The new code, call `wal_validate_after_end_of_transactions()` to ensure that there are no more valid transactions in the WAL.

We start scanning the WAL after the last valid transaction one page at a time. Transactions are always on a page boundary, so that makes things easier for us. If 
we find something that looks like a transaction, we will see if it is valid. A valid transaction that is lower than the `last_tx_id` on the database is fine, we a
already have this data in the data file. That can happen after the WAL has been reset. You can see the full details in <<wal_validate_after_end_of_transactions>>.

If the transaction is valid and come _after_ the last known transaction, we found an error and need to fail the recovery. If we found data that we can't validate
we need to go over the rest of the WAL file one page at a time, trying to see if there is a transaction there. 

[source]
[[wal_validate_after_end_of_transactions]]
.`wal.c` - Validating that no further transactions in the WAL are valid
```
include::./code/wal.c[tags=wal_validate_after_end_of_transactions]
```

The point in `wal_validate_after_end_of_transaction()` is that we'll run through the WAL and any corruption will cause us to skip the transaction. We'll then scan for 
more valid transactions and if there are any, we can detect that there was an error. Note that a corruption on the _last_ transaction isn't something that we can detect. 
There is no way to separate that from a transaction that wasn't committed properly to the disk and we'll have to consider that as rolled back.

Startup time of the database matters, and we'll usually find the next valid transaction (with an old transaction id) in the WAL. When we get to that point, we know that we
are good and that we don't need to scan the rest of the WAL. 

In <<wal_validate_after_end_of_transactions>> we are using a few utility functions, you can see them in <<wal_range>>.

[source]
[[wal_range]]
.`wal.c` - Utility functions to make iterating over the WAL contents easier
```
include::./code/wal.c[tags=wal_range]
```

<<wal_range>> shows some trivial functions, but we'll add more meaning to them in the future. 

=== Remembering the transaction id

The transaction id is incremented by Gavran on any successful commit. It is very important for us during the WAL recovery, to know what is the latest transaction
that we recovered and make sure that there aren't any future valid transactions that are in the WAL after we stopped the recovery. That leads to an interesting 
question, how can we remember the transaction id if we reset the WAL and perform no recovery on startup?

[CAUTION]
.Do we have to worry about overflowing `tx_id`?
====
The `tx_id` is a `uint64_t` value, its maximum value is `18,446,744,073,709,551,616`. If we run 100,000 transactions per second, it will take us over 
_5.8 million_ years to get _near_ the top values of `uint64_t`. For all intents and purposes, that is big enough range that can consider it infinite.
====

There are two ways we can handle that. When we reset the WAL, instead of writing a page of zeroes, we can write an empty transaction that will hold the last 
transaction id. Alternatively, we can store the value in the data file directly. I chose the later option, and this happens in `txn_commit()`. You can see
the behavior in <<txn_commit_09>>. 

[source]
[[txn_commit_09]]
.`txn.c` - Saving the transaction id on commit in the file header
```
include::../ch08/code/txn.c[tags=txn_commit]
```
<1> New code, storing the transaction id in the file's header.

Note that in <<txn_commit_09>> we are actually modifying the metadata of the first page using the `txn_modify_metadata()`, so any committed transaction will
have modification to the first page of the data file. 

The advantage of putting the last transaction id in the header is that after recovery, we can verify (again) that the numbers match and we know that we can
simply increment it again on the next write transaction.

Right now, the only _use_ we have for the transaction id is for the WAL but it is actually quite useful to be able to track transactions. Features such as
incremental backups can rely on this to make such features much easier. The transaction id also make it _much_ easier to debug certain things, because you
can easily track the changes over multiple transactions and can refer to them explicitly. 

=== Optimizing `fsync` calls

I mentioned a few times that `fsync()` is expensive, but it bears repeating. Yes, `fsync()` is that expensive. Our current system will cause us to call `fsync()` 
whenever there are no active transactions. That means that if we have a set of write transactions that run sequentially, we'll have an `fsync()` after every
transaction. That is going to be _expensive_. We actually saw that in the last chapter, when we analyzed the `strace` output of a unit test. Right now, if we 
have no active transactions when we close a transaction, we'll trigger a checkpoint and will call `fsync()`. 

We can delay that cost very simply. Instead of running a checkpoint whenever there are no active transactions, we'll do so only when there are no active
transactions _and_ the WAL is over half full.  That gives us the option to delay the `fsync()` calls significantly and the user can use the `wal_size` to 
decide how often the checkpoint process should run.

[source]
[[wal_will_checkpoint_09]]
.`wal.c` -  Deciding to checkpoint only when the WAL is over half full, to delay expensive `fsync()` calls.
```
include::./code/wal.c[tags=wal_will_checkpoint]
```

The code is minimal, but the implications are profound. We'll now do a checkpoint (and the relevant `fsync()`) only when the WAL if over half full, which 
in most cases means that we can call `fsync()` on the changes of _many_ transactions and ameliorate the cost significantly. There is also a good chance that this
delay will allow the operating system to already write some of the data we wrote to the data file on its own, further reducing our costs.

It is important to distinguish between writing to the data file and a checkpoint. We'll always write to the data file as soon as we are able, but we do that using
buffered I/O and we aren't going to trust that until `fsync()` is called. A checkpoint is exactly that, an `fsync()` called on the data file followed by trimming of
the data on the WAL.

=== Reducing the cost of writing to the WAL

Our current WAL solution requires that we'll write any modified page to the WAL, regardless of what changed inside the page.  Consider a call to `txn_allocate_page()`, 
which will also modify the metadata page. we are only touching 64 bytes in the metadata, but we have to write the whole metadata page. Or the fact that any `txn_commit()`
call will update the `last_tx_id` on the file header. In this case, that is only an 8 bytes change, but we modify the whole page. 
There are many more cases where the changes you are actually making to the modified page are smaller than the full page. Instead of writing the full page to the WAL, we 
can write the _difference_ between the modified page and the original version. 

.Writing the modified data, one transaction at a time...
****
In `wal_recover()`, we are scanning through the transactions in the WAL and write the pages in them to the data file, one transaction at a time. When we do the
same using `txn_gc()` during live database operations, we call `txn_merge_unique_pages()` first, to make sure that we are only writing a page a single time and
not however many times it has changed in the set of transactions that we are currently writing to disk.

Why the difference between the two modes? We _could_ scan through the WAL and gather a list of pages to write to the disk, keeping only the latest version for
each one of them. I didn't go with this route because we are going to implement diffed writes, which reduce the amount we write to the disk 
_significantly_, but require us to modify the page for each transaction.
****

There are many binary diff algorithms, VCDiff and XDelta both come to mind as general purpose binary diff systems. I looked into them to manage the diffs we'll
write to the WAL, but they are _generic_ algorithms. They do a lot more than what I need, and they come with baggage that I don't really want to deal with. 

I have a _lot_ of advantages when writing a diff function like that:

* The size of the `origin` and `modified` buffers match.
* Both are some multiple of PAGE_SIZE and are 8KB aligned. 
* Most of the data changes will be 8 bytes aligned and I don't need byte level granularity.

When we call `txn_raw_modify_page()`, we do a Copy on Write from the previous instance of the page. If we keep track of that page, we can easily do the diff. 
The `previous` field in `page_t` is set  `original.address` in `txn_raw_modify_page()` for this exact purpose. There is one caveat to the `previous` usage, if
we allocate a page that is in memory with a different size. This is an edge case that we treat as if we had no previous value. This can happen if we allocate
a page, freed it and then reused the space for an allocation of a different size.
We'll need to change the implementation of `wal_setup_transaction_data()`, where we copy the actual pages from the transaction to the WAL. You can see what
needs to be done in <<wal_setup_transaction_data_09>>.

[source]
[[wal_setup_transaction_data_09]]
.`wal.c` - The WAL will now write diffed versions of the modified pages, if possible
----
include::./code/wal.c[tags=wal_setup_transaction_data]
----
<1> New code, instead of calling `memcpy()`, we'll use `wal_diff_page()` to write to the `output` buffer. The page's flag are set based on the `output` size.
    If we wrote less than the size of the modified page, we wrote a diff, and we indicate as such.

The idea is that `wal_diff_page()` is going to be able to write just the _differences_ between pages. In the case of the `last_tx_id` change on the file header,
we have an 8 bytes change in an 8KB page. In the previous chapter, we'll need to write the full 8KB page to the WAL, but `wal_diff_page()` allows us to write a lot
less. 

The `wal_diff_page()` is passed three buffers. The previous version of the page, the current version and the output. The input buffers (previous & current versions)
must have the same size and the function will write up to that size to the `output`. If it can't find a diff that is smaller than the data size, it will simply copy
the current version to the `output` and use that. 

In the common case, we expect to be able to reduce the size we write to disk by a lot, but we have to be prepared to the option that we can't do any diffing whatsoever.
That means that we still have to allocate as much memory as we have modified pages in the transaction. Another important thing to remember is that because `wal_diff_page()`
will hopefully write less than its input, the _output_ of each page is no longer going to be page aligned.

[TIP]
.Taking advantage of Linux's memory allocation behavior
====
Linux uses lazy memory allocation scheme. When you ask for memory from the operating system, what you'll actually get is _memory reservation_, without
allocating the physical memory required for this. Using `glibc`, we can assume that allocations greater than 128Kb (16 pages) will go directly to the 
operating system, which will exhibit this behavior.

Our diff algorithm is going to produce output that is up to the size of the diffed pages, so the worse case scenario is that we run the diff and
we have to write the same amount of data as the pages that were modified. More commonly, we'll be able to see substantial reduction in size and so
even though we allocated a large buffer, we aren't actually paying for that in terms of memory usage.
====

The interesting work happens inside the `wal_diff_page()`, where we scan both versions of the page and figure out what is the minimum output we'll need to write to recover
the data. You can see the implementation in <<wal_diff_page>>.

[source]
[[wal_diff_page]]
.`wal.c` - Diffing two versions of a page and produce a list of changes between them
----
include::./code/wal.c[tags=wal_diff_page]
----

The code in `wal_diff_page()` operates in over 8 bytes at a time (`uint64_t`) and try to find the first index where the two versions of the page are 
different. Once a difference is found, it tries to find how big that range of different values are. We then write a `wal_page_diff_t` header that
tells us where to place the diff and what is the length of the range that we need to copy. The actual buffer we copy for each range are stored 
immediately following the `wal_page_diff_t` value. 

There is a small optimization for zero filled ranges. Those are common enough (for example, zeroing a page) that I deemed it advisable to handle it
directly. In the case of zero range, we'll store a _negative_ length in `wal_page_diff_t`, without storing a range of zeros afterward. You'll note that
each different range costs us the size of `wal_page_diff_t`, which is 8 bytes in size. We could probably reduce it further, but there is little need.

If we see that there are a lot of differences in the file, the diffed output will exceed the size of the page and we'll just write the page as is to
the WAL. That means that we can predict what is the maximum amount of memory that we need to utilize in advance. 

Note that if we have _no_ previous value it means that the previous version of this page is actually either a partial page (a large allocation that was 
broken into multiple pieces) or merged pages (multiple small allocations that were merged). These tend to be rare, so we aren't going to try to optimize
for them. We'll just threat these as impossible to diff and copy them as is to the output buffer. 

[TIP]
.I'm saying page, but I mean pages, plural
====
I'm using the term page to talk about `wal_page_diff_t`, but it will operate on multiple pages just as well. If we have a value that is big enough to
use multiple pages, we'll treat it as a single long page, and not as a series of consecutive pages. 

This is also important because the same page may have different sizes in different transactions. Imagine that we allocate page 6 as a single page in
transaction 17. Then we free the page in transaction 18 and allocate 3 pages in transaction 19. When we recover, we need to be able to handle all
kind of permutations like that. 

For that reason, in `wal_recover()` we are operating at the lowest level, directly on the pages on disk, instead of utilizing higher level concepts such
as transactions or buffering. 
====

We now write diffed records to the WAL, but we need to also handle the other side, recovery. In the last chapter, we looked at `wal_recover_page()` which
simply copied the data from the WAL to the recovery buffer. Now that we have diffs, we need to have more sophisticated behavior to manage that. You can 
see the changes to `wal_recover_page()` in <<wal_recover_page_09>>.

[source]
[[wal_recover_page_09]]
.`wal.c` - Recovering pages using either full writes or applying a diff from the previous version.
----
include::./code/wal.c[tags=wal_recover_page]
----

There are substantial changes to `wal_recover_page()`, we now decide how to recover a page based on what kind of data we have in the WAL. If we have the
full page data (`wal_txn_page_flags_none`) we'll use the old behavior and simply copy the data verbatim from the WAL to the new page buffer. However, if
we have a diff (`wal_txn_page_flags_diff`) we'll create a temporary transaction and get the current data from the disk using `pages_get()`. We copy that
value to an in memory buffer and the call `wal_apply_diff()` on that buffer. 
It is the _result_ of the diff application on the data we copied from the disk that will be eventually written back to the disk. 

There is negligible cost of opening a read transaction, as we previously saw, and that allows us to get the data from the disk easily. Even here, we use
the Copy on Write technique to avoid writing directly to the disk. We first read the data, then we modify it using `wal_apply_diff` and then we register
the modified buffer in the `pages` hash table. In `wal_recover_tx()` we'll write all those buffers to the disk after all pages have been processed.

The fact that we are writing to the data file on each transaction means that we don't have to deal with the in memory pages mapping that we usually have 
to deal with when using transactions. During recovery, there can _be_ no other work in the database, so we are free to write to the data file immediately.

.Not syncing the database changes after recovery allows reasonable startup times with slow I/O. 
****
IOPS means I/O Operations Per Second. And they are something that I constantly think about as a database developer.

Why shouldn't we call `fsync()` after we complete all the writes to the database file? That would save us the need to run recovery again the next time, no?

That may be _desirable_, but at the database startup, we cannot afford the cost. Consider Gavran running on a HDD volume. Or
using a cloud drive with low amount of IOPS. The recovery process uses buffered memory mapped I/O _intentionally_. If we have a large WAL to go through we
may be modifying a _lot_ of pages in the data file. Which will result in a _lot_ of IOPS. Because we are using buffered I/O, the number of actual disk
writes is going to be minimal. But calling `fsync()` would force us to flush everything to the file...

I have seen this exact scenario running on a 7,200 RPM hard disk where you can expect to get under 100 IOPS for random writes. And in the cloud
the situation isn't much better. Assume that we have a 16MB WAL file and that have changes to 3,000 pages in it (you'll likely get _many_ more).
That means that it will take 30 seconds with 100 IOPS to call `fsync()`. 

However, with buffered I/O and no `fsync()`, we can start the database up much faster. We'll still need to do a checkpoint, but we can do that
at the background. In fact, that is exactly what will happen automatically the next time we'll do a write in our current code. Later on in this book we'll
see how we can move the checkpoint costs from the performance-critical write path.

Startup time tends to be very expensive and we need to avoid doing things that we can do later during the startup phase. Otherwise you may end up with a database
that appears to hang during load. The reasonable administrator response in this case would be to kill the process and start again, which will simply cause us to
do another expensive `fsync()` and for all intents and purposes we have a database that cannot be opened. 
****

Now that we understand how the changes to `wal_recover_page()` work, let's look at how we process the diffed pages. You can see the implementation of `wal_apply_diff()`
in <<wal_apply_diff>>.

[source]
[[wal_apply_diff]]
.`wal.c` - Applying a binary diff on the page to recover the final state.
----
include::./code/wal.c[tags=wal_apply_diff]
----

The code in <<wal_apply_diff>> is _really_ simple. Run over the `input` and either zero the memory or copy from the buffer. I'm really proud of how simple the code ended 
up being. This is interesting because both `wal_apply_diff()` and `wal_diff_page()` together fits in a couple of pages. Admittedly, `wal_diff_page()` is pretty dense, but
what they bring to the table is pretty important.

[IMPORTANT]
.The diff is idempotent
====
The diff format Gavran uses has almost no requirements from the underlying system. As long as we apply the diffs in order, we can start from any state of the page since the
last checkpoint and we'll get to the right value in the end. This is also the basis of Gavran's requirement that the underlying storage can store either the previous version
of a write or the current one, but no random data. In practice, I know of no storage system that violate this requirement, as that is a pretty low bar to manage.
====

Gavran's diff format has an important optimization for zeroed sections, but beyond that, I didn't try to get too deep into space reduction. The diff output usually 
gets rid of all the data that didn't change, but we can do better and reduce the amount we write to disk even further. 

.The process of recovery must proceed to the end
****
During the recovery process, Gavran will run over the WAL and apply each transaction in order. When using diffed writes, we may get into some interesting situations. Let's 
consider a scenario where we have a page with the following values (each 8 bytes in size, aligned on 8 bytes, to match the diff format): `[0,0,0,0]`. We have a sequence 
of transactions with the following modifications to this page:

* Tx no. 7 - `[7,0,7,0]` - the diff is `7 . . 7`
* Tx no. 8 - `[7,8,7,8]` - the diff is `. 8 . 8`
* Tx no. 9 - `[9,8,7,8]` - The diff is `9 . . .`
* Tx no. 10 - `[9,10,10,8]` - The diff is `. 10 10 .`

At the start of the recovery, the state of the page in question is `[9,8,7,8]`. We start running through the recovery process and we apply transaction no. 7. The result will
be: `[7, 8, 7, 8]`. Note that this is an _invalid_ result. There was no transaction that had this value in the page. If we'll run the entire WAL on the page,
on the other hand, we'll get to the final result, but any values that we have in the middle are suspect and shouldn't be used.

We are writing the changes to the data file during the recovery process using buffered I/O and read it back on the next transaction to apply further diffs. We could probably
reduce the cost of that by running the entire recovery in memory and only writing the final results. While that _sounds_ like a good idea, it is very likely that it isn't.
What happens if we have a very large WAL to process, modifying many pages? We'll have to hold everything in memory.

The simplest solution here is to write the data to the file, and let the operating system to manage the in memory buffers as it sees fit. In the vast majority of the cases, 
we'll do pure in memory I/O, without really needing to touch the disk. The additional system calls cost that may be involved isn't meaningful enough to matter in the grand
scheme of things.
****

=== Compressing the transactions

I'm spending a lot of time in this chapter on reducing the amount of data that we write to the WAL, why is that? I/O is _expensive_, as in _really_
expensive. Reading a 4KB value from a _good_ SSD will take about 150 μs (microseconds), in comparison compressing the same 4KB will take 8 μs. 
The source is https://gist.github.com/jboner/2841832[Latency Numbers that Every Programmer Should Know]. For durable writes, the situation is _much_
worse. 

Interestingly enough, the size of the write doesn't matter all that much, what matter is the _scale_. Writing 8KB or 16KB makes very little difference 
when we are talking about durable writes. My assumption is that we are looking at sequential write speeds and we use the same block or 
segment in the disk, so there isn't really much of a difference between the sizes. Writing 256KB instead of 1MB, on the other hand, is absolutely noticeable in 
terms of performance.

Using diffs can reduce the amount of data we write significantly, but we can do better by adding compression to the mix. Our diff format is actually 
_designed_ to be compressed, that is why I didn't try to squeeze it too much in terms of bytes taken. The nice thing about compression is that it will 
remove duplication. If we have writes that are big enough to benefit significantly from compression, it is _very_ likely that we'll have duplication
and get good compression ratio.

The typical algorithms used for data compression in databases are Lz4, Snappy and Zstd. Snappy is optimized for speed over compression ratio, but LZ4 and Zstd 
are both better in compression ratio _and_ speed these days. Snappy's main advantage is decompression speed, but that isn't a major consideration for
me. I assume that we'll rarely, if ever, need to actually recover from the WAL during normal operations.

Lz4 is typically better in terms of compression and decompression speed and typically and has better compression ratios. It has a size 
limitation of about 1.9GB, however. That is a problem because I want to support transactions of any size. In Voron, we use LZ4 and added support 
for compressing very large transactions 1.9GB at a time. 

Zstd is in the same range as Lz4 in terms of compression speed but can offer better compression ratio. It can also compress values of any size,
meaning we don't need to write any code to manage very large transactions. Because of those reasons, I choose to use Zstd as the compression
algorithm. 

To enable compression, we need to modify `wal_prepare_txn_buffer()`, as you can see in <<wal_prepare_txn_buffer_09>>. Yopu 

[source]
[[wal_prepare_txn_buffer_09]]
.`wal.c`  - Compressing the transaction's buffer
----
include::./code/wal.c[tags=wal_prepare_txn_buffer]
----
<1> We are no longer trying to get the `tx_header_size` to be page aligned, we'll compact it as much as possible, instead.
<2> New code, calling `wal_compress_transaction()` to compress the transaction.
<3> Because of diffing and compressing, the `tx_size` is no longer required to be a multiple of `PAGE_SIZE`, writes to the WAL are _always_ page aligned,
    so we record the page aligned size here.
<4> Zero the memory beyond the `tx_size` to `page_aligned_tx_size`. That may contain uncompressed data and is not relevant for us, better to zero it.

Aside from adding the call to `wal_compress_transaction()`, there is very little that we actually need to do here, we are dealing with raw buffers, and
the fact that we moved from holding the pages data to diffs to compressed diffs has no relevance for most of the code. Now, let's look at how we are
actually performing the compression. You can see the details in <<wal_compress_transaction>>.

[source]
[[wal_compress_transaction]]
.`wal.c` - Actually running the compression on the transaction data
----
include::./code/wal.c[tags=wal_compress_transaction]
----

In `wal_compress_transaction()` we compute the size of the transaction and then what would be the maximum size of the data with compression. A surprising 
issue that we have to account for is that compressing a buffer may actually require _more_ data than the original buffer. It's easiest to see that by 
compressing some data twice. There is _some_ overhead to compression. In the diff example from before and see that we use 8 bytes for offset and length,
for example. Many compression routines have similar structure (more efficient, of course, but you still need some space). 

You might want to read http://matt.might.net/articles/why-infinite-or-guaranteed-file-compression-is-impossible/[Impossibly good compression] for details
on why that is the case. For our purposes, I'll just note that we need to handle this scenario. 

We allocate a buffer big enough to hold the compressed data and run the compression itself. If we failed to allocate the memory, failed to compress or if
the compressed size is bigger than the input's size, we return immediately and the `wal_txn_t` remained marked as non compressed. If, however, we are able
to gain space reduction from the compression, we'll mark the WAL transaction as `wal_txn_flags_compressed` and copy the compressed data from the compressed
buffer to the transaction's buffer. The result of this function is the new end of the transaction buffer.

.Memory allocation costs
****
We are allocating a buffer for the transaction and another buffer for the compression. In general, for high performance work, we need to police our allocations
because memory allocation and de-allocations are a common cause for performance issues. I'm going to write the simplest code that I can write right now and
then use the benchmarks to guide me on how to fix this.
****

The `wal_compress_transaction()` marks the transaction as compressed if it was able to reduce the size of the input. That will be relevant when we
need to recover from the transaction. This is done in the `wal_validate_transaction()` function shown earlier in <<wal_validate_transaction>> using the
`wal_decompress_transaction()` function. The details are shown in <<wal_decompress_transaction>>. It is important to note that we decompress the data _after_
we validate the hash on the transaction buffer.

[source]
[[wal_decompress_transaction]]
.`wal.c` - Support decompressions of transaction buffers during recovery
----
include::./code/wal.c[tags=wal_decompress_transaction]
----
<1> If the transaction buffer is not compressed, there is nothing to do here.
<2> Compute the required size for decompression and make sure that we have a large enough buffer for it.
<3> Decompress to the buffer, note that we start the decompression a bit after the start of the buffer, to leave room for the `wal_txn_t` transaction header.
<4> Copy the `wal_txn_t` transaction header from the original buffer to the start of the decompressed one.

Unlike before, we aren't going to allocate and free a buffer in the `wal_decompress_transaction()` function. This is because we need to access the contents of the
buffer outside of this function. To enable that scenario, we use the `buffer` field in the `file_recovery_info_t` we are passed. This allows us to keep the buffer
for the duration of the recovery process. Multiple recovery operations are likely to use the same underlying buffer, saving us the cost of allocations.
I've added `defer(free, recovery_state.files[0].buffer);` to the `wal_recover()` function to handle the releasing of this buffer. 

And that is about it, we are now able to compress the WAL transactions as we write them and significantly reduce the overall I/O that we need in most cases.
Adding the diffs for the process, we have lightened our I/O budget in the critical path. The cost of committing a transaction is lowered, because we trade I/O time
for CPU time. From real world usage, I'll note that in many cases we are able to reduce the size of the transaction modifications by a _lot_. Such tradeoffs aren't
_always_ a good idea, but in this case, they have a big impact.

=== Unit tests

Testing features such as compression, page diffing or verifying transaction are hard to write. We are not testing the functionality of the system, but 
the _manner_ in which it acts. As you can see in the tests, that means that I have to rely on testing by inference. I'm afraid that the tests are starting to get a 
bit long in this case. 

There is some amount of non trivial setup that we have to go through in order to properly test some scenarios, and C isn't the nicest language
to express that. Nevertheless, the tests has been absolutely invaluable in figuring out where stuff breaks or does something that I didn't expect. I'm running all 
the tests with `valgrind --leak-check=full --show-leak-kinds=all --track-origins=yes --error-exitcode=1 --exit-on-first-error=yes`, which means that aside from testing
functionality, I'm also testing that we don't step on any memory that we don't own.

You can see the tests in <<test_09>>. Note that there are some tricks that I had to employ here. Testing of WAL truncation or corruption was actually hard, because of
WAL diffing and compression, we write 8KB per transaction to the WAL but the actual transaction data is only on the start of that. Most of the rest is zeroed. I was
very confused for a while when I cut the file size of the WAL in half and the recovery proceeded normally. That was because we extend the size of the WAL file when we
open it to the specified `wal_size`. That means that the new data is zero extended. Because the last portion of the transaction was already zeroed, it passed validation
and was properly recovered.

I'm using writes with random data (cannot be compressed) in order to get reasonable size of the WAL so I can properly write bad data in meaningful places and truncate
the WAL to get the right errors. 

[source]
[[test_09]]
.`test.c` - Unit testing complex interactions with the WAL
----
include::./code/test.c[tags=tests09]
----