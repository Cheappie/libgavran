== Improving the Write Ahead Log

In the pervious chapter, we build a Write Ahead Log and made sure that it is written to the disk in a durable manner. We implemented recovery on startup as well
as checkpoint-ing, the trimming of the WAL when we know that all the data in it is safely stored in the data file. The implementation we have in the previous chapter
_works_, but it was meant primarily to provide you with the _concept_ of the Write Ahead Log and how it perform its role.

There is a _lot_ that is still missing from the implementation and I intend to make this chapter a journey into completing those holes in the WAL.

=== Verifying transaction integrity

A transaction is considered committed if it was written to the WAL successfully. On startup, we'll recover any change that were made in this transaction and ensure
that we have no data loss. What will happen if the write to the WAL ended prematurely? Let's say that we have a transaction that modified 10 pages, in our current
implementation, we'll need 88KB in the log to record the transaction. If we crashed midway through the way, and only the first 40KB made it into the WAL, what will
happen?

Right now? We'll very likely copy empty or random pages into the data file, causing data corruption. This is because we aren't verifying the _integrity_ of the 
transactions. In order to handle that, we'll need to compute a hash of the transaction before writing it and validate it during recovery. In the previous chapter
I mentioned that the choice I have are between `xxHash64` (a non cryptographic fast hash) and `Blake2B` (a cryptographic hash). The `xxHash64` is much faster, but
I'm going to use `Blake2B` for now. 

[TIP]
.Why `Blake2B` over `xxHash64`?
====
Either one will serve us well. The `Blake2B` requires 32 bytes compared to 8 bytes for `xxHash64`, but I'm not concerned about the size. We have plenty of space
in the transaction header. So why `Blake2B`? I could argue about safety, but I have no threat model that require me to handle attackers that can modify the data
on disk.

We are trying to protect ourselves either from partial writes or corruption on disk. Either option will be detected just as well by `xxHash64`. The answer is 
simple, until I have a performance trace that says that `Blake2B` is causing performance issues, it is easier for me to just use it rather than `xxHash64`. 
The `libsodium` package, which I used to get the `Blake2B` implementation is easy to setup and consume. For `xxHash64` I had to include some headers and modify
the build process. 

I'm going to show the code for both, but unless the perf argument is a killer, we'll use `Blake2B`. I don't expect to even notice the hashing performance, mind you.
====

The first step along the way to verifying the transaction integrity is to add hashing in the `wal_append`. I've added a call to `wal_hash_transaction` immediately
after the call to `wal_setup_transaction_for_file`. Listing 9.1 shows the implementation for `Blake2B`.

[source]
.Listing 9.1 - wal.c - Implementing `Blake2B` hashing of the transaction
----
include::./code/wal.c[tags=wal_tx_t]

static result_t wal_hash_transaction(wal_tx_t *wt,
                                     struct palfs_io_buffer *io_buffers,
                                     size_t required_pages, size_t index) {
  crypto_generichash_state state;
  ensure(!crypto_generichash_init(&state, 0, 0, crypto_generichash_BYTES),
         msg("Failed to init hash"));
  ensure(!crypto_generichash_update(
             &state, (const uint8_t *)wt + crypto_generichash_BYTES,
             (required_pages * PAGE_SIZE) - crypto_generichash_BYTES),
         msg("Failed to hash tx data"));

  for (size_t i = 1; i < index + 1; i++) {
    ensure(!crypto_generichash_update(&state, io_buffers[i].address,
                                      io_buffers[i].size),
           msg("Failed to hash page data"), with(i, "%zu"));
  }

  ensure(!crypto_generichash_final(&state, (uint8_t *)wt->tx_hash_sodium,
                                   crypto_generichash_BYTES),
         msg("Unable to complete hash of transaction"));

  return success();
}
----

I've added a `tx_hash` to the `wal_tx_t` instead of the `magic` field. The code in `wal_hash_transaction` will first has the transaction header itself, aside from
the prefix for the hash itself, then continue to run over all the pages modified in the transaction and update the hash with their contents. Finally, we get the 
result and store it in the transaction header. 

Validation of the transaction integrity is done by `wal_validate_transaction_hash`, called from  in `wal_recover`, shown in Listing 9.2.

[source]
.Listing 9.2 - wal.c - Validating the transaction integrity
----
include::./code/wal.c[tags=wal_validate_transaction_hash]

include::./code/wal.c[tags=wal_recover_loop]
----

Validating the transaction is easier than computing the initial hash. In the case of the transaction, all the buffers are in one location and consecutive. That 
makes it simpler to work with. As you can see, if we fail to validate the hash, we consider this to be a broken transaction and abort the recovery process.
That means that a partial write will be consider as noop and will have no effect, exactly as we want it to be.

For completion sake, Listing 9.3 shows the `xxHash64` implementation. Unsurprisingly, it is quite close to the `Blake2B` from the perspective of the calling code.

[source]
.Listing 9.3 - Verifying transaction integrity with `xxHash64`
----
static result_t
wal_hash_transaction(wal_tx_t *wt, struct palfs_io_buffer *io_buffers,
                              size_t required_pages, size_t index) {
  XXH64_state_t *const state = XXH64_createState();
  defer(free, state);
  ensure(state, msg("Failed to init hash"));
  XXH64_hash_t const seed = wt->tx_id;
  ensure(XXH64_reset(state, seed) != XXH_ERROR, msg("reset hash failed"));

  size_t tx_header_size = (required_pages * PAGE_SIZE) - sizeof(XXH64_hash_t);
  ensure(XXH64_update(state, (char *)wt + sizeof(XXH64_hash_t),
        tx_header_size) != XXH_ERROR, msg("Failed to hash tx state"));

  for (size_t i = 1; i < index + 1; i++) {
    ensure(XXH64_update(state, io_buffers[i].address, io_buffers[i].size) !=
              XXH_ERROR, msg("Failed to hash page"), with(i, "%zu"));
  }

  wt->tx_hash = XXH64_digest(state);
  return success();
}

static result_t wal_validate_transaction_hash(wal_tx_t *tx,
                                                     bool *passed) {
  XXH64_hash_t hash = XXH64((char *)tx + sizeof(XXH64_hash_t),
                            tx->tx_size - sizeof(XXH64_hash_t), tx->tx_id);

  *passed = hash == tx->tx_hash;
  return success();
}
----

In Listing 9.3, the `tx_hash` field is a `uint64_t` instead of a 32 bytes buffer, but aside from that, there is really nothing much that is different. 
It was easier to integrate with `Blake2B` via `libsodium`, so I went with that. We'll have performance tests later and see if that matters.

=== Validating future transactions

Now that we can verify the transaction integrity, we need to consider an interesting scenario. What happen if we fail to verify a transaction, but there
are valid transactions _beyond_ that invalid transaction? Consider the scenario shown in Figure 18.

.Invalid transaction (tx 5) followed by a valid transaction (tx 6)
image::{img-src}/fig18.png[]

We have transaction 5, which has a bad hash value. Notice that its size (the surrounding rectangle) goes into the next transaction. In our current code,
we'll run through the WAL, find that transaction 5 cannot pass verification and stop. That is fine, if this was an uncommitted transaction. However, the
situation in Figure 18 is not an uncommitted transaction, partially written to disk. It is a data corruption error in the middle of the WAL. It corrupted 
transaction 5, but transaction 6 is still there.

There isn't much that we can _do_ about such a scenario, mind. If we can't verify the hash, it is not possible to copy the pages and move on. We need to 
_report_ this scenario, however, and not allow the system to proceed and erase committed transactions.

[IMPORTANT]
.Is failing recovery a viable options? It is the _only_ option!
====
In a case such as Figure 18, we have three options. We can attempt to write the transaction pages to the disk and hope for the best. That is likely going
to cause us data corruption and _strange_ errors down the line. Let's avoid that. We can abort the recovery process on the first transaction that failed
to verify. 

That sounds like a good idea, but it has two separate problems. The first is that we have transactions committed after the corrupted transaction that will
now be (hopefully) rolled back. That is actually the best case scenario. Remember that recovery isn't the only thing that writes to the data file. We are
also writing to the data file on an ongoing basis. It is entirely possible that some of the pages from the corrupted transaction (or later ones) were saved
to the data file, while other haven't. That leads back to data corruption and... _challenging_ debugging sessions.

If we detect such an issue and abort, we can at least have the user do something meaningful. Like restore from backup, which is probably the only safe option
that is left, after we found that we have disk corruption at the WAL.
====

You can see the implementation of this check in Listing 9.4. The code there is the last thing that happens in the `wal_recover` function, to make sure that 
after we found the latest viable option, there aren't any valid transactions that we might miss. 

[source]
.Listing 9.4- wal.c - Validating that there aren't any _future_ valid transactions in the WAL after a bad one
----
include::./code/wal.c[tags=wal_recover_validate_after]
----

We start scanning the WAL after the last valid transaction one page at a time. Transactions are always on a page boundary, so that makes things easier for us. If 
we find something that looks like a transaction, we will see if it is valid. A valid transaction that is lower than the `last_tx_id` on the database is fine, we a
already have this data in the data file. That can happen after the WAL has been reset.
If the transaction is valid and come _after_ the last known transaction, we found an error and need to fail the recovery. Note that if we found a valid transaction
we can skip the whole range of the transaction. If we _didn't_ find such a transaction, we'll move to the next page and the cycle repeats to the end of the WAL.

=== Remember the transaction id

Gavran maintains the last transaction id in the log. When we checkpoint the log, we write a zero page to the beginning of the WAL. That means that we won't 
remember the last transaction id if we restart the database and and find a WAL file with no valid transactions to process. That is probably not what we want.

[CAUTION]
.Do we have to worry about overflowing `tx_id`?
====
The `tx_id` is a `uint64_t` value, its maximum value is `18,446,744,073,709,551,616`. If we run 100,000 transactions per second, it will take us over 
_5 millions_ years to get _near_ the top values of `uint64_t`. For all intents and purposes, that is big enough range that can consider it infinite.
====

The solution for maintaining the transaction id in the presence of WAL rest on checkpoint is to _not_ write a zero page to the WAL. Instead, we'll write an
empty transaction record. That would solve the WAL reset and maintain the continuity of the transaction id. Listing 9.5 shows the changes required to 
`wal_checkpoint`.  

[source]
.Listing 9.5- wal.c - Preserving transaction id across WAL reset by writing empty transaction
----
include::./code/wal.c[tags=wal_checkpoint]
----

We write a single transaction with no actual pages modifications, just so we'll have the proper transaction id persisted regardless of what is going on with the 
WAL. This is important feature, since we need to make sure that we aren't going back in time with regards to the transaction id. We rely on the sequentiality of
transaction ids in the WAL processing logic, after all.

=== Reducing the cost of writing to the WAL

Our current WAL solution require that we'll write any modified page to the WAL, regardless of what changed inside the page.  Consider a call to `txn_allocate_page`, 
which will also modify the metadata page. we are only touching 64 bytes in the metadata, but we have to write the whole metadata page. There are many cases where 
the changes you are actually making to the modified page are smaller than the full page. Instead of writing the full page to the WAL, we can write the _difference_
between the modified page and the original version. 

.Writing the modified data, one transaction at a time...
****
In `wal_recover`, we are scanning through the transactions in the WAL and write the pages in them to the data file, one transaction at a time. When we do the
same using `txn_gx_tx` during live database operations, we call `txn_merge_unique_pages` first, to make sure that we are only writing a page a single time and
not however many times it has changed in the set of transactions that we are currently writing to disk.

Why the difference between the two modes? We _could_ scan through the WAL and gather a list of pages to write to the disk, keeping only the latest version for
each one of them. I didn't chose to go with this route because we are going to implement diffed writes, which reduce the amount we write to the disk 
_significantly_, but require us to modify the page for each transaction.
****

There are many binary diff algorithms, VCDiff and XDelta both come to mind as general purpose binary diff systems. I looked into them to manage the diffs we'll
write to the WAL, but they are _generic_ algorithms. They do a lot more than what I need, and they come with baggage that I don't really want to deal with. 
Let's take a look at the code in Listing 9.6, which shows how we can compute the difference between versions of a page. 

I have a _lot_ of advantages when writing a diff function like that:

* The size of the `origin` and `modified` buffers match.
* Both are some multiple of PAGE_SIZE and that they are page aligned. 
* Most of the data changes will be 8 bytes aligned and I don't byte level granularity.

When we call `txn_modify_page`, we do a Copy on Write from the previous instance of the page. If we keep track of that page, we can easily do the diff. 
I added `previous_address` field to the `page_t` struct and set it to the `original.address` in `txn_modify_page` and the setup was done. 


=== Compressing the transactions


=== Extending the size of the WAL