== Improving the Write Ahead Log

In the pervious chapter, we build a Write Ahead Log and made sure that it is written to the disk in a durable manner. We implemented recovery on startup as well
as checkpoint-ing, the trimming of the WAL when we know that all the data in it is safely stored in the data file. The implementation we have in the previous chapter
_works_, but it was meant primarily to provide you with the _concept_ of the Write Ahead Log and how it perform its role.

There is a _lot_ that is still missing from the implementation and I intend to make this chapter a journey into completing those holes in the WAL.

=== Verifying transaction integrity

A transaction is considered committed if it was written to the WAL successfully. On startup, we'll recover any change that were made in this transaction and ensure
that we have no data loss. What will happen if the write to the WAL ended prematurely? Let's say that we have a transaction that modified 10 pages, in our current
implementation, we'll need 88KB in the log to record the transaction. If we crashed midway through the way, and only the first 40KB made it into the WAL, what will
happen?

Right now? We'll very likely copy empty or random pages into the data file, causing data corruption. This is because we aren't verifying the _integrity_ of the 
transactions. In order to handle that, we'll need to compute a hash of the transaction before writing it and validate it during recovery. In the previous chapter
I mentioned that the choice I have are between `xxHash64` (a non cryptographic fast hash) and `Blake2B` (a cryptographic hash). The `xxHash64` is much faster, but
I'm going to use `Blake2B` for now. 

[TIP]
.Why `Blake2B` over `xxHash64`?
====
Either one will serve us well. The `Blake2B` requires 32 bytes compared to 8 bytes for `xxHash64`, but I'm not concerned about the size. We have plenty of space
in the transaction header. So why `Blake2B`? I could argue about safety, but I have no threat model that require me to handle attackers that can modify the data
on disk.

We are trying to protect ourselves either from partial writes or corruption on disk. Either option will be detected just as well by `xxHash64`. The answer is 
simple, until I have a performance trace that says that `Blake2B` is causing performance issues, it is easier for me to just use it rather than `xxHash64`. 
The `libsodium` package, which I used to get the `Blake2B` implementation is easy to setup and consume. For `xxHash64` I had to include some headers and modify
the build process. 

I'm going to show the code for both, but unless the perf argument is a killer, we'll use `Blake2B`. I don't expect to even notice the hashing performance, mind you.
====

The first step along the way to verifying the transaction integrity is to add hashing in the `wal_append`. I've added a call to `wal_hash_transaction` immediately
after the call to `wal_setup_transaction_for_file`. Listing 9.1 shows the implementation for `Blake2B`.

[source]
.Listing 9.1 - wal.c - Implementing `Blake2B` hashing of the transaction
----
include::./code/wal.c[tags=wal_tx_t]

static result_t wal_hash_transaction(wal_tx_t *wt,
                                     struct palfs_io_buffer *io_buffers,
                                     size_t required_pages, size_t index) {
  crypto_generichash_state state;
  ensure(!crypto_generichash_init(&state, 0, 0, crypto_generichash_BYTES),
         msg("Failed to init hash"));
  ensure(!crypto_generichash_update(
             &state, (const uint8_t *)wt + crypto_generichash_BYTES,
             (required_pages * PAGE_SIZE) - crypto_generichash_BYTES),
         msg("Failed to hash tx data"));

  for (size_t i = 1; i < index + 1; i++) {
    ensure(!crypto_generichash_update(&state, io_buffers[i].address,
                                      io_buffers[i].size),
           msg("Failed to hash page data"), with(i, "%zu"));
  }

  ensure(!crypto_generichash_final(&state, (uint8_t *)wt->tx_hash_sodium,
                                   crypto_generichash_BYTES),
         msg("Unable to complete hash of transaction"));

  return success();
}
----

I've added a `tx_hash` to the `wal_tx_t` instead of the `magic` field. The code in `wal_hash_transaction` will first has the transaction header itself, aside from
the prefix for the hash itself, then continue to run over all the pages modified in the transaction and update the hash with their contents. Finally, we get the 
result and store it in the transaction header. 

Validation of the transaction integrity is done by `wal_validate_transaction`, called from  in `wal_recover`, shown in Listing 9.2.

[source]
.Listing 9.2 - wal.c - Validating the transaction integrity
----
include::./code/wal.c[tags=wal_validate_transaction]
----

Validating the transaction is easier than computing the initial hash. In the case of the transaction, all the buffers are in one location and consecutive. That 
makes it simpler to work with. As you can see, if we fail to validate the hash, we consider this to be a broken transaction and abort the recovery process.
That means that a partial write will be consider as noop and will have no effect, exactly as we want it to be.

For completion sake, Listing 9.3 shows the `xxHash64` implementation. Unsurprisingly, it is quite close to the `Blake2B` from the perspective of the calling code.

[source]
.Listing 9.3 - Verifying transaction integrity with `xxHash64`
----
static result_t
wal_hash_transaction(wal_tx_t *wt, struct palfs_io_buffer *io_buffers,
                              size_t required_pages, size_t index) {
  XXH64_state_t *const state = XXH64_createState();
  defer(free, state);
  ensure(state, msg("Failed to init hash"));
  XXH64_hash_t const seed = wt->tx_id;
  ensure(XXH64_reset(state, seed) != XXH_ERROR, msg("reset hash failed"));

  size_t tx_header_size = (required_pages * PAGE_SIZE) - sizeof(XXH64_hash_t);
  ensure(XXH64_update(state, (char *)wt + sizeof(XXH64_hash_t),
        tx_header_size) != XXH_ERROR, msg("Failed to hash tx state"));

  for (size_t i = 1; i < index + 1; i++) {
    ensure(XXH64_update(state, io_buffers[i].address, io_buffers[i].size) !=
              XXH_ERROR, msg("Failed to hash page"), with(i, "%zu"));
  }

  wt->tx_hash = XXH64_digest(state);
  return success();
}

static result_t wal_validate_transaction_hash(wal_tx_t *tx,
                                                     bool *passed) {
  XXH64_hash_t hash = XXH64((char *)tx + sizeof(XXH64_hash_t),
                            tx->tx_size - sizeof(XXH64_hash_t), tx->tx_id);

  *passed = hash == tx->tx_hash;
  return success();
}
----

In Listing 9.3, the `tx_hash` field is a `uint64_t` instead of a 32 bytes buffer, but aside from that, there is really nothing much that is different. 
It was easier to integrate with `Blake2B` via `libsodium`, so I went with that. We'll have performance tests later and see if that matters.

=== Validating future transactions

Now that we can verify the transaction integrity, we need to consider an interesting scenario. What happen if we fail to verify a transaction, but there
are valid transactions _beyond_ that invalid transaction? Consider the scenario shown in Figure 18.

.Invalid transaction (tx 5) followed by a valid transaction (tx 6)
image::{img-src}/fig18.png[]

We have transaction 5, which has a bad hash value. Notice that its size (the surrounding rectangle) goes into the next transaction. In our current code,
we'll run through the WAL, find that transaction 5 cannot pass verification and stop. That is fine, if this was an uncommitted transaction. However, the
situation in Figure 18 is not an uncommitted transaction, partially written to disk. It is a data corruption error in the middle of the WAL. It corrupted 
transaction 5, but transaction 6 is still there.

There isn't much that we can _do_ about such a scenario, mind. If we can't verify the hash, it is not possible to copy the pages and move on. We need to 
_report_ this scenario, however, and not allow the system to proceed and erase committed transactions.

[IMPORTANT]
.Is failing recovery a viable options? It is the _only_ option!
====
In a case such as Figure 18, we have three options. We can attempt to write the transaction pages to the disk and hope for the best. That is likely going
to cause us data corruption and _strange_ errors down the line. Let's avoid that. We can abort the recovery process on the first transaction that failed
to verify. 

That sounds like a good idea, but it has two separate problems. The first is that we have transactions committed after the corrupted transaction that will
now be (hopefully) rolled back. That is actually the best case scenario. Remember that recovery isn't the only thing that writes to the data file. We are
also writing to the data file on an ongoing basis. It is entirely possible that some of the pages from the corrupted transaction (or later ones) were saved
to the data file, while other haven't. That leads back to data corruption and... _challenging_ debugging sessions.

If we detect such an issue and abort, we can at least have the user do something meaningful. Like restore from backup, which is probably the only safe option
that is left, after we found that we have disk corruption at the WAL.
====

You can see the implementation of this check in Listing 9.4. The code there is the last thing that happens in the `wal_recover` function, to make sure that 
after we found the latest viable option, there aren't any valid transactions that we might miss. 

[source]
.Listing 9.4- wal.c - Validating that there aren't any _future_ valid transactions in the WAL after a bad one
----
include::./code/wal.c[tags=wal_recover_validate_after]
----

We start scanning the WAL after the last valid transaction one page at a time. Transactions are always on a page boundary, so that makes things easier for us. If 
we find something that looks like a transaction, we will see if it is valid. A valid transaction that is lower than the `last_tx_id` on the database is fine, we a
already have this data in the data file. That can happen after the WAL has been reset.
If the transaction is valid and come _after_ the last known transaction, we found an error and need to fail the recovery. Note that if we found a valid transaction
we can skip the whole range of the transaction. If we _didn't_ find such a transaction, we'll move to the next page and the cycle repeats to the end of the WAL.

=== Remembering the transaction id

Gavran maintains the last transaction id in the log. When we checkpoint the log, we write a zero page to the beginning of the WAL. That means that we won't 
remember the last transaction id if we restart the database and and find a WAL file with no valid transactions to process. That is probably not what we want.

[CAUTION]
.Do we have to worry about overflowing `tx_id`?
====
The `tx_id` is a `uint64_t` value, its maximum value is `18,446,744,073,709,551,616`. If we run 100,000 transactions per second, it will take us over 
_5 millions_ years to get _near_ the top values of `uint64_t`. For all intents and purposes, that is big enough range that can consider it infinite.
====

The solution for maintaining the transaction id in the presence of WAL rest on checkpoint is to _not_ write a zero page to the WAL. Instead, we'll write an
empty transaction record. That would solve the WAL reset and maintain the continuity of the transaction id. Listing 9.5 shows the changes required to 
`wal_checkpoint`.  

[source]
.Listing 9.5- wal.c - Preserving transaction id across WAL reset by writing empty transaction
----
include::./code/wal.c[tags=wal_will_checkpoint]
----

We write a single transaction with no actual pages modifications, just so we'll have the proper transaction id persisted regardless of what is going on with the 
WAL. This is important feature, since we need to make sure that we aren't going back in time with regards to the transaction id. We rely on the sequentiality of
transaction ids in the WAL processing logic, after all.

=== Optimizing `fsync` calls

I mentioned a few times that `fsync` is expensive, but it bears repeating. Yes, `fsync` is that expensive. Our current system will cause us to call `fsync` 
whenever there are no active transactions. That means that if we have a set of write transactions that run sequentially, we'll have an `fsync` after every
transaction. That is going to be _expensive_. 

We can delay that cost very simply. Instead of running a checkpoint whenever there are no active transactions, we'll do so only when there are no active
transactions _and_ the WAL is over half full.  Listing 9.6 shows how it is done.

[source]
.Listing 9.6- wal.c - Deciding to checkpoint only when the WAL is over half full, to delay expensive `fsync` calls.
----
include::./code/wal.c[tags=wal_will_checkpoint]
----

The code is minimal, but the implications are profound. We'll now do a checkpoint (and the relevant `fsync`) only when the WAL if over half full, which 
in most cases means that we can call `fsync` on the changes of _many_ transactions and ameliorate the cost significantly.

=== Reducing the cost of writing to the WAL

Our current WAL solution require that we'll write any modified page to the WAL, regardless of what changed inside the page.  Consider a call to `txn_allocate_page`, 
which will also modify the metadata page. we are only touching 64 bytes in the metadata, but we have to write the whole metadata page. There are many cases where 
the changes you are actually making to the modified page are smaller than the full page. Instead of writing the full page to the WAL, we can write the _difference_
between the modified page and the original version. 

.Writing the modified data, one transaction at a time...
****
In `wal_recover`, we are scanning through the transactions in the WAL and write the pages in them to the data file, one transaction at a time. When we do the
same using `txn_gx_tx` during live database operations, we call `txn_merge_unique_pages` first, to make sure that we are only writing a page a single time and
not however many times it has changed in the set of transactions that we are currently writing to disk.

Why the difference between the two modes? We _could_ scan through the WAL and gather a list of pages to write to the disk, keeping only the latest version for
each one of them. I didn't chose to go with this route because we are going to implement diffed writes, which reduce the amount we write to the disk 
_significantly_, but require us to modify the page for each transaction.
****

There are many binary diff algorithms, VCDiff and XDelta both come to mind as general purpose binary diff systems. I looked into them to manage the diffs we'll
write to the WAL, but they are _generic_ algorithms. They do a lot more than what I need, and they come with baggage that I don't really want to deal with. 
Let's take a look at the code in Listing 9.6, which shows how we can compute the difference between versions of a page. 

I have a _lot_ of advantages when writing a diff function like that:

* The size of the `origin` and `modified` buffers match.
* Both are some multiple of PAGE_SIZE and that they are page aligned. 
* Most of the data changes will be 8 bytes aligned and I don't byte level granularity.

When we call `txn_modify_page`, we do a Copy on Write from the previous instance of the page. If we keep track of that page, we can easily do the diff. 
I added `previous_address` field to the `page_t` struct and set it to the `original.address` in `txn_modify_page` and the setup was done. Let's see what changes
are required in our code to make this happen.  Listing 9.7 shows the changes to `wal_append`.

[source]
.Listing 9.7 - wal.c - Using diffs to reduce the amount of data we write to the WAL
----
include::./code/wal.c[tags=wal_append]
----
<1> Compute the size we need for the WAL transaction. Unlike before, we now also need to account for the modified pages.
<2> Allocate a single buffer for the transaction header and the modified pages, whereas the last implementation used the already existing modified pages as buffer.
<3> Setup the actual data that will be written to the WAL. We'll look into that in depth shortly.
<4> The size of the data we write is no longer a multiple of a `PAGE_SIZE`, but the _Writes_ to the WAL must be (because of the use of `O_DIRECT`), so we make sure
    that the rest of the data in the page is zeroed.
<5> Computing the hash over a single continuous buffer is much easier and doesn't require a dedicated function.
<6> Do the actual write to file using our usual `palfs_write_file` routine.

Instead of pulling the data for the write from individual pages, we are copying them to a single buffer. Of course, the idea is to _not_ copy the entire page but
to write just the difference between the original and modified versions, so we'll hopefully write a lot less than the full page size.

[TIP]
.Taking advantage of Linux's memory allocation behavior
====
Linux uses lazy memory allocation scheme. When you ask for memory from the operating system, what you'll actually get is _memory reservation_, without
allocating the physical memory required for this. Using `glibc`, we can assume that allocations greater than 128Kb (16 pages) will go directly to the 
operating system, which will exhibit this behavior.

Our diff algorithm is going to production output that is up to the size of the diffed pages, so the worse case scenario is that we run the diff and
we have to write the same amount of data as the pages that were modified. More commonly, we'll be able to see substantial reduction in size and so
even though we allocated a large buffer, we aren't actually paying for that in terms of memory usage.
====

You can note in Listing 9.7 that we are no longer using vectored writes. Because we have to build a buffer from the diffs between the page versions
we can allocate the whole thing in one shot and use the standard `write` to send it to disk.

Listing 9.8 shows the code for `wal_setup_transaction_data`, where we go over all the modified pages and write them to the buffer.

[source]
.Listing 9.8 - wal.c - Preparing the transaction buffer to be written to the WAL
----
include::./code/wal.c[tags=wal_setup_transaction_data]
----

The `wal_setup_transaction_data` iterates over the modified pages in the transaction and call `wal_diff_page` on them, figure out whatever the diff
has successfully reduced the page size and generated a diff or whatever we used the raw page data. The interesting work happens inside the `wal_diff_page` 
that is shown in LIsting 9.9.

[source]
.Listing 9.9 - wal.c - Diffing two versions of a page and produce a list of changes between them
----
include::./code/wal.c[tags=wal_page_diff]

include::./code/wal.c[tags=wal_diff_page]
----

The code in `wal_diff_page` operates in over 8 bytes at a time (`uint64_t`) and try to find the first index where the two versions of the page are 
different. Once a difference is found, it tries to find how big that range of different values are. We then write a `wal_page_diff_t` header that
tells us where to place the diff and what is the length of the range that we need to copy. The actual values we copy for each range are stored 
immediately following the `wal_page_diff_t` value. 

There is a small optimization for zero filled ranges. Those are common enough (for example, zeroing a page) that I deemed it advisable to handle it
directly. In the case of zero range, we'll store a negative length in `wal_page_diff_t`, without storing a range of zeros afterward. You'll note that
each different range costs us the size of `wal_page_diff_t`, which is 8 bytes in size. We could probably reduce it further, but there is little need.

If we see that there are a lot of differences in the file, the diffed output will exceed the size of the page and we'll just write the page as is to
the WAL. That means that we can predict what is the maximum amount of memory that we need to utilize in advance. 

[TIP[
.I'm saying page, but I mean page_s_
====
I'm using the term page to talk about `wal_diff_page`, but it will operate on multiple pages just as well. If we have a value that is big enough to
use multiple pages, we'll treat it as a single long page, and not as a series of consecutive pages. 

This is also important because the same page may have different sizes in different transactions. Imagine that we allocate page 6 as a single page in
transaction 17. Then we free the page in transaction 18 and allocate 3 pages in transaction 19. When we recover, we need to be able to handle all
kind of permutations like that. 

For that reason, in `wal_recover` we are operating at the lowest level, directly on the pages on disk, instead of utilizing higher level concepts such
as transactions or buffering. 
====

We now write diffed records to the WAL, but we need to also handle the other side, recovery. This is shown in Listing 9.10.

[source]
.Listing 9.10 - wal.c  - Recovering the data from the WAL on startup
----
include::./code/wal.c[tags=wal_recover]
----
<1> Enable writes on the data file pages using mmap. This is usually disabled, we enable this solely for the purpose of applying the WAL to the data file. 
We are using the memory mapped writes to avoid expensive system calls when we modify the data of certain pages many times during the database recovery.
<2> Define a scratch buffer for the transactions, will be used later.
<3> Verify the transaction.
<4> The `wal_get_transaction` is used to get the transaction, will be used later.
<5> Compute the start of the diff stream and and iterate over the modified pages.
<6> Get the _raw_ page from the data file, this is _mutable_ and will be changed directly by the `wal_apply_diff`.
<7> Apply the diff to the page.
<8> Or copy the whole page from the WAL if it wasn't diffed.
<9> Check that there are no additional transactions after the last valid one (may indicate WAL corruption).
<10> Explicitly _not_ sync the changes to the disk, we'll discuss why shortly. 

Quite a lot is going on in `wal_recover`. The first new change is that we are writing directly to the file using memory mapped I/O. A single page may appear
multiple times in the recovery, and we need to apply the changes to the page in order. Instead of writing to the data file on each change, we use a
writable memory map for the recovery process. Once the recovery process is complete, we'll mark the memory as read only again. That has the advantage of 
letting the operating system batch changes to the data file and significantly reduce the number of system calls and memory copies that we need to do.
It has the disadvantage that if we fail midway through, we modified the data on the file. We don't worry about that, because on recovery, we'll just re-run
through the WAL again and fix any changes to match what we have in the WAL.

.Not sync the database changes after recover allows reasonable startup times with slow I/O. 
****
IOPS means I/O Operations Per Second. And they are something that I constantly think about as a database developer.

Why shouldn't we call `fsync` after we complete all the writes to the database file? That would save us the need to run recovery again the next time, no?

That may be _desirable_, but at the database startup, we cannot afford the cost. Consider Gavran running on a HDD volume. Or
using a cloud drive with low amount of IOPS. The recovery process uses buffered memory mapped I/O _intentionally_. If we have a large WAL to go through we
may be modifying a _lot_ of pages in the data file. Which will result in a _lot_ of IOPS. Because we are using memory mapped I/O, the number of actual disk
writes is going to be minimal. But calling `fsync` would force us to flush everything to the file...

I have seen this exact scenario running on a 7,200 RPM hard disk where you can expect to get under 100 IOPS for random writes. And in the cloud
the situation isn't much better. Assume that we have a 16MB WAL file and that have changes to 3,000 pages in it (you'll likely get _many_ more).
That means that it will take 30 seconds with 100 IOPS to call `fsync`. 

However, with buffered I/O and no `fsync`, we can start the database up much faster. We'll still need to do a checkpoint, but we can do that
at the background. In fact, that is exactly what will happen automatically the next time we'll do a write. Startup time tends to be very expensive and 
we need to avoid doing things that we can do later during the startup phase.
****

Note that we are explicitly allowing to write to the data file memory map. This is done using `palfs_enable_writes` and reverted using 
`palfs_disable_writes`, both of them are shown in Listing 9.11.

[source]
.Listing 9.11 - fs.c  - Enable and disabling writes to a memory mapped region
----
include::./code/fs.c[tags=palfs_enable_writes]
----

Using diffs instead of writing the full page to the WAL will reduce the size that we need, but there are other tricks that we can play to reduce the 
size of the WAL writes even further. 

=== Compressing the transactions

I'm spending a lot of time in this chapter on reducing the amount of data that we write to the WAL, why is that? I/O is _expensive_, as in _really_
expensive. Reading a 4KB value from a _good_ SSD will take about 150 μs (microseconds), in comparison compressing the same 4KB will take 8 μs. 
The source is https://gist.github.com/jboner/2841832[Latency Numbers that Every Programmer Should Know]. For durable writes, the situation is _much_
worse. 

Interestingly enough, the size of the write doesn't matter all that much, what matter is the _scale_. Writing 8KB or 16KB makes very little difference 
when we are talking about durable writes. My assumption is that we are looking at sequential write speeds and we need to switch to different block or 
segment in the disk that will likely cause the performance to matter. Writing 256KB instead of 1MB, on the other hand, is absolutely noticeable in 
terms of performance. 

Using diffs can reduce the amount of data we write significantly, but we can do better by adding compression to the mix. Our diff format is actually 
design to be compressed, that is why I didn't try to squeeze it too much in terms of bytes taken. The nice thing about compression is that it will 
remove duplication. If we have writes that are big enough to benefit significantly from compression, it is _very_ likely that we'll have duplication
and get good compression ratio.

The typical algorithms used for data compression are Lz4, Snappy and Zstd. Snappy is optimized for speed over compression ratio, but LZ4 and Zstd 
are both better in compression ratio _and_ speed these days. It's main advantage is decompression speed, but that isn't a major consideration for
me. I assume that we'll rarely, if ever, need to actually recover from the WAL during normal operations.

Lz4 is typically better in terms of compression and decompression speed and typically and has better compression ratios. It has a size 
limitation of about 1.9GB, however. That is a problem because I want to support transactions of any size. In Voron, we use LZ4 and added support 
for compressing very large transactions 1.9GB at a time. 

Zstd is in the same range as Lz4 in terms of compression speed but can offer better compression ratio. It can also compress values of any size,
meaning we don't need to write any code to manage very large transactions. Because of those reasons, I choose to use Zstd as the compression
algorithm. 

Listing 9.12 shows the compression routine, which is called just after `wal_setup_transaction_data` by the `wal_append`.

[source]
.Listing 9.12 - wal.c - Compressing the transaction before it is written to the WAL
----
include::./code/wal.c[tags=wal_compress_transaction]
----
<1> We compute the size required by Zstd to compress the transaction data. All compression scheme have inputs that would cause them to _inflate_
    the size of the data, so we have to account for that.
<2> In `wal_append` we allocated a buffer that was equal in size to the number of modified pages. We already run a diff on the data, so we might
    be able to fit the compressed size in the remaining buffer. If so, we'll be able to skip any allocations in this method.
<3> If we allocated a buffer, `defer` will free it, otherwise, a `free(0)` is a no-op. 
<4> We run the compression itself and check that it completed with no errors and that the compressed size is smaller than the input. 
<5> We mark the transaction as compressed and move the memory from the compression buffer to the buffer we're about to write to the WAL. We have
    to use `memmove` here because if we were able to skip allocating the buffer earlier, the buffers may overlap.

The `wal_compress_transaction` marks the transaction as compressed if it was able to reduce the size of the input. That will be relevant when we
need to recover from the transaction. This is done in the `wal_get_transaction` which is called from `wal_recover`, you can see the details in
Listing 9.12.

[source]
.Listing 9.12 - wal.c - Decompressing the transaction buffer during recovery
----
include::./code/wal.c[tags=wal_get_transaction]
----
<1> If the transaction isn't compressed, we can just return the buffer we got.
<2> With Zstd, you can ask what is the decompressed size of a compressed buffer. We use that to compute how much space we need and then allocate
    a larger value if needed.
<3> Actual decompression happens here, as well as error checking if we couldn't decompress the data.
<4> We are using a separate buffer from the compressed data, therefor we copy the header of the transaction to the new buffer and return a pointer
    to that, with the decompressed data immediately following it.

The format we end up with is a transaction header that is stored uncompressed and followed by the compressed buffer. We decompress the transaction
data to a separate buffer and copy the transaction header to the beginning, recreating the original transaction data.

The `wal_recover` method uses a `tx_buffer` that is passed passed to the `wal_get_transaction` function. That is used to hold the
memory required for the transaction buffer. The idea is that we save on allocations by reusing the buffer for the duration of `wal_recover`. The
`wal_get_transaction` will use `realloc` to increase the size of the buffer as needed. 

=== Unit tests

Testing features such as compression, page diffing or verifying transaction are hard to write. We are not testing the functionality of the system, but 
the _manner_ in which it acts. As you can see in the tests, that means that I have to rely on testing by inference.

[CAUTION]
.The bug the tests caught
====
If you'll recall, we ensure that we have a single write transaction by using a `db_state_t* active_write_tx` field to track the active transaction.
During the tests, I started to get errors that a write transaction is already opened. That was strange, because I'm using `defer` in C and `with`
in Python specifically so I wouldn't have to deal with these kind of issues.

The problem was that we call `realloc` on the `tx_state_t` on some cases, moving the pointer. It usually doesn't matter, the user holds the `txn_t`
instance that point to it, but if we hold a `txn_state_t*` we have to realize that it may not be stable. That is what happened in this case, the 
hash table grew and the `txn_state_t` was moved, but the `active_write_tx` value was not updated. 

I changed it to be a `uint64_t active_write_tx` which tracks the transaction _id_, which is stable for the life time of the transaction. But that
is the kind of bug that would have been messy to figure out later on.

This is another reason why we don't have any multi threading code in Gavran now. Finding issues in single threaded code is _easy_, so I want to 
keep it that way for as long as possible. It makes my life easier. 
====


[source,python]
.Listing 9.13 - Unit test transaction verification and WAL compression
----
include::../pyapi/tests09.py[]
----

