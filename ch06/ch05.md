
# Page management

We now have a system that is capable of allocation a file, work with individual pages and allcoate and free pages dynamically. We are almost ready to start working on 
data structures. There is a minor isue, the most that we can use is a single page at a time. That works if everything that we need to work with is 8Kb or less, but isn't
really a viable option to go on with.

We need to introduce the notion of an _overflow_ page. This refers to a value that must reside on multiple pages (so it overflow the page boundary). Listing 5.1 shows
what we want to achieve using the current code. As you can see, it is badly broken.

```{caption="Trying to set a value greater than 8Kb on a page" .c}
assert(create_transaction(&db, 0, &tx));
assert(allocate_page(&tx, &page));

// will write beyond the boundary of the page!!!
memcpy(page.address, buffer, 12288);

assert(commit_transaction(&tx));
assert(close_transaction(&tx));

assert(create_transaction(&db, 0, &tx));
assert(get_page(&tx, &page));

char new_buf[12288];
// Again, reads past end of file
memcpy(new_buf, page.address, 12288);
```

The issue is that we are allocating a single page at a time, which won't work. And worse, when we try to _read_, the `get_page` will give us a single page because
it doesn't understand it. Right now, in order to handle this properly we'll need to allocate two pages and do separate writes and reads to get it.
It seems obivous that we need to provide a mechanism at the database level to manage values that cross more than a single page. The way we'll do that is to allow
to allocate pages in a group. So you can ask for a page, or three pages, etc. 

In order to support this, we'll have to make the following API changes, shown in Listing 5.2.

```{caption="API changes to support allocating and using multiple pages" .c}
typedef struct page
{
  uint64_t page_num;
  void *address;
  uint32_t overflow_size; // new
} page_t;

MUST_CHECK bool allocate_page(txn_t *tx, 
		page_t *page, uint64_t near_page);
```

In other words, we will use the `page_t` structure to pass the requested size of the value that we intend to put in the page. We use a `uint32_t` here, so the 
maximum size of a single value would be 4GB. Why are we passing the _size_ of the value instead of the number of requested pages? The reasoning behind that is 
that we need to know what is the size of the value when we get it back from `get_page`. 

> **Value vs. page**
>
> I'm using two different terms here that may be confusing. The overflow page (rather, pages) is a consecutive set of pages that are managed as a single 
> unit by Gavran. The value I refer to here is the _user provided_ value, which does not need to fit on a page boundary. If I want to persist a value 
> that is 10,000 bytes in size, I'll need to use two pages for that.
>
> When I'm reading it back using `get_page`, we'll read those two pages, but we need to provide the caller with the size of the that was previous stored 
> in those pages.

In addition to adding the `overflow_size` field, I'm adding a `near_page` parameter for `allocate_page`, so callers can specified their preferred location.
The changes to `allocate_page` are simple, we already have all the mechanics in place to handle this featute. And indeed, you'll not run into issues when
handling this. The problem starts when you have to write `get_page`.

The actual issue is pretty simple, where are we going to put the `overflow_size` value? We need to put it _somewhere_, but we have no room for it. This isn't
information that goes into the page, it is information _about_ the page. Where would it go?

## Designing the page metadata

The typical location for metadata information about a page in a header inside the page. The storage engine will reserve the first few bytes of a page to itself for its
own book keeping purposes. You can see how this looks in Figure 5.1. This is how it works for LMDB and Voron, for example, among many others.

![A page with inlined metadta in the header](./ch05/img02.png)

Keeping the metadata in the page header has a number of useful properties:

* Keeping the metadata in page header is simple to explain and implement.
* There is great locality of reference for the metadata.

It also have an annoying downside, it means that you don't actually have the full page available for the user. In Voron, the header size if 64 bytes and LMDB has a 16 
bytes header. As you can see in Figure 5.1, that means that you are left with 8,128 bytes to work with in a page with Voron and 8,176 bytes in LMDB. That isn't bad, per
se, but it does have a _very_ important implication. The size of a value is never going to be a power of two if you are using page headers. 

Consider how we'll need to implement the free space bitmaps in the precense of page headers. It will make things much harder. Not _too_ hard, admittedly, but this is a
very simple case. There are several data structures which assume that the data is going to be based on a power of 2. Two such examples are roaring bitmaps and external
hash tables.

I also want to create proper layering in the system. We have now a page based storage system that can manage, allocate and free pages, and we don't have any constraint
on what is going on. That is a very desriable property in my opinion. So I want to avoid a page header. 

If the page metadata isn't on the page, where are we going to place it? Let's assume that we need to keep 16 bytes of metadata per page. The obvious solution is to 
go with the same route we did with the free space bitmap. We'll put the pages metadata in the pages themselves. A look at Figure 5.2 might help makes things cleare.

![Dedicated pages metadata page contain metadata for all pages, including themselves, the free space bitamp and the header](./ch05/img02.png)

Assuming that we have 16 bytes per page, that means that we can fit 512 pages metadata in an 8Kb page. In other words, we use one 8Kb metadata page for every 4MB
of disk space we use. And each GB of disk space will require 2MB of metadata. Note that we were going to need to make use of this space _anyway_, by the way. 
The issue is whatever this is internal use in a page or external with dedicated space.

In Figure 5.2, we place the metadata of the 3rd page in the file. That is easy to do and works well with the code that we have so far. However, that approach contains
an important flaw. What happens when the data file grows too large to fit all the metadata values in a single page. That isn't a far off concern. It will happen as
soon as the file exceeds the 4MB mark.

With the free space bitmap, I didn't concern myself with worrying about this. A 512GB file will use a total of 8Mb, so if we run out of space for the bitmap up, 
the cost of moving the bitmap to another location is negligble. With the pages metadata, on the situation is different. A 512GB file will use 1Gb of disk space to
hold the metadata for the entire file. At that size, moving it when needed is... prohibitive. 

There are other issues that we have to consider:

* Placing all the pages metadata in one location means that a disk corruption in the pages metadata section will have a chance of impacting a _lot_ of data at once.
* We _must_ be able to access a particular page's metadata in O(1) access time. That is, we cannot afford to do any sort of search. In the same way that accessing
  a page is done using pointer artimatic and serving directly from the memory mapped location, we need to have the same behavior for the pages metadata.

A good solution would be to place a metadata page every 4Mb. That will give us a constant computation to figure out where a particular page's metadata is. It will also
ensure that a single bad block in the disk will not have a disasterous impact on the entire data file. However, if we choose this method, we'll run into another issuer. 
Let's take a look at Figure 5.3 to see what would happen then.

![Placing a metadata page every 4 pages (simulating for page per 4Mb).](./ch05/img03.png)

Looking at Figure 5.3, can you see what is the problem with this approach? What if we want to do an allocation that is 5 pages long? We _have_ the free space for it.
We just don't have a continious range of it. It would probably be safe to accept this limitation. Having 4Mb max size limit for a single value isn't typically a 
concern. Even if you want to store large values, it usually make sense to break them to manageable pieces first. That said, I'm not perfrectly happy with this approach.

There is a limit already on the size of a value in Gavran, that is based on the `overflow_size` being a `uint32_t`, so 4Gb. That is big enough to either not care 
ever about this limit (most values) or knowing that you'll have values in the tens / hundreds of GB (typically videoes). At this point, you're not going to be dealing 
with the full data size anyway, so you can place it in multiple chunks.

That is actually a very important factor. It is rare to need to deal with a single buffer that is multiple GB in size. But we deal with multi megabytes buffers every
day. I just checked my phone and the last image I took and it was 2.5Mb, I took a screen shot and it was over 1Mb. Placing a limit as low as 4Mb is not sustainable.
If we want to allow users to allocate enough space to store 4Gb values, that means that the metadata pages needs to be at least 8Gb apart. 

To cover 8Gb of pages metadata, we'll need to use 16Mb of disk space. It means that we won't need to move the whole metadata section whenever we increase the size of
the database. But what about our current database file? We currently have it setup to start at 128Kb, how would we handle the metadata in this case? 

Listing 5.3 shows how we can compute the appropriate metadata page offset for a particular page. There are probably ways to make it shorter, but I wrote
the function in long hand to make sure that I'm getting the point across.

```{caption="Finding the relevant metadata page entry for each page in constant time" .c}
#define PAGES_IN_METADATA_PAGE (1024 * 1024)

static MUST_CHECK bool get_metadata_page_offset(
    uint64_t total_number_of_pages, uint64_t page_num,
    uint64_t *metadata_page_num, size_t *metadata_index)
{
  if (page_num >= total_number_of_pages)
  {
    push_error(ERANGE, 
    	"Requested page %lu is outside of the file limits: %lu",
        page_num,
        total_number_of_pages);
    return false;
  }

  uint64_t metadata_page_start, section_size_bytes;
  uint64_t metadata_page_end = page_num +
    	(PAGES_IN_METADATA_PAGE - (page_num % PAGES_IN_METADATA_PAGE));
  if (metadata_page_end < total_number_of_pages)
  {
    // full metadata section
    section_size_bytes = 
    	(PAGES_IN_METADATA_PAGE * sizeof(page_metadata_t));
    metadata_page_start = metadata_page_end;
  }
  else
  {
    // past the end of the file, so we 
    // need to compute the size of the remainder
    const uint64_t remaining_pages_for_last_section =
        total_number_of_pages % PAGES_IN_METADATA_PAGE;
    section_size_bytes = 
    	(remaining_pages_for_last_section * sizeof(page_metadata_t));
    metadata_page_start = total_number_of_pages;
  }

  const uint64_t section_size_pages = 
	  	section_size_bytes / PAGE_SIZE +
	    (section_size_bytes % PAGE_SIZE ? 1 : 0);
  metadata_page_start -= section_size_pages;

  const size_t index_of_metadata_in_section = 
  		page_num % PAGES_IN_METADATA_PAGE;

  const size_t index_of_page_in_section =
      index_of_metadata_in_section / (PAGE_SIZE / sizeof(page_metadata_t));

  *metadata_page_num = metadata_page_start + index_of_page_in_section;
  *metadata_index = index_of_metadata_in_section % 
  		(PAGE_SIZE / sizeof(page_metadata_t));
  return true;
}
```

There are two basic states that we need to handle in `get_metadata_page_offset`. We check where the `page_num` we got falls in an 8Gb range. If it falls
within the _last_ range^[more exactly, the last range in the file that isn't smaller than by 8Gb], we'll go the last page on that 8Gb range. If it 
falls on the last range in the file (which is smaller than 8Gb) we'll go to the last page in the file. 

Then we compute the size of the range and count backward from that range to find what is the size of the metadata section. The rest is just finding the
number of the relevant page in the metadata section and the index of the specific metadata value for that particular page. 

Let's see how we solve this function for a few options:

* `(total_number_of_pages = 16, page_num = 5)` - In this case, the file size is 128Kb (16 pages * 8 Kb). We know that we need a 
  section size of 16 * `sizeof(page_header_t)` (16 bytes). So the total section size in bytes is 256 bytes. That fits on a single page.
  The result is `metadata_page_num = 15` and inside that page, `metadata_index = 5`.
* `(total_number_of_pages = 131,072, page_num = 35,225)` - In this case, the file size is 1 GB. The section size for 131,072 pages is 
  2 megabytes or 256 pages. The section we are looking at isn't 8Gb in size, so the metadata section starts at `130,816`. We then compute that the 
  metadata page for `35,225` and find that it is on the 68th metadata section page. So the result is `metadata_page_num = 130,884`  and 
  the `metadata_index = 409`.
* `(total_number_of_pages = 1,310,720, page_num = 35,225)` - In this case, the file size is 10 GB. And we are looking for a page that is inside a 
  compelete 8Gb range, so we go to last page of the 8 GB range (`1,048,576`) and the metadata section size is 16 MB (2,048 pages). So the metadata 
  section starts at `1,046,528`. The metadata page for `35,225` is still 68 and the position inside the page remains 409. So the final result is 
  `metadata_page_num = 1,046,596` and the `metadata_index = 409`.
* `(total_number_of_pages = 1,310,720, page_num = 1,189,786)` - In this case, the file size is 10 GB again, and the page we are looking for is just 
  after the 9 GB mark. Because the range it belongs to is in the 8 GB - 10 GB area of the file, which isn't full 8 GB in size, we will start our search
  from the past in the file `1,310,720`, compute the size of the metadata section required (262,144 pages in 2 Gb, times 16 bytes = 4 MB or 512 pages).
  That means that the metadata section starts on `1,310,208` and the metadata page for this is in the 274th page in the metadata section. The final
  result  is `metadata_page_num = 1,310,483`  and the `metadata_index = 410`.

This may seem like overcomplicated, but it has major advantages in the design of the rest of the system. Note that in this case, I"m setting 
`PAGES_IN_METADATA_PAGE` to be 8 GB (1,048,576 pages with 8Kb each) as the range size. I'm doing this to ensure that I can fit the full 4 GB maximum
value between metadata sections. You can _change_ this value. I added the `pages_in_metadata_section` field to the `file_header_t` structure that serve
the same function as the `page_size`, to validate that the code doesn't try to handle a file that came from a different configuration.

Setting the `PAGES_IN_METADATA_PAGE` to a lower value, such as 32 MB (4,096 pages) will result in the same behavior, but instead of having a 16 MB 
metadata section every 8 Gb, we'll have 64 KB section every 32 Mb. The downside of this approach is that you won't be able to create store values that
are longer than 31.9 Mb. 

Aside from the additional complexity in looking up the page metadata, there is a possible stronger consideration to avoid this sort of approach. One 
major advantage in using the page header system is that you get data locality. With the page metdata being external to the page, we are going to need
at least _two_ accesses to get the page's data. I think that it is this reason that page headers are so common. 

There are few reasons why I think that this is a good approach:

* Having the page metadata in a centralized location means that we can pretty much guarantee that the relevant pages will be cached in memory. Even if 
  we consider very large databases. A 1 TB database will have 2 GB dedicated to pages metadata. But actually, we'll have 128 ranges of 8 GB each with a 
  16 MB metadata section. It is very likely that only some of those will be needed and be resident in memory. 
* Having the pages metadata near each other means that I can do page metadata scanning _more_ more efficiently. You'll see a good example of that when
  we'll start talking about data structures.
* Having a separate metadata section means that certain features are _much_ easier to manage. For example, in Voron, the reason that the page header is
  64 bytes is that support transparent data encryption. And we need to store the authentication tag of the encryption so we can verify it has't been 
  tampered with. With external metadata, not all databases will have to pay for this cost, only ones that actually need encryption.
* We can rely on the pages metadata in aggregate to store information about a range of pages that would require an additional page. Again, I'm jumping
  ahead a bit, we'll discuss this when we start building data structures.

After this length discussion, let's us see what is actually needed to add support for this to Gavran.

## Implementing page metaadata

The first place to start is in `initialize_file_structure`, where we'll need to allocate and setup the metadata section page. Listing 5.4 shows the
current structure of the `page_metadata_t`. 

```{caption="The current page metadata structure" .c}
enum page_flags {
  page_free = 0,
  page_single = 1,
  page_overflow_first = 2,
  page_overflow_rest = 4,
  page_metadata = 8,
} __attribute__((__packed__));
_Static_assert(sizeof(enum page_flags) == 1,
	"Expecting page_flags to be a single char in size");

typedef struct page_metadata {
  union {
    struct {
      uint32_t overflow_size;
      char flags;
    };
    char _padding[16];
  };
} page_metadata_t;
```

There is much here, because we haven't started to implement any data structures. We allow a page to be `page_free` or in one of several states. The `page_single` option
means that this is an allocated value that spans a single page. The `page_overflow_first` indicates that this is the first overflow page in a large value, spanning
multiple pages. The `page_overflow_rest` means that this is a page that is part of a large value, but it is not the first page in the sequence.

The nice thing about this structure is that all empty pages (which are usually zero filled by the operating system when we allocate the disk space) are already
ready. What we need to do is to mark the pages that are busy. Why would we need to do that both for the free space bitmap and on the metadata? 

One reason is double entry booking. That is, we'll verify that the value is correct and we didn't mess something. The other reason is that the `page_free` vs `page_single`
doesn't matter much here, but `page_single` vs. `page_overflow_first` has a big impact. The `page_metadata` allows us to mark a page as part as the overall meta pages 
that allow us to manage the database itself.

For now, we need to add the setup of the metadata to the initialization of the database file. Listing 5.4 shows the relevant code.

```{caption="Handling registration of pages metadata as part of creating the database" .c}
header->pages_in_metadata_section = PAGES_IN_METADATA_PAGE;

page_t meta_page;
size_t index;
for (size_t i = 0; i < busy_pages; i++) {
  page_metadata_t *metadata;
  pass(modify_page_metadata(&tx, i, &metadata));
  memset(metadata, 0, sizeof(page_metadata_t));
  metadata->flags = page_single | page_metadata;
}
for (size_t i = 0; i < header->number_of_pages;
     i += PAGES_IN_METADATA_PAGE) {
  uint64_t section_pages;
  get_metadata_section_stats(header->number_of_pages, i,
                             &meta_page.page_num, &section_pages);
  uint64_t base_page_num = meta_page.page_num;
  for (size_t j = 0; j < section_pages; j++) {
    page_metadata_t *metadata;
    pass(modify_page_metadata(&tx, base_page_num + j, &metadata));
    memset(metadata, 0, sizeof(page_metadata_t));
    metadata->flags = page_single | page_metadata;
  }
}
```

We start by scanning the busy pages (the header and the free space bitmap) as well as the pages that makes up the actual metadata section as well. This code is a bit
more complex than it really needs to be, because it is prepared to handle very large data sizes. We'll need that when we will start looking into how we can extend the
size of the file after the first initialization. The code in Listing 5.4 relies on the `modify_page_metadata` function, shown in Listing 5.6. 

```{caption="Pull just the relevant entry from the metadata section, ready for modifications" .c}
static MUST_CHECK bool
modify_page_metadata(txn_t *tx, uint64_t page_num,
                     page_metadata_t **page_metadata) {
  file_header_t *header = &tx->state->db->header;
  page_t page = {page_num, 0, 0, 0};
  size_t metadata_index;
  pass(get_metadata_page_offset(header->number_of_pages, page_num,
                                &page.page_num, &metadata_index),
       EINVAL, "Failed to metadata offset %lu", page_num);

  pass(modify_page(tx, &page), EINVAL,
       "Unable to get metadaa page for %lu", page.page_num);
  *page_metadata = (page_metadata_t *)page.address + metadata_index;
  return true;
}
```

We are using our own infrastructure to manage the database, which means that changing a metadata section page works _just_ like modifying a regular page. No special cases,
hurray! In addition to `modify_page_metdata`, there is also `get_page_metadata`, which is almost identical, but call `get_page` instead of `modify_page`.

And with the metadata properly setup, we can go back to `allocate_page` to manage it. Listing 5.7 shows the changes that we needed to make to accomodate this new feature.

```
// now need to mark it as busy in the bitmap
for (size_t j = 0; j < required_size; j++) {
  pass(mark_page_as_busy(tx, pos + j), EINVAL,
       "Unable to mark page %lu as busy", pos + j);
  page_metadata_t *metadata;
  pass(modify_page_metadata(tx, pos + j, metadata), EIN
       "Failed to metadata entry for %lu", pos + j);
  memset(metadata, 0, sizeof(page_metadata_t));
  if (required_size == 1) {
    metadata->flags = page_single;
  } else if (j == 0) {
    metadata->flags = page_overflow_first;
    metadata->overflow_size = page->overflow_size;
  } else {
    metadata->flags = page_overflow_rest;
    metadata->overflow_size =
      page->overflow_size - j * PAGE_SIZE;
  }
}
```

We also need to hanlde this change in `get_page`, `modify_page` and `free_page`, the full details are on Listing 5.8.

```{caption="Changes required to handle page metadata in the rest of the pager's layer" .c}
bool get_page(txn_t *tx, page_t *page_header) {
  assert_no_existing_errors();

  page_metadata_t *metadata;
  pass(get_page_metadata(tx, page_header->page_num, &metadata));
  page_header->overflow_size = metadata->overflow_size;

  // no futher changes
}

bool modify_page(txn_t *tx, page_t *page_header) {
  assert_no_existing_errors();
  
  page_metadata_t *metadata;
  pass(get_page_metadata(tx, page_header->page_num, &metadata));

  page_header->overflow_size = metadata->overflow_size;

  // no further changes

}

bool free_page(txn_t *tx, page_t *page) {

  page_metadata_t *metadata;
  pass(modify_page_metadata(tx, page->page_num, &metadata));
  memset(metadata, 0, sizeof(page_metadata_t));

  // no further changes

}
```

The code in `modify_page` uses `get_page_metadata` instead of `modify_page_metadata`. This is because even if the page itself was changed, you are likely not going 
to change the metadata. And we explicit don't expose the metadata as part of getting the page, we'll require that the caller call that directly.
There is another reason here, though. The `modify_page` function is called from `modify_page_metadata`, if we'll call `modify_page_metadata` from `modify_page`, 
we'll run into infinite recursion. We could handle it in a similar way to how we handled the `get_page`, but we want to avoid modifying pages that we don't need to.
It is wasteful.

The `free_page` function has an additional responsability, in addition to freeing the allocated page, it also needs to zero out the page metadata. 

## Working with multiple pages

## Extending the file