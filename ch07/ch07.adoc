== Implementing ACID transactions

ACID stands for Atomic, Consistent, Isolated and Durable. We have a (very limited) form of atomicity in our transactions right now, because we only write the changes
in the commit. That isn't nearly enough. We want to allow concurrent transactions to operate transparently, which means that we have to define _how_ they will operate.

Gavran is going to implement a model called single writer / concurrent readers. In this model, we split the transactions into two modes. We have read transactions
and a write transaction. At any given point in time, we may have any number of read transactions but only a single write transaction. Supporting multiple write 
transactions is usually something that you need to do if you have a chatty network interface, and as embedded database, that is not required for Gavran.

.Single write transaction vs. Concurrent writers
****
Most embedded databases implement some for of single write transactions. LMDB, Sqlite and Voron all share this design decision. Berkley DB has a similar 
limitation, but with Berkley DB, you can only have a single write transaction, with no concurrent read transactions.

Databases such as MySQL or PostgreSQL has the notion of concurrent write transactions, but they use a very different model. In a network database with 
transactions that involve multiple network round trips, the latency is too high to allow a single write transaction. You have to have concurrent ones and
deal with all the locking issues that this entails. 

The problem with concurrent write transactions is that they require that you'll implement locking.  In 
https://dl.acm.org/doi/full/10.1145/3226595.3226635[OLTP Through the Looking Glass, and What We Found There], about 30% of the overall performance goes 
to locking and latching to manage concurrent writers. A single threaded solution can actually be faster.
****

With concurrent readers, we need to decide on the concurrency strategy for Gavran. Take a look at Figure 13, where we show a timeline of a few transactions.

.A timeline of concurrent read transactions and a write transaction
image::{img-src}/fig13.png[]

Read transaction no. 1 was started after the write transaction and ended after the write transaction was committed. Read transaction no. 2 was created
before the write transaction and closed afterward. Only read transaction no. 3 was created after the write transaction commit. Only the third transaction
is going to be able to see the modifications made by the write transaction. The other two transactions are frozen at the time of their creation, seeing
unchanging data of the last transaction that has committed when they were opened.

Gavran is going to implement *snapshot isolation* for transactions. Once a transaction is opened, it will have a consistent and unchanging view of the 
database. A write transaction commit will only impact _later_ transactions.

This behavior is the first task in the road for ACID, the question is, how are we going to implement it? We already have _some_ of it down. While a 
transaction is running, other transactions are unaware of the changes that it is making because we use Copy On Write. What if we would extend that 
approach a bit and see where it takes us?

Consider the following timeline of operations, which shows what we have now.

1. Write tx is created
2. Read tx is created 
3. Write tx modifies page 2
4. Read tx reads page 2, see no changes (the data is held in buffer private to the write tx)
5. Write tx commits & closed
6. Read tx reads page 2, see the changes that happened because `txn_commit` wrote it.

That is the _current_ behavior of Gavran, and it is almost where we want it to be. The only change in behavior we need is that when the write transaction
commits, already opened transactions will _not_ see the changes. How can we implement this? Well, the reason that they can see the changes is that 
we write them to the file, which is shared. What would happened if we _won't_ write to the file? Let's find out. 

=== Implementing isolation between transactions

I'm afraid that I had to touch quite a few locations for this to work. I'm going to show them in pieces, so we can paint the whole picture. We'll start
with Listing 7.1.

[source]
.Listing 7.1 - txn.c - Delay writing the modified pages to the data file.
```
include::./code/txn.c[tags=txn_commit]

include::./code/txn.c[tags=txn_close]
```
<1> Setting the transaction flags as committed and the usages count as well as registering the current transaction state in a doubly linked list of 
committed transactions.
<2> If the transaction was _not_ committed, we can `free` the memory associated with it immediately.
<3> Reduce the usage count and call `txn_try_apply` , we'll discuss these later in this chapter.

The key change in `txn_commit` is that we are no longer writing to the file and `txn_close` is no longer freeing the modified pages.
Let's us see why we keep the pages around for committed transactions as well as the changes for `txn_create` in Listing 7.2.

[source]
.Listing 7.2 - db.h, impl.h & txn.c - Modifications to the transaction creation
```
// from db.h
include::./code/db.h[tags=tx_flags]

// from impl.h
include::./code/impl.h[tags=transaction_state]

// from txn.c
include::./code/txn.c[tags=txn_create]
```
<1> We now have flags that we must pass to the `txn_create` function, checking if the transaction is a read or write transaction. There is also a flag to
    mark that a transaction has been committed. 
<2> New fields in the `txn_state_t` structure to support the new behavior.
<3> If we have a read transaction, we use the state from the `last_write_tx` and increment its usage count.
<4> New code here, setting the `previous_tx` from the `last_write_tx` for write transactions and incrementing the transaction id.

Things start to get interesting now. Read transactions now require no allocation or work to start. And write transaction set themselves up to be the next 
state for all future read transactions. What is actually going on in the code for implementing transactions? Let's check Listing 7.3.

.Thread safety, what's that?
****
I'm currently building Gavran with absolutely no thread safety. That is intentional, since thread safety adds a significant complexity for the code and 
require very careful setup to get working properly. I want to first get the design laid out before we start touching on additional concerns. 

We'll have complete thread safety, mind you, but I want to add that once all the moving pieces has stopped shaking.
****

[source]
.Listing 7.3 - txn.c - Get a page, from the current transaction or a past one or the file.
```
include::./code/txn.c[tags=txn_get_page]
```
<1> Here we iterate over the past transactions and try to find a modified copy of the page. Only if it doesn't exist do we go to the file.

Listing 7.3 shows the beauty of this approach, when we need to find a page, we start search the modified page map, just like before. But we are going to look
in our own map as well as the maps of _past transactions_. The `previous_tx` creates a linked list of transactions (and their modified pages) that we
can use to find the most recent version of a page. We'll only check the data file for the page if there is no transaction in the list that has this page. 

You can see how that works in Figure 14, we have 5 transactions there, 3 writes and 2 read transactions. You can see how the effective pages for each one
of those transactions.

.Multiple concurrent read transactions each with their own immutable view of the world based on past write transactions mapping
image::{img-src}/fig14.png[]

For read transaction 2, the effective value of page 3 is from transaction 1 while it is the value from transaction 3 for read transaction 3. This allow each
transaction to have its own view of the world and protect us from data being modified while we read the data. Listing 7.4 show how implement `txn_modify_page`
with the new behavior.

[source]
.Listing 7.4 - txn.c - Modify a page whose source may be the data file or a previous transaction
```
include::./code/txn.c[tags=txn_modify_page]

include::./code/txn.c[tags=page_metadata]
```
<1> We validate that this is a write transaction, a read transaction cannot modify the database.
<2> We first check if the page was modified already in this transaction
<3> We scan through the previous transactions to see if we have a copy of the page.
<4> If we can't find the page, we'll read it from the file.
<5> We create a copy of the page and register it in the transaction.
<6> I've also simplified `txn_get_metadata` and `txn_modify_metadata`, to express them based on our existing API rather than repeat ourselves.

The `txn_modify_page` function now checks older transactions for updated copies of the page before reading it from disk. It is important to note that if the
page was found in an older transaction, we'll create a _copy_ of the page. That is because other transactions may be looking at it, so we need to maintain
the immutability of the data from already committed transactions. In effect, we do another Copy On Write on top of the older copy. 
This approach allows us to maintain multiple _levels_ of isolation in our transactions.

.MVCC - Multi version concurrency control
****
This style of work, when we have multiple concurrent version of the data at play, is called MVCC. This is used by databases such as LMDB, Sqlite, PostgreSQL 
and Voron. The _manner_ in which they achieve this ability is very different, however. If you want to read more about it, 
https://ayende.com/blog/175073/voron-internals-mvcc-all-the-moving-parts[I have a post about Voron's MVCC implementation], and 
https://medium.com/@kousiknath/how-mvcc-databases-work-internally-84a27a380283[this post discusses PostgreSQL, LMDB and CouchDB's implementations].
****

What about when we have _no_ previous transactions? This is handled at the database startup, as shown in Listing 7.5.

[source]
.Listing 7.5 - impl.h & db.c - Setting up the `last_write_tx` as part of the database startup
```
// from impl.h
include::./code/impl.h[tags=database_state]

// from db.c - inside db_create
include::./code/db.c[tags=default_read_tx]
```
<1> Added new fields to track the most recent write transaction.
<2> Added a default `txn_state_t` to the allocated size for `db_state_t`.
<3> Setup the default read transaction which will be used as the base for `txn_get_page` and `txn_modify_page`.

And that is pretty much it. We have MVCC and isolation between transactions. We are well on our way to having _real_ transactions. Hurray!
Listing 7.6 shows working code using this feature.

[source]
.Listing 7.6 - main.c - Using MVCC features to check a page that was modified in a write transaction created after the read transaction.
```
include::./code/main.c[tags=mvcc]
```
<1> Creating a write transaction
<2> Creating a read transaction (before committing the write transaction), none of the write transaction changes will be visible to the 
    read transaction.
<3> Committing the write transaction, which does not change what the read transaction can see.
<4> There is nothing to output here, this transaction cannot see changes that happened after it was started.

As you can see, `rtx` was created after `wtx`, but it isn't aware of anything that happened inside `wtx`, even after the commit of `wtx`. 

=== Writing to disk again

The approach we used is pretty elegant, we get to use Copy on Write to automatically maintain a consistent view of the world even when there 
are ongoing changes. It is also _highly_ problematic, because we haven't addressed a number of issues:

* When can we write the data to the data file?
* When can we `free` the memory that we use for Copy on Write?

As it currently stands, we have a transient dataset as well as a memory leak. And as we have more and more transactions, the need to iterate
over the past transactions is going to make our code slower and slower. We need some way to clean this up, so we need to decide when we can 
write to the disk. Let's take a look at Figure 15 and see what we can do about this issue.

.At the top, committed write transactions, at the bottom, read transactions that refers to them
image::{img-src}/fig15.png[]

What we can see in Figure 15 is that _all_ the read transactions belong to transaction 4 or higher. There are no read transactions associated
with transaction 3. What does this mean? It means that we can be sure that no transaction is going to to try to read the data from the file
for the pages modified by transaction 3. 

At the point in time shown in Figure 15, it would be safe to write all the pages from transaction 3 or lower to the data file. It wouldn't 
be possible for us to _release_ the transaction yet, however. The other transactions may still refer to the pages that it holds. We would
need to wait until all of _those_ transactions are closed before we can free the memory held by those transactions.

In other words, we have two events that we need to consider here:

* Writing the data from the transaction to the file can be done as soon as there are no transactions that are looking at the transaction 
_or any older transaction_. 
* Freeing the memory from the transaction can be done after all the transactions that were open at the time that we wrote to the data file
are closed.

You already saw that we have `usages` field on a `txn_state_t` now. This is used to tell whatever there are any read transactions that are
using this transaction state as their base. There is also the `next_tx` field, which is used to go from each write transaction
to its successor (and together with `previous_tx` creates a doubly linked list). We use these in order to tell what is the current state
of the system at large. 

Let's look at the code in Listing 7.7 and then we'll discuss how it works in depth.

[source]
.Listing 7.7 - txn.c - The `txn_gc_tx` will check which transactions are still referenced and which can be freed
```
include::./code/txn.c[tags=txn_gc_tx]
```
<1> The `default_read_tx` is always the oldest transaction, we start scanning unused transaction from it.
<2> An unused transaction is one whose `usages` is zeroed. We iterate through the transactions list in reverse creation order to find the 
    newest transaction that has no references _and_ all its predecessors also don't have any references.
<3> We create a single hash table with all the unique pages from all the transactions that has no references. A page that appears in multiple
    transactions will only show up once, as its latest version.
<4> Actually write to disk. This structure ensures that there is no active transaction that is going to look at those pages on disk,
    so it is safe to so while other transactions are running.
<5> Check if the transactions we don't care about can be removed in the transactions list that we know no one is looking at.
<6> See if the current transactions can be cleaned up immediately or if we need to register them for later disposal.

That is a _lot_ of behavior to go through, but I managed to break it down nicely to separate functions, which makes things easier. 
The `txn_gc_tx` starts by scanning from the oldest transaction (always `default_read_tx`) and move forward by following the `next_tx` links.

The aim of this search is to find the _newest_ transaction that has no references directly or to any of its predecessors. Such a transaction
is ready to be sent to disk. There is no other viable transaction that can look at the pages that were modified by these transactions. We can
safely send them to disk.

It is _very_ common for multiple transactions to modify the same set of pages. Imagine the metadata page, for example, which will be modified
in many transactions. That means that the pages that were modified by consecutive transactions will have multiple copies in memory. That is 
what MVCC is all about, after all. We don't want to write the data to disk in transaction order, we'll need to overwrite some pages many times
so the first thing we do is scan through the transactions list and find all the unique pages, favoring the latest version of each of them.
That is the responsibility of `txn_merge_unique_pages`, shown in Listing 7.8.

[source]
.Listing 7.8 - txn.c - The `txn_merge_unique_pages` create a hash table of all modified pages, with the latest version of each page.
```
include::./code/txn.c[tags=txn_merge_unique_pages]
```

The `txn_merge_unique_pages` function in Listing 7.8 starts from the newest transaction that is known to be abandoned and goes backward,
trying to find pages modified by the transactions that weren't modified by prior writes. That helps us avoid overwriting popular pages,
such as the metadata pages, over and over again. 

Note that we are actually re-using the hash table that is attached to the `txn_state_t` of the newest eligible transaction. That simplify
the amount of code that needs to be written. It is safe to change this `txn_state_t`, since we have just proven that no one else is looking 
at it. 

Next we have `txn_write_state_to_disk`, shown on Listing 7.9.

[source]
.Listing 7.9 - txn.c - The `txn_write_state_to_disk` writes the modified pages to the data file
```
include::./code/txn.c[tags=txn_write_state_to_disk]
```

There isn't much to say about the code in Listing 7.9. It should be familiar to you, it used to reside in `txn_commit` and did the same work
there. The only thing to notice here is that we are going to be running this on a `txn_state_t` that contains pages from multiple transactions.
The input of `txn_write_state_to_disk` is the result computed by `txn_merge_unique_pages`.

Thus far, the implementation has been fairly straightforward. There is one caveat that we have to handle, though. How are we going to `free` 
the memory for those transactions? Take a look at Figure 16, where all transactions are after Transaction no. 7. On the face of it, we should
be able to free the memory for Transaction no. 5 and 6 right now, no? 

The problem is that Read Transaction no. 7 may still follow its link back to Transaction no. 5. We could only release the memory for Transaction 
no. 1 when all the transactions that might consider going back to Transaction no. 5 are done. Because of this, we have a somewhat complex model
in memory for storing transactions. Let's take a look at Figure 16 and try to understand what is going on here. 

.The memory model of transactions referencing older transactions and keeping track of when old transactions can be deleted
image::{img-src}/fig16.png[]

If we'll call `txn_gc_tx` on the state in Figure 16, we'll find that the newest transaction we can work with is Tx 6. All other transactions still
have references to them, after all. We can see that in terms of active transactions, we have transaction 5 and 6. For now, let's ignore the 
1 - 4 transactions we have in the image. 

Calling `txn_merge_unique_pages` will merge the pages that was modified in transactions 5 and 6 and `txn_write_state_to_disk` will write those pages
to disk. The question is, when can we call `free` on that memory. It turns out that we can't, really. Transaction 7 may need to go back into pages
held by Transaction 6, and so on until we get to the end. 

One of the things we do in `txn_cleanup_now_or_register` is to break this chain, but that isn't immediate, we need to wait for all the existing 
transactions to complete before actually calling `free`. And we need to _remember_ to call `free` on the data. We do that by setting the value 
of `transactions_to_free` in the current active transaction to the  the newest transaction we can clear.

The structure that we have in Figure 16 is almost lattice like. We need to scan backward on the list of transactions and find any such registrations 
to free the transactions. A bit complex, I'll admit, but it works to clear up the memory and it doesn't actually make things too complicate, unlike
some other schemes, which are far more horrible. 

With that explanation behind us, let's look at Listing 7.10, which shows the actual code to implement this behavior.

[source]
.Listing 7.10 - txn.c - Implementing (careful) freeing of transaction, only when they are no longer visible to any code. 
```
include::./code/txn.c[tags=txn_free_registered_transactions]

include::./code/txn.c[tags=txn_cleanup_now_or_register]
```

I think that the most interesting part is in `txn_cleanup_now_or_register`, where we check if there are no active transactions, and we can `free` the 
memory immediately. If not, we register the transactions in the current active transactions and we'll get to it in the future.

With those out of the way, we are left with some minor utility functions shown in Listing 7.11.

[source]
.Listing 7.11 - txn.c - Actually doing the work to free the transactions
```
include::./code/txn.c[tags=free_tx]
```

The behavior around `txn_gc_tx` is complex, enough that I think that it would be a bad idea to move forward before we covered this piece of code with
a good set of tests. This is a good time to stop then and write some units tests. We'll get to that in a second. I want to look at what we have achieved
and what remains to be done first.

We now have pretty much figured out how to implement Atomicity and Isolation piece of ACID. We have a single write / concurrent reader system where readers use 
_snapshot isolation_ model for consistency. Once a read transaction is opened, the world is frozen, as far as the transaction is concerned. At the same
time, we may create and commit write transactions and not have to do any coordination. There is a hurdle that we need to go through. Managing exactly when
we can write the data to disk and at what point we can actually free the memory is not trivial.

.The downside of MVCC
****
Using a system with MVCC is easy. You don't need to worry about locks or contentions. The way we implemented it in Gavran means that we have full snapshot
isolation without really having to do much work at all. However...

The design we have here has a weakness that must be considered. Gavran is only able to write to the data file (and discard Copy on Write memory) when there
are no more read transactions that reference those pages. That means that if you have a read transaction active, it is going to block Gavran from writing
to the disk anything that was written after that read transaction was opened.

In other words, if you have a long lived read transaction, you are going to stall the process of writing to disk. In practice, in embedded databases there
is rarely much need for long lived read transactions. And the issue only happens when you have:

* A long lived read transaction.
* A _lot_ of write transactions.

It takes both issues to cause the problem to manifest, and the combination is quite uncommon. Nevertheless, it is something to be aware of. The same issue
is also present with other databases implementing MVCC.
****

What we _don't_ have yet is the durability story. We are doing buffered I/O, which means that it is entirely possible for a power failure to cause us to
lose data or corrupt the data file. There is a good path forward from where we are now to get to durable system, which we'll explore in the next chapter.
And now, let's get to unit testing...

=== Unit tests

Write tests here