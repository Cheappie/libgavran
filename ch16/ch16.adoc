== B+Trees

|=== 
| Data Type    | B+Tree
| Write cost   | O(logN)
| Lookup cost  | O(logN), exact match, range queries
| Data type    | `sorted_map<span_t, uint64_t>`
| Maximum size | Unlimited
| Iteration    | Sorted iteration
|===

A B+Tree is an adaptation of a binary tree to the realities of the hierarchy of storage access speed. This is probably a very strange way to phrase things, I know.
Let's go back a step. I'm going to assume that you are familiar with https://en.wikipedia.org/wiki/Binary_search_tree[binary search trees] in this chapter. If you 
need a refresher, the Wikipedia page on the topic is a great introduction. Common implementation of binary search trees are AVL Trees and Red/Black Trees.

The key observation about binary search trees is that each time you visit a node, you cut the search space in half. That gives you the very desired property of 
`O(log~2~N)` insert and query times. There is a problem with such trees, however. You have to do a _lot_ of pointer chasing to get to the final result. If all your 
data is in main memory, you can usually ignore this cost (not really, but we'll pretend for a while). When your data is stored on disk, the situation is different.

Consider the code is <<naive-search-tree>>, which implements a binary search on top of a file. What would be the cost of running this code? The complexity is still
going to be `O(log~2~N)`, after all, but what would be the _cost_?

[source]
[[naive-search-tree]]
.A naive search tree implementation on top of FILE I/O
----
bool bst_find(FILE* f, char* key, uint64_t* val){
    node_t node;
    read_node(f, &node);
    int res = strcmp(key, node.key);
    if (res == 0){
        *val = node.value;
        return true;
    }
    int64_t pos = res < 0 ? node.left : node.right;
    if(pos < 0) return false; // not found
    fseek(f, pos, SEEK_SET);
    return bst_find(f,key,val);
}
----

In <<naive-search-tree>> we are doing one of the most expensive operations in terms of file I/O, small random reads. If the file in question holds 10,000 items
we'll need to perform _fourteen_ reads from the disk. Let's talk about 
https://www.freecodecamp.org/news/must-know-numbers-for-every-computer-engineer/[latency numbers every computer engineer must know]. This is a list of some really
interesting numbers in terms of latency. Doing a seek on a (good) HDD takes 3 milliseconds. That means that on a hard disk, the best speed for the `bst_find()` call
with just 10,000 items is 42 milliseconds. Even when running on SSD, you'll still have a cost of > 0.25 milliseconds running this function. That is a _lot_.

I'm using a very low number of items in the tree intentionally, to show how expensive binary search tree is. When the cost of accessing some parts of the tree are not
equal, the binary search tree costs are very high. This is were B+Trees are coming into play. Instead of having a node with just `left` and `right` pointers, we are going
to have much bigger nodes and an array pointers to child nodes. You can see how that looks like in <<btree>>.

[[btree]]
.A B+Tree has multiple pointers from each node to its children
image::btree.png[]

The idea with B+Trees is that we don't want to use `log~2~` for each node, which is the case where we have just two pointers. The typical number of pointers in a B+Tree
node is in the _hundreds_. That means that if we have 10,000 items, we don't need to do `log~2~(10,000)` but `log~256~(10,000)`. That means that instead of having 
to deal with 14 disk calls, we are going to need just 2. And if we raise the amount of items?
With 10 million items, a B+Tree with 256 pointers per node will request just 3 disk accesses. A binary search tree (just two pointers) will require 24 reads from the disk.

B+Trees are _far_ more efficient in this kind of environment. Indeed, B+Trees are on of _the_ most common persistent data structure you'll find. There is a _vast_ 
body of research on B+Trees and they have been in use for over half a century.

.B+Trees and memory hierarchies
****
I mentioned in the beginning of this chapter that B+Trees are meant to adapt a binary search tree to the realities of memory hierarchies. In any modern computer, you'll
find that you have roughly the following hierarchy of memory access speed:

* Registers
* L1 Cache
* L2-L4 Cache
* Main memory
* Disk

I'll admit that this is _still_ a simplified model. If you are running on NUMA nodes (non uniform memory access, common in multi socket machines), for example, you have
local memory and remote memory as well. Disk is often divided into multiple layers with different access speeds as well. 

In general, regardless of how you store the data, there are usually huge difference in cost vs. size vs. speed between layers. It takes a nanosecond to read a value from
the L1 cache, but 100 nanoseconds to read the same value from RAM. It takes 16 micro seconds to read a value from SSD, and far longer from an HDD. In general, the bigger
the storage, you can assume that the more expensive it is. And the economic realities means that you are going to have to deal with memory hierarchies. 

You _can_ get machines with many TB of RAM, mind you. That still doesn't mean that you can ignore the memory hierarchy. I looked at AWS and found `u-24tb1.metal`, which is
a bare metal server with 24 TB of RAM. This has 8 separate NUMA nodes to take into account. It has 448 cores running `Intel Xeon 8280L`. These chips has
1.75MB of L1 cache, 28MB L2 cache and 38.5 MB L3 cache. Even in this case, you have to juggle the memory hierarchy and the different latencies between them. 

B+Tree works in pages which allows us to take advantage of locality of reference. That was initially meant for reducing disk access, but it turns out that they work great
for reducing costs in any memory hierarchy. A good discussion on the topic is also on the https://queue.acm.org/detail.cfm?id=1814327[You Are Doing It Wrong] paper. 
****

This is meant to be a practical book, not a theoretical discussion on implementation decisions. For that, read the 
https://w6113.github.io/files/papers/btreesurvey-graefe.pdf[Modern B+Tree Techniques] paper does a great job covering the full details of the history of design and 
implementation of B+Trees.
I'm going to outline my approach for implementing the B+Tree for Gavran and explain what I'm doing, without going too deeply into alternatives or divergent design.

We previously built an extendible hash table and we simplified our model by limiting the data structure to be `map<uint64_t, uint64_t>`. In the same manner, we are
going to implement a B+Tree that would behave like a `sorted_map<span_t, uint64_t>`. In other words, we're going to have the API shown in <<btree_api>>.

[source]
[[btree_api]]
.`gavran/db.h` - API for working with B+Trees in Gavran
----
include::../include/gavran/db.h[tags=btree_api]
----

You can see that the API is very similar to the one we have built for hash tables. For now, I'm omitting the API for iterating over the B+Tree, we'll discuss this 
later on in this chapter. Because B+Trees are ordered, this means that the iteration API is more complex, for now I want to focus on the Read / Write / Delete operations.
Let's look at the actual implementation and see how that works.

=== Creating a B+Tree

.B+Trees can be complex beasts
****
In terms of line of code, the B+Tree implementation in Gavran is on par with the Extendible Hash Table and the Containers we previous looked at. In terms of complexity,
however, I find the B+Tree code to be far more complex.

Conceptually, the idea of a B+Tree is fairly simple and obvious, but the implementation tends to be quite tricky. One of the most common issues is that any modification
to the tree may have repercussions higher up the tree. When dealing with variable length values, that can make the code more complex and expose several edge cases that
you have to handle. 

To manage that complexity, we have the `btree.debug.c` file, which has API to dump a textual representation of the tree that you can inspect by hand. That can be 
quite important to understand how the B+Tree is mutated by your changes. It can be invaluable for understanding and debugging issues. When writing Voron, I actually had
the engine output HTML trees that would allow inspection of the B+Tree structure of the data. 
****

A B+Tree is composed of two types of pages, we have a _branch_ page, which points to other pages, and we have _leaf_ pages, which hold the actual value. In both cases,
they are structured in the exact same manner, since the only value we allow in our trees are `uint64_t` and that is the value what we need to store in the branch pages 
to point to the leaf pages. As such, the process of creating a B+Tree is simple, as you can see in <<btree_create>>.

[source]
[[btree_create]]
.`btree.c` - Creating a new B+Tree with a single leaf page 
-----
include::./code/btree.c[tags=btree_create]
-----

There isn't much going on in `btree_create()`, is there? We simply allocate a page and setup its metadata. You can see that like the container pages, we have the notion
of `floor` and `ceiling` values for the page. Indeed, much of the same techniques are reused between the two data structures. 

=== Writing to the a B+Tree

Writing a value to a B+Tree is done using the `btree_set()` method. It accepts a `btree_val_t` which has the key and the `uint64_t` value and may optionally return the
old value of the key. In the container, we were able to simply able to append the new value to the page wherever we had room, but in the case of the tree, we cannot do
that. One of the key aspects of a B+Tree is that it is sorted, after all.

[IMPORTANT]
.Keys in the B+Tree
====
The B+Tree we will use is going to use binary data as keys. The key is of type `span_t`, so the key _data_ is just any arbitrary memory of some size. We are going to limit
the key size to 512 bytes. It simplify our lives and there is rarely the need for larger keys. 

If the keys are arbitrary data, how can we sort on them? We are going to use the `memcmp()` function as the sorting function. It means that the keys you'll generate should
be sortable in lexical order using their raw binary value. It would be simple to provide a comparison function to the B+Tree to allow for more complex comparisons, but 
I find that it is usually only make things more complex to the implementor and the user alike. 

The fact that the keys are opaque bytes has some interesting implications on sorting when using numbers, for examples. We'll address this in detail later in this chapter.
====

The structure of a page in the a B+Tree is shown in <<btree_page>>, you can see the metadata on the left, with the ceiling and floor markers. Then we have the values.
The bottom of the page is also used to store the indexes of the values in the page, and the positions of the _values_ in the page doesn't matter. It is stored on a first
come, first served basis. 

The order of the _positions_ on the page, on the other hand, matters greatly. If you'll look at the bottom of the page in <<btree_page>> you can see that the positions
themselves are sorted, using the key.

[[btree_page]]
.The structure of a single B+Tree page
image::btree_page.png[]

In other words, when we need to insert a new item into a B+Tree page, we'll append the actual value to the top of the `ceiling`, lowering it along the way.
This is just like we do with the container pages.
But instead of just recording the index of the value in an arbitrary location, we'll sort the `positions` array at the bottom of the page according to the sort order of 
the key.

The idea is that this means that we'll have to move far less memory, which is more efficient in general and will also play really well with our WAL format. As a reminder,
we use a diff based WAL format, which means that the less changes we can make to a page, the less we have to write to disk.

==== Searching a key in a page

Let's look first at what is probably _the_ most important piece of code for our B+Tree, the sort routine shown in <<btree_search_pos_in_page>>.

[source]
[[btree_search_pos_in_page]]
.`btree.c` - Searching for the right position of a key inside a B+Tree page
-----
include::./code/btree.c[tags=btree_search_pos_in_page]
-----

The code in <<btree_search_pos_in_page>> is a simple binary search. It turns out that most binary search implementation have a bad bug in them, which you can read in 
https://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html[Nearly All Binary Searches and Mergesorts are Broken]. 
Our code has the fix for this, but it wouldn't matter anyway for our purposes, we are dealing with much smaller range of values. 

We are doing a binary search through the page's entries. Using the `positions` array to get to the stored key and compare it to the one what was supplied to the function.
We start at the middle of the `positions` array and we split the remaining range in half each time we find a greater or lesser value. If we find an exact match, we can 
return the position immediately. 

If we _don't_ have a match, we have to return where the value _should_ go. If the `last_match` we had was smaller than the current value, then the value needs to be on the 
_following_ position, so we increment it. Finally, we can return the bitwise complement to the position, to indicate a missing value. The `\~` operator allows us to distinguish
between a value that was found in the page or not. But why not return `-1` or something similar?

We are going to use `btree_search_pos_in_page()` to tell us where we should be _placing_ a value in the page, not just where it _is_. Using `\~` allows us to do the work of
searching in the page just once. Why does this matter? We'll be calling to `btree_search_pos_in_page()`  a _lot_. 
If you'll run the B+Tree code under a profiler, you are very likely to find this function in your hot spots. Any optimization in the search is going to have a big impact on
our B+Tree performance.

==== Finding _where_ to make the write

The `btree_set()` function is shown in <<btree_set>> and isn't really that interesting. All it does is find the appropriate page to set the value on and then set the value
on that page. 

[source]
[[btree_set]]
.`btree.c` - Setting a value in the B+Tree
-----
include::./code/btree.c[tags=btree_set]
-----

The work in `btree_set()` can be divided into finding the right page to set the value using `btree_get_leaf_page_for()` and actually setting the value in `btree_set_in_page()`.
Both of them are far more complex under the covers than you might expect. Let's start with the first step, finding the leaf page. Why do we even need that?

A B+Tree is composed of branches and leaves. The actual values for the keys we store in the B+Tree are held only on the leaves. On the branches, we simply hold pointers to the
leave pages. So in order to actually set the key and value on the B+Tree, we find need to find the appropriate leaf page to add the value to. Let's look at 
<<btree_get_leaf_page_for>> to see how this is done.

[source]
[[btree_get_leaf_page_for]]
.`btree.c` - Finding the appropriate leaf page for a given key
-----
include::./code/btree.c[tags=btree_get_leaf_page_for]
-----

Starting from the root page, we search for the given key using `btree_search_pos_in_page()`. If we are on a branch page, we use the position from the search to go down the tree
toward the appropriate leaf page. If you'll look back in <<btree>> you'll see that the branch page don't hold all the keys of the leaf pages. Instead, it hold the a key
that is less than or equal to the first key in the page that it points to. That means that when scanning branch pages, we'll very often see searches that are beyond 