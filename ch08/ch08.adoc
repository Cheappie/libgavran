== Implementing durable writes

We now have transactions that support Atomicity and Isolation, but all our writes are done using buffered I/O. That means that we are subject to a whole
_host_ of potential issues. The most obvious one is power loss. If we pull the plug on the system immediately after a transaction is committed, we'll
lose data. 

For that matter, we only write data to disk after all existing transactions are no longer looking at it. If we have a long running transaction, that may
prevent us from writing to the disk for a _while_, significantly increasing the risk of data lose. <<data_loss>> shows an example of losing data.

[source]
[[data_loss]]
.`test.c` - Losing committed writes because of a rude abort
```
include::./code/test.c[tags=data_loss]
```
<1> Opening a read transaction which will prevent us from writing to disk.
<2> Crete write transaction, write to a page and *commit* the transaction.
<3> Simulate a crash byt simply closing all the resources of the database rudely.
<4> Open the database again and try to read the committed data.

If you'll run the code in <<data_loss>> you'll see that after we open the database the second time, the committed data was lost. The changes never made it to disk.
There are valid reasons why you'll want to have a storage engine with this behavior. Performance is usually the cited concern, and for some systems, that
is an appropriate choice. Not for Gavran, I want it to be _transactional_, which means full durability. If a transaction commit was successful, it needs to be 
there through anything short of actual hardware failure that corrupted the data on disk. 

.It doesn't end with a `write()`
****
In <<data_loss>>, the issue is that we aren't writing to the disk. If we _were_ to write to the disk, however, it would only mask the problem. Writing to the
disk is anything but easy, if you care about durability. We talked about this a bit in Chapter 3, when we had to jump through hoops to get the file creation
to persist regardless of what is going on.

When writing to a file, we need to take into account that calling `write()` is just the start. The OS may decide to arbitrarily delay the actual write to disk
and even the disk may buffer the writes. The order in which you issue writes and the order in which they are persisted is not guaranteed. And writes to the
disk are _not_ atomic. 

https://danluu.com/deconstruct-files/[Files are fraught with peril] and http://danluu.com/file-consistency/[Files are hard] posts sum up the situation 
quite nicely. I also referred to the https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-pillai.pdf[All File Systems Are Not Created Equal]
paper (also called ALICE paper) in Chapter 3. In the paper, a whole bunch of applications (including multiple database engines) were tested and 
almost all of them had some issues around properly writing to disk. We clearly need more than a simple `write()` to help us.

We'll be testing Gavran with https://github.com/madthanu/alice[ALICE tracing] later in this book.
****

We know that calling `write()` won't be sufficient, but before we can start on fixing this, we need to first understand where we'll be writing _to_. We
can't write to the data file, there are transactions that are looking at the data there. We have to write to some other location.

=== Writing to a Write Ahead Log

Instead of writing to the database file directly, we'll write all the changes that we _want_ to make to another file. The technique is also called 
Write Ahead Log (WAL) and is used by the vast majority of databases that want to achieve durability. There are two exceptions that I can think of with 
regards to the use of WAL to allow durability of transactions:

* LDMB - Instead of using using WAL, it uses Copy on Write to other pages in the same data file. In other words, you don't have stable page numbers, but
each time you `modify` a page, a new page number will be allocated for you. That has implications for the amount of data that changes whenever a 
modification occurs. You have to `modify` not just that particular page but any that references it. On large databases, that can cause a single byte
change to require dozens of KB to be written to disk. The upside of this approach is that LMDB requires no startup logic or recovery phase.
* CouchDB - Instead of a WAL, it uses an append only mode. Writes are always done at the end of the file. Similarly to LMDB, a `modify` will cause the
data as well as anything that reference it to be written to the end of the file again. On startup, the database scans from the end of the file to find
a commit marker and start from there. An additional downside is that the database needs to run occasional compactions to free wasted disk space.

The idea of a WAL is that we'll write what we _intend_ to do, so if we crash, we can can replay our actions and end up in the same situation as we 
were before the crash.

We'll start working on the WAL by looking at the interface it exposes, shown in <<wal_api>>.

[source]
[[wal_api]]
.`gavran/internal.h` - The internal API for the WAL in Gavran
```
include::../include/gavran/internal.h[tags=wal_api]
```

The roles of these functions are:

* The `wal_open_and_recover()` function is called from `db_init()`, to run over the WAL and apply any changes from committed transactions. 
* The `wal_append()` is called from `txn_commit()`, that is what ensures that the data is durable. 
* And `wal_checkpoint()` (and `wal_will_checkpoint()`) is called from `txn_write_state_to_disk()` to tell the WAL that the transactions up to the specified `tx_id` have been safely stored in the disk 
and can be cleared from the WAL.
* And `wal_close()` is called from `db_close()` to safely shut down the WAL (note that is explicitly not required for things to continue working).

We'll start looking at the WAL logic from the data structures that are here to support it, shown in <<wal_data_structs>>.

[source]
[[wal_data_structs]]
.`gavran/db.h` - In memory structures used to manage the WAL behavior while the database is running
```
include::../include/gavran/db.h[tags=wal_data_structs]
```

The `wal_state_t` is used to manage the overall WAL state, it is an in memory structure only, held by the `db_state_t`. This is used to manage the WAL interactions
while we are running the database. In addition to the in memory data, there is also _persistent_ data that is written to the WAL itself. Those are shown in <<wal_txn_t>>.
Note that in the case of both <<wal_txn_t>> and <<wal_data_structs>>, we are showing the _end_ result. In this chapter we are going to work with some of the data in these
structs, and we'll complete the WAL work in the next chapter. 

[source]
[[wal_txn_t]]
.`wal.c` - Persistent structures used to store transactions data to disk
```
include::./code/wal.c[tags=wal_txn_t]
```

The `wal_txn_t` represents a single transaction that was written to the WAL. And the `wal_tx_page_t` structure holds information about a single page modified in that transaction. 
Those can all be considered as the _metadata_ of the transaction in the WAL. Let's see how we are using this in the WAL, <<wal_append>> shows the `wal_append()` function, which persists
the transaction's operations. We call this function from `txn_commit()`, and a successful call to `wal_append()` ensures that if there is any kind of failure, when we start the database
we'll be able to recover any changes made by the transaction.

[source]
[[wal_append]]
.`wal.c` - Appending the transaction's data to the WAL
```
include::./code/wal.c[tags=wal_append]
```

The `wal_append()` function isn't actually doing much. It ask `wal_prepare_txn_buffer()` to do the work, then it writes the data to the `cur_file` using `pal_write_file()`. There is 
some book keeping to manage where the next write will go to, but that is about it. For now, I'm ignoring things like `current_append_file_index` and WAL file management, these will 
be explained in the next chapter. What is important right now is to understand that we gather all the data from the transaction and write it to disk. Later on we can use this information
to recover on startup, repeating any committed transaction.

This ability to replay transaction by writing the modifications they made to the disk is a _really_ important property of the WAL. It means that we can mostly be free from considerations
of what changes happened, what is the _meaning_ of them, etc. We are dealing with the raw pages and look at the data as binary changes that we can operate on.
The interesting work in appending to the WAL is happening in `wal_prepare_txn_buffer()`, which is shown in <<wal_prepare_txn_buffer>>.

[source]
[[wal_prepare_txn_buffer]]
.`wal.c` - Preparing the buffer to write to the WAL
```
include::./code/wal.c[tags=wal_prepare_txn_buffer]
```

The purpose of `wal_prepare_txn_buffer()` is to prepare the buffer, and it is well named, because the _real_ work is done in `wal_setup_transaction_data()` while `wal_prepare_txn_buffer()` 
will just... prepare the buffer. The first thing that happens is that we compute the size of the buffer. In addition to `wal_txn_t`, we also have an array of `wal_tx_page_t` that will hold
the metadata of the transaction, followed by the actual pages we need to write. 
Notice that we use `mem_alloc_page_aligned()` to allocate the memory. As a reminder, the term "page" in this context means OS memory page, which is 4KB in 
size, not the internal pages we have in Gavran. Using page aligned memory is not really important at this point, but will become crucial later, when we start issuing direct I/O operations. 

We record the transaction number and the number of pages and then call to `wal_setup_transaction_data()` to move the data to the prepared buffer. Note that we pass it the buffer _after_ the
`tx_header_size` and that the `tx_header_size` is page aligned so the buffer that is sent to `wal_setup_transaction_data()` will also be page aligned. Afterward, we record the size used as well
as the `page_aligned_tx_used`. Our job done, we can return to `wal_append()`, which will write the data to disk directly. Let's dig into what is going on when we setup the WAL transaction
data, you can see what is going on in <<wal_setup_transaction_data>>.

[source]
[[wal_setup_transaction_data]]
.`wal.c` - Copying the data from the transaction `modified_pages` to the WAL transaction buffer
```
include::./code/wal.c[tags=wal_setup_transaction_data]
```

In `wal_setup_transaction_data()` we are simply going to iterate over the `modified_pages` in the transaction and copy them to the buffer that was setup for us. We also update the `pages` 
array on the `wal_txn_t` with the offset of the page from the beginning of the transaction, the page number and the number of pages written. 

.Using `pwritev()` instead of `write()`?
****
In `wal_append()` we are writing from many memory locations (all the modified pages) to a single location on disk. That is a feature tailored made
for the `pwritev()` call, which allows us to coalesce multiple buffers into a single write. It would allow us to save the single buffer allocation and
the cost associated with copying the memory to a single buffer.

There are a few reasons why we _want_ to have a single buffer for the transaction, we'll look at them in depth in the next chapter, for now, I want to
focus on the implications of using `pwritev()` in a storage engine like Gavran.

In particular, I wanted to touch on cross platform compatibility with advanced API. For many database operations, `pwritev()` is the right tool for the job. 
On Windows, on the other hand, you have `WriteFileGather` which allows you to pass an array of pages. Slightly more complex, but works fairly well. 

For the most part, you can assume that most operating systems will have similar API at the file system level. They all have to work with the
same constraints, after all. MacOS, however, doesn't seem to have this API, it has `writev()`, though. 

That said, `WriteFileGather` is available from 2003 and `pwritev` exists only from 2009. The `writev` system call, on the other hand, goes 
to about 2001 or so. As a database developer, I love to use API that has been around for a decade or two. It means that most of the bugs
have been shaken out.
****

We wrote the transaction to the WAL file, but what did we actually write? <<wal_tx_structure>> shows the structure of a single transaction in the WAL.

[[wal_tx_structure]]
.The structure of a single transaction inside the WAL
image::wal_tx_structure.png[]

The purpose of the WAL is to make sure that committed transactions aren't lost. In other databases, it is also responsible for rollback uncommitted 
transactions, but Gavran is using a different model and doesn't have to worry about that. We can now reconstruct committed transactions by looking 
at the WAL for the pages that they modified.

.A note on error handling
****
The PAL code probably deserve some additional checks for error handling. For example, we need to take into account that we may get `EINTR` and 
need to retry the operation again. There is a _lot_ that goes into proper writing to files, I'm afraid.
****

Recovering a committed transaction is easy, take the copy of the pages in the WAL and put them in the data file, done. As simple as this is, that
is all we need to get proper recovery in this scenario.

.The format of write ahead log
****
One of the things that make it _hard_ to build a database is that there are so many interconnected details. Consider the impact of the WAL on the
database design, or how the specifics of an MVCC implementation affect the on disk data format. 

Write ahead logs aren't new, they have been around for as long as there have been transactional databases. They are one of the few ways to _achieve_
durability and usually the only way that can meet realistic performance goals. A common algorithm that uses WAL is called 
https://dl.acm.org/doi/10.1145/128765.128770[ARIES] - Algorithms for Recovery and Isolation Exploiting Semantics. It is used by SQL Server, DB2, etc.
It is an involved system, requiring Redo and Undo logs and a fairly complex dance on startup to ensure that you are back in the same place as you were
before.

If you'll notice, we aren't doing anything so complicated. We record the pages in the WAL and can apply them in place to recover committed transactions.
The entire recovery code fits inside a single page and there is no need for an Undo step, because uncommitted changes never hit the WAL. This is one of
the advantages of having an embedded system, we don't have to cater to the latency inherit to most networked database where transactions may span many 
seconds or even minutes and hours.
****

In <<data_loss>> we have a scenario where a read transaction prevents us from writing to the data file, and then we simulate a crash rudely aborting
the database and transaction. Now that we have `wal_append()`, we have written the changes in the committed write transaction to disk. The next step is 
to see how we can recover from such a scenario. Take a look at <<wal_open_and_recover>>, where we explore the `wal_open_and_recover()`.

[source]
[[wal_open_and_recover]]
.`wal.c` - Opening the WAL and calling `wal_recover()` on startup
```
include::./code/wal.c[tags=wal_open_and_recover]
```

The `wal_open_and_recover()` function is called by `db_create()` just before `db_init()`. The `wal_open_and_recover()` isn't really doing much. It setup
the opens the WAL file using `wal_open_single_file()` and calls to `wal_recover()` which does the actual work. Before we dive into that, let's look into 
what does it means, to open a WAL file, you can see the code in <<wal_open_single_file>>.

[source]
[[wal_open_single_file]]
.`wal.c` - Opening a WAL file
```
include::./code/wal.c[tags=wal_open_single_file]
```

<<wal_open_single_file>> setup the WAL file name, which is just the `<db_name>-a.wal` and then opens it, set its minimum size (with the database 
option `wal_size`) and maps the file to memory. The real fun happens in `wal_recover()`, shown in <<wal_recover>>, where we start the actual recovery
process.

[source]
[[wal_recover]]
.`wal.c` -  Recovering committed transactions from the WAL
```
include::./code/wal.c[tags=wal_recover]
```

In `wal_recover`, we scan through the WAL file using the `wal_next_valid_transaction()` function, which we'll see shortly. Then recover the transaction
using `wal_recover_tx()`. Once we are done running through the valid transactions, we can complete the recovery and validate the recovered pages. For
the purpose of validating the recovered pages, we use the `recovered_pages` hash table we pass to `wal_recover_tx()`. 

The idea is that in `wal_recover_tx()`
we'll run through all the transactions in the WAL and use the `recovered_pages` as a hash _set_ for all the page numbers that were modified in all the transactions.
In this case, we aren't interested in the pages' values, only the page numbers. We use the `recovered_pages` to validate that the pages we recovered are valid
and that there are no issues with them. Let's look how we get the next valid transaction in <<wal_next_valid_transaction>>.

[source]
[[wal_next_valid_transaction]]
.`wal.c` - Getting the next transaction from the WAL, while ensuring that it is valid
```
include::./code/wal.c[tags=wal_next_valid_transaction]
```

The code in <<wal_next_valid_transaction>> is a bit complex at this point, because we have `wal_next_valid_transaction()` and `wal_validate_transaction()`. 
It would actually be easier to merge them into a single function, but we'll soon add more complex behavior to the WAL and there would be meaningful separation
between those two functions. 

The stop condition for `wal_next_valid_transaction()` is interesting. Either we run through the entire WAL or we found a transaction whose id is lower than the current
transaction id. That would indicate to us that we are looking at old transactions. This can happen if we reset the file because we got to a checkpoint, which we'll discuss shortly.
We are checking that the `tx_id` is not empty, but that is the sum total of the validation we do. We could use a marker value to verify that the value is a transaction, but that wouldn't
give us much. If we want proper validation, we need to verify that _entire_ transaction has been saved properly. In order to handle that, we'll need to compute a hash and validate it.

.Is this a toy implementation?
****
You might have noticed that I'm doing the minimum necessary to get things working here. Surprisingly enough, you can take this kind of implementation quite
far, in real world scenarios. However, we will patch in all the holes in the behavior in the next chapter. There are enough moving wheels in building a proper
storage engine that I would rather introduce something simple that works and then expand on it rather than try to show you the whole thing all at once.

The WAL we are showing here is a trivial implementation. There are quite a few things that I'm leaving unhandled. I wanted to make sure that I show
a fully functional system that you can easily reason about before we go and do anything more complex.

For example, we aren't handling partial writes to the WAL. We assume that writes to the WAL are okay, which is _not_ a safe assumption to make. We'll
deal with that in the next chapter.
****

With just this code, if we run the code in <<data_loss>>, we'll see that after the crash, the data from a committed transaction is properly back in the database.
Can we shout hurray now? Not quite yet, I'm afraid. We need to also consider a few other things:

* When we recover, we'll run through the entire WAL. If a page has been modified multiple times, we'll write to it multiple times.
* There is no mechanism to trim the size of the log, as it currently stands, transactions will grow the log without any bound.
* In other words, the transactions a database has, the more expensive it is to restart it. That is... not a good position to be in. 

The answer for that is to let the WAL know when we are saving the pages to the database. This is typically called a checkpoint.

=== Checkpoints and WAL trimming

A checkpoint notify the WAL that the data file has properly persisted (and made durable) the data in the WAL up to a certain point. That allows the WAL to trim itself 
from transactions that are known to be in the data file already. That is the role of the `wal_checkpoint` function, which is called from `txn_write_state_to_disk()`. When called, it 
will check to see if the transaction that was saved to the data file is the last transaction in effect, which will allow us to reset the WAL location. 

Before we get to the impact of checkpoints on the WAL, let's see what this means for the data file itself. <<txn_write_state_to_disk>> shows the changes to `txn_write_state_to_disk()`
to support checkpoints. 

[source]
[[txn_write_state_to_disk]]
.`txn.c` - Flushing the file data when the WAL says that it is a good idea
```
include::./code/txn.c[tags=txn_write_state_to_disk]
```
<1> New code here, if the WAL says that it is time to checkpoint, we'll call `fsync()` on the data file and then tell the WAL to checkpoint.

It is important to understand what is going on in `txn_write_state_to_disk()`, we write the data to the WAL on `txn_commit()`, and the `txn_write_state_to_disk()` will write the data to the data
when there are no active transactions looking at it. However, writing to the data file is done in a non durable mode. We write to the data file using buffered I/O, which means that we 
are _not_ guaranteed to have the data persisted on disk. It is only after we call to `fsync()` that we can rest assured that the data has been properly stored in persistent medium.

Once that happens, we can let the WAL know about it. At this point, any WAL data for the transaction that was just flushed to disk (and any earlier transactions) are no longer needed. We kept
them around to ensure that we can recover in the case of an error, but now we have verified that they are properly stored. That means that it is time to clean up the WAL. 

The `fsync()` call is *expensive*, if you'll recall. We don't want to run it too often. That means that the WAL will need to tell us when it is a good idea to call to `fsync()`. You can see
how that is done in <<wal_will_checkpoint>>.

[source]
[[wal_will_checkpoint]]
.`wal.c` - Deciding when to call `fsync()` on the data file needs to consider safety concern vs. performance
```
include::./code/wal.c[tags=wal_will_checkpoint]
```

If we call `fsync()` too often, we put a lot of load on the I/O and can hurt our performance. If we don't call it often enough, on restart, we'll have to go over all the transactions that we
aren't _sure_ were properly persisted to durable media. That may increase the startup time of the database. In `wal_will_checkpoint()` we use a simple rule, we will allow a checkpoint once over
half of the WAL is full. The size of the WAL is controlled by the user and they can use that to set the interval for `fsync()` calls. 

That isn't the only limit to checkpoints, however. A checkpoint is only meaningful if the transaction in `txn_write_state_to_disk()` is the latest transaction in the WAL. Let's dig into this issue.
The `txn_write_state_to_disk()` is going to run on a transaction that is guaranteed to have no one looking at it. In practice, that means that we can run a checkpoint whenever the WAL is over half
full and there are no active transactions at all. Why is that required?

If the transaction that was just written to disk is the same as the most recent one it means that the entire WAL file has been made redundant. Let's see how we actually handle the checkpoint process
in <<wal_checkpoint>>.

[source]
[[wal_checkpoint]]
.`wal.c` - Discarding the WAL file that is no longer required
```
include::./code/wal.c[tags=wal_checkpoint]
```

The WAL checkpoint process is actually really simple. We write a page of zeros to the start of the file, and set the `last_write_pos` to the start of the file, that is all. 
What does _that_ do? Let's consider the implications:

* If we restart now, the first page will be zeroed and no recovery will be performed. The zero page will not pass `wal_validate_transaction()` and will stop the iteration on the WAL.
* If we write another transaction, it will be written to the start of the WAL file. We'll then read that transaction and then try to read the next.
  The next transaction would either have lower id than the previous one (causing us to stop) or we'll read into the middle of an old transaction, which will also 
  cause us to stop. In the later case (reading into the middle of an old transaction) we may get unlucky and think that this is a valid transaction, we'll fix that in the next chapter.

Either way, by overwriting the first page we have trimmed the recovery to only transactions that happened _after_ the checkpoint, exactly what we wanted 
to happen. Admittedly, in this case there are _no_ transactions after the checkpoint at the time we actually checkpoint, but we need to take baby steps to get to end without getting
tangled with too many details at once.

=== Durability concerns

We now have a WAL that help ensure that our commit isn't lost, surely that is all we need, right? Almost. We still need to make sure that the following
things will happen:

* When we write to the WAL, the wal file is _durable_.
* When we checkpoint, the _data file_ will be durable. 

Let's try to address these in order. We currently write to the WAL using buffered I/O. If the system suffer a power loss or a crash, our changes will _not_
be preserved. We need to ensure that the WAL writes will actually hit the disk. That turns out to be a rather hard thing to do. Disk I/O is _slow_ and there
are multiple caches in the middle that are design to shield us from the reality of having to wait for the hardware. 

One way to ensure that the data will hit the disk after the write is to use `fsync()` after each call to `write()`. That will ensure that data actually hits
the disk, right? This is also _atrociously_ slow. The https://www.percona.com/blog/2018/02/08/fsync-performance-storage-devices/[blog post from Percona] 
have some interesting numbers on the topic. A dozen years or so ago there was a huge furor 
http://shaver.off.net/diary/2008/05/25/fsyncers-and-curveballs/[around `fsync()` after FireFox started to use this excessively].

The story is quite interesting and touch on of the key issues with `fsync()`, it is a Big Hammer approach. It tells the file system and the disk to flush
the writes to the physical medium. However, even though `fsync()` is called with a file descriptor, what actually happens is that the _entire_ drive cache
is flushed. In some cases, that may mean _seconds_ of wait for the operation to complete. That is... not ideal.

There are some way around it. the `fdatasync()` is much better, it flushes the _data_ of the file, not the metadata. Remember how careful we were about 
the file size? That is part of the reason why it is important. It saves us significant amount of time on an ongoing basis. 
The `fdatasync()` call is still expensive, but pretty much _all_ the calls that touch the real hardware are going to be slow. In the Percona post they 
tested _NVMe_ drives have meaningful latency for `fsync()` and `fdatasync()`. 

The key issue is that we need to flush the drive buffers. First we write to the file, which does buffered I/O and then we flush it using `fdatasync()`. 
These buffers may have already been flushed down to the drive / disk level, where the association to the actual file is lost. We now need to just issue
a "flush it all", which is expensive. 

We can do better by bypassing the buffered mode entirely. If we could tell the operating system that we want to send the data directly to disk, that would
save the need to flush the entire cache. It turns out that there is a way to do so, when we open the file, we can mark is using `O_DIRECT | O_DSYNC`. That 
would do just that, as far as the operating system is concerned. On Windows, we'll need to use `FILE_FLAG_NO_BUFFERING | FILE_FLAG_WRITE_THROUGH` in order
to get the same behavior.

Enabling those flags impose certain alignment requirements on the writes you can make. All you operations must be 4KB aligned. This is because using these
flags, you are using DMA (Direct Memory Transfer), so the write needs to be page aligned. The `O_DSYNC` or `FILE_FLAG_WRITE_THROUGH` will tell the system
to send a FUA (Force Unit Access) flag to the disk. This tells the disk that it needs to put the data on persistent medium. That applies to a particular
write and saves us from flushing caches. Note that you need _both_ flags for the desired behavior, either one of them on their own will not get you the 
right result. There are scenarios where using one of those flags independently is good, but not for our needs.

The issue of durable writes is _complex_ and there are a lot of material about it. 
The https://bobsql.com/sql-server-on-linux-forced-unit-access-fua-internals/[SQL Server On Linux: Forced Unit Access (Fua) Internals] post talks about this
extensively and can give you some idea about the challenges involved in making sure that data is properly durable. 
The LWN article about https://lwn.net/Articles/457667/[Ensuring data reaches disk] is also a good read that is worth pursuing.

.The design of the WAL and the hidden benefits
****
A _lot_ of thought has gone into the design of the WAL. I know that it doesn't _looks_ like it, right now the whole WAL code is under a few hundreds lines of code, but
it was setup in such a way that we have major benefits:

* The WAL pre-allocates the file size. Which means that we will have reduce file fragmentation and likely have more sequential disk space allocated to us.
* Not needing to modify the file size means that we can use data sync, not full sync, reducing the I/O costs substantively.
* All writes are done on page boundary and are sequential in nature. That is the _best_ way to get speed from the hardware. 
* There are no reads or any random I/O during normal operations and a simple sequential scan when recovery is running.
* We are writing to the data file using buffered I/O as soon as we can, but calling `fsync()` only when the last transaction in the WAL has been committed. 
  That gives the file system _time_. Time to do background writes to disk so when we call `fsync()`, we won't have a huge pile of data that needs to be sent
  all at once. It isn't a _massive_ performance boost, but every little bit helps.
****

If you'll look at <<wal_open_single_file>> you can see that I actually had a bit of foreshadowing. In <<wal_open_single_file>> we pass `pal_file_creation_flags_durable`
to `pal_create_file()`. When we send this flag to `pal_create_file()`, we will call `open()` on the file with `O_DIRECT | O_DSYNC` (I mentioned that we'll talk about these flags
later when we looked into this function in chapter 3, now is that later). That means that writes to the WAL
_must_ be aligned on 4KB boundary and the buffer we pass to `write()` should also be 4KB aligned in memory. The idea is that the `write()` will basically hand the buffer
we gave it directly to the disk drive and flag the write with the FUA flag (instructing the drive to bypass any cache and write to persistent medium). 

[CAUTION]
.Do you trust the hardware?
====
The fact that we sent the FUA flag to the disk doesn't mean that it has to respect that. Some drives will ignore the FUA request because they are designed to run with 
independent power source and will have time to flush any cache they have in the case of a failure. But some cheap consumer drives simply don't implement this flag at all. 

In such a case, there is usually no indication that the drive didn't respect our wishes and no way to ensure durability of the data. That isn't unique to Gavran, such
drives would render any database suspect, because it violate the invariants that we rely on. Most of the drives in the market today will respect the FUA flag and _any_
enterprise grade disk will handle that properly.
====

What is _actually_ going on in this scenario is a bit more complex, but that is the key for ensuring that a write like that will be durable. Note that because we are sending
the write marked from the start as one that should be made durable, there is no flushing required of the the cache, we simply bypass these as we go through the layers of the
operating system, file system, disk driver and the disk controller itself. 


.Handling partial writes
****
We have durable writes to the WAL file, which ensure that our transaction data will be safe. That still isn't enough. What happens if we crash midway
through writing a transaction? Since the call didn't complete, we have no guarantees what will actually be on the device. We need to have insurance from
partial writes. 

This time, there are no flags that we can set, we have to use our initiative and write some code to ensure everything will work. The way to _handle_ that
is to run a hash on the data and then validate the result during recovery. Good options for hashing are either `xxHash64` or `BLAKE2b` hash. The major
difference between the two is that the first is non cryptographic hash while the `BLAKE2b` is a cryptographic hash. 
The `xxHash64` produce an 8 bytes (64 bits) value while the `Blake2B` produce a 32 bytes (256 bits) value. 

A cryptographic hash has a number of https://en.wikipedia.org/wiki/Cryptographic_hash_function[required properties], which boil down to what you think
is your threat vector. If you are concerned about the missing data or disk corruption, you can probably suffice with CRC or something similar, although
CRC is actually _slower_ than `xxHash64`. The `xxHash64` in a fast, in the order of 10GB / sec and gives good indication that the data was modified. 
The `BLAKE2b` function is also fast, but only in the company of other cryptographic hashes. It gets to 0.95 GB / sec, significantly slower that `xxHash64`. 
The advantage is that you can safely assume that no one is going to be _able_ to manufacture collisions for your data. 

Given that CRC was used for this purpose for a long time, and that `BLAKE2b` is actually _faster_ than CRC, I'm just going to use `BLAKE2b`.
I'll defer implementing hashing to the next chapter, however.
****

A good tool to use is `strace`, which can tell you what system call are generated by a process. That is very useful when trying to understand what
your code is _actually_ doing, rather than what you _think_ it is doing. 

=== Unit Tests

This chapter has just a few unit tests, showing what happens when we close the database with an outstanding read transaction. That transaction 
prevent us from writing to the data file, so we need to run recovery to get the data from the committed transactions.
We also have a test to check that the checkpoint process works and we don't do any recovery after a checkpoint.

Testing the storage layer in this manner test to be hard, we'll be doing a lot more with that using ALICE toward the end of the book, to make
sure that we are _properly_ using the system and not leaving ourselves open to some rare alignment of the stars. 

You can see the tests in <<test_08>>.

[source]
[[test_08]]
.`test.c` - Unit testing WAL behavior
----
include::./code/test.c[tags=tests08]
----