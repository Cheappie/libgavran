== At the pages layer, building transactions

We have gotten to the point where we can read and write data to a file. It will take a while to understand why we didn't simply call `write` or `read` directly and call
it a day. We'll discuss that in detail when we implement transactions. Right now, I want to focus on how we are going to lay out the data on the file. 

Files are typically thought of as sequence of bytes. Consider the text file shown below. It shows a simple text file consisting of several lines. 

----
One
Three
Four
----

If I want to add `Two` to the file in the right location, what do I have to do? There is no easy way to add data in the middle of the file. I have to write out
what I want and then continue writing to the end of the file (extending it along the way). There actually _are_ ways to extend a file in such a manner, in turns
out. It isn't very useful for most scenarios, but I want to discuss that a bit so you'll understand how that works.

You can call `fallocate` with `FALLOC_FL_INSERT_RANGE` to insert a range of blocks in the middle of a file. That works because while the file system present
you with the illusion of a sequence of bytes, the reality is very different. Take a look at Figure 3, which shows the physical outline of the text file above.

.Physical layout of a file on disk
image::{img-src}/img3-1.png[]

Any file that is smaller than 512 bytes will fit on a single block one block (sometimes called sector). A block is 512B - 4KB in size, in most cases and you can
assume that it will be 4KB on pretty much any modern disk. And by modern, I'm talking anything anything made in the past _decade_. 

Pretend that the file is actually large enough to reside on multiple blogs. You can use the `filefrag` command to view the blocks on the file. This is _exactly_ 
what happens when we talk about fragmentation of files. The physical layout of the file is made up of blocks, which reside on a block device (the disk drive).
It is the job of the file system to map those blocks into a file and present us with the file stream abstraction. 

With `FALLOC_FL_INSERT_RANGE`, we can tell the file system that we want to add new blocks to the file, somewhere in the middle. That is a cheap operation, because
we aren't actually moving anything. We simply change the list of blocks that are assigned to the file and write new data to the block. You can see how that looks
on Figure 4.

.A file's physical layout after adding a block in the middle
image::{img-src}/img3-2.png[]

So we can add data cheaply in the middle of a file, although it requires somewhat unusual API calls. The problem is that we can do that only on block boundary. On 
most modern systems, that is 4KB in size. We can also only insert data in increments of 4KB, so this approach isn't generally usable.

At the same time, we are going to think about how we are going to manage the data we put into storage. We need to consider how we read and write data. As it 
turns out, there are quite a few common methods to handle this scenario:

* Append only - we always write at the end of the file, and the file format ensures that new values are found before old values. This has the benefit of being 
  simple to start with, since there is very little to manage, but you'll very quickly end up with most of your space being used by values that has been 
  overwritten. Getting rid of these old values require you to _compact_ the file, which may take twice as much space as the original file take and introduce
  a _lot_ of load on the I/O system.
* Fixed size records - in this model, we define the size of a record upfront (64 bytes, for example) and then we can treat the file as an array of those records.
  This is how many applications stored their data and it is a very simple method that is surprisingly powerful. It has the downside, of course, that you are 
  forced to pick a set size and use it for the life time of the data.
* Page model - the file is divided into pages (4KB - 4MB) in size that are treated as independent buffers. We are always working on a the page level, which is
  a buffer that is read and written to in an atomic fashion. In other words, we replicate exactly how the file system think of the world. This is similar to the 
  fixed size records option, but instead of storing fixed size records, we have fixed size pages and we are free to manage their internal structure as we see
  fit. The page size is also large enough that we don't usually need to 

=== Getting to grips with paging

For our storage engine, we are going to use the paging model. That allows the most flexibility and is the most common choice for storage engines. I'm not going to
go too deeply into the design choices, you might want to refer the https://www.databass.dev/[Database Internals] book for details. In particular, the terms you
are looking for are Log Structure Merge vs Page Structure. 

[TIP]
.Pages are a way to work with the data inside the storage engine
====
Our storage engine is going to divide the file into pages that are 8KB in side. Instead of thinking about the whole of a file as a stream of bytes, we are going
to cut it into pages and work on each one of them independently. This is a very common decision for databases since there are many benefits for this approach.
Modifying a single page solve the need to insert data in the middle of a file, we can overwrite the page as a whole. 

Pages are to a database as bytes are to a file. The basic building blocks, but they aren't sufficient. In order to actually make _sense_ of the system, we have
to layer additional features and data structures. I'm building Govran from the ground up and explaining each section independently. It all come together and
then we'll have a beautiful picture and gorgeous architecture.
====

The next question to ask is what will be the page size we'll select. The page size is of great important for the storage engine. That is the atomic unit on which
all operations are made. The page size must be a multiple of the file system block size. In practice, that means that it should be a multiple of 4KB these days. 
When building Voron (RavenDB's storage engine) we have run a whole bunch of benchmarks and the sweet spot for our needs was a page size that was 8KB. I'm going
to use that value again and maybe we'll play with the size when we get to writing benchmark code (which is still very far away).

The API that we wrote so far isn't really suitable for working with pages. That is intentional, we are now building another layer in the storage engine. On top
of our PAL code (which deals with files) we now have a paging layer. I'm going to need to jump ahead a little bit and declare a few things that will not make 
_any_ sense now. Instead of building pages and then the database infrastructure, I'm going to define the code as it should be in the end. You can see the 
relevant definitions in Listing 3.1. 

[source]
.Listing 3.1 - db.h & impl.h - The declarations of db_t and txn_t
----
// from db.h - public API
include::./code/db.h[tags=paging_api]

// from impl.h - private API
include::./code/impl.h[tags=paging_api]
----

In Listing 3.1, we also define the `PAGE_SIZE` constant and the `page_t` struct. There isn't much there at this point, we simply have the page number and
the mapped address for the page as well as the total size. 

We can use that to get a particular page or to write a set of pages. Let's explore how we can use this API to implement the same read & write operation as we did in 
Listing 2.11. 
One caveat we have to take into account is that we _can't_ modify the result of the `pages_get` directly. This is because the memory is mapped as readonly. Attempts
to write to it will result in a segmentation fault. In order to deal with that, we'll copy the page to our own buffer and use the `pages_write` to update its content.

[TIP]
.Always work _with_ the hardware
====
One additional factor for using pages as the manner in which we manage the data in the file is that this means that all our I/O is now going to be page based.
This in turn means that drives such as SSD or NVMe are going to have easier time, because we are never going to issue a read or a write the crosses a page
boundary. This also tend to allow you to get better lifetime from your hardware, since you are reduce the amount of work it needs to do.
Writing on page boundary has been shown to increase the longevity of the hard drives and can also allow the disk to optimize the data access better.
====

We are working with pages, so we want to use page aligned memory, which usually means that we want to make use of `posix_memalign`. That function, however, has awkward
usage within our API, so we'll wrap it in `txn_allocate_page`, shown on Listing 3.2.

[source]
.Listing 3.2 - mem.c - Allocating memory in chunks of pages
----
include::./code/mem.c[tags=palmem_allocate_pages]
----

The `palmem_allocate_pages` is pretty bare boned, but that is sufficient for now. With that at hand, let's see how we can make use of the API to read and write from the file.

[source]
.Listing 3.2 - main.c - Reading and writing using the paging API
----
include::./code/main.c[tags=create_and_write_file]
----
<1> Changes from Listing 2.11 start here. We setup the `db_state_t` with the previous defined values we created.
<2> We ask to get a (read only) page 0.
<3> We allocate a (mutable) page from memory (unrelated to the file).
<4> We copy the data from page 0 to the mutable page.
<5> We copy a string to mutable page.
<6> We write the modified page back to the file.
<7> Using memory mapped I/O, we read the values we just wrote to the file.

We get the page, copy it to our own buffer, modify our own copy and then write it back. This technique is called Copy On Write and it has some highly desirable
properties. For example, until I call `pages_write`, there has been no change to the file. That means that I can abort an operation midway through, free the 
copy of the memory I used and move on without need to write compensation logic to restore things to the way it was. Listing 3.3 shows the implementation of
`pages_get` and `pages_write`.

[source]
.Listing 3.3 - paging.c - Reading and writing using the paging API
----
include::./code/paging.c[]
----

The code for `pages_get` and `pages_write` right now is mostly about forwarding the calls. I created these functions because they will become much more important
down the line. 

[IMPORTANT]
.Trust the hardware, but verify
====
One of the things that we'll need to implement at some point is checksum of the pages data. If the data was corrupted on disk, we want to learn about this as soon
as possible. We'll likely implement checksum operations in those methods. Data encryption is also very likely to be implemented at this location as well. 
====

I'm likely not going to be able to keep track of everything in a system that need to modify more than a single page.  We need some way to manage pages in a more
seamless manner, to handle the Copy On Write, the writes of all the modified pages back to the file and to free all that memory.

For that matter, what happens if I need to modify the same page twice? Am I going to have multiple copies? What will end up in the file in the end?
All of that leads me to realize that we can't remain at the level of working with a single page, we need some higher scope to work with here. We need to implement
the concept of a database and a transaction.

=== A skeleton database

We aren't really ready for real transactions, because it will take time until the code will be able to deliver the appropriate transactional properties, but 
that is the right term for where we will be.

A transaction is a short lived object that is meant to do a specific operation. This is important because we don't expect a transaction to last very long or be very large.
I'm sure we'll get some of those, but we are going to optimize for small & short transactions. Long or big transactions will work, but they aren't our optimization target.

Look at Listing 3.4, which shows the API we are going to expose for our users. 

[source]
.Listing 3.4 - db.h - Transaction API declaration for our storage engine
----
include::./code/db.h[tags=tx_api]
----

The idea in Listing 3.4 is that we want to do _everything_ within the scope of a transaction. Note that if you want to get or modify a page, you must provide a 
valid transaction to do so. The functions are also named `txn_low_get_page`, the `low` part of the name indicates that they are low level interface. I don't 
intend for our users to _ever_ work at that level. These functions will be moved to an internal header and not exposed as we provide more features to callers. 

We now have a `txn_t` struct that we can use to represent the transaction, but the actual implementation is hidden away in the `txn_state_t`. We'll shortly 
see why that is a _very_ useful approach for what we want to do. We also introduced the concept of a `db_t`, which will hold the `file_handle_t` and any other 
resources that will be required.

[CAUTION]
.Paying attention to the man behind the screen
==== 
C is a great low level language, but I'm really feeling the lack of data types. I want to make sure that everything that goes into the engine 
is covered here, but I'm not sure how useful it will be to have you go through  a hash table implementation, for example.
 
I'm assuming that if you are reading this book, you probably are familiar with this enough so that should be of no interest to you. For now,
I want to keep putting all the code here, but I think that when I'll start using code beyond the standard library, I'm going to just point
you to that code and not put it here.
====

As you can imagine, the transaction API is going to be of enormous importance for our storage engine and as such, so we'll spend some time fleshing out the API.
This is where we start to put together the skeleton of Gavran. Let's take a look at the first stage, creating a database, shown in Listing 3.5.

[source]
.Listing 3.5 - db.c - Creating an database instance
----
include::./code/db.c[]
----

A `db_t` is a simple structure that holds a single opaque pointer. I'm using the same approach in `txn_t` as well and it means that I have both encapsulation as well
as a way to modify the state of the object without needing to change the value that holds it. We'll see how useful that is when we get to building transactions.

The `struct database_state` itself isn't very big at the moment. It contains the database options (currently only the minimum size of the database), the file handle
and the memory map result. As times goes by, we'll see more and more data being managed at the database level, but it is important to start with the proper structure
now, to avoid extra work later.

We check and validate the options, then we use our existing PAL file system API to create a file, set it to the right size and map it. With the use of `ensure` and
`try_defer`, the entire thing reads as if we had no error handling. Note that we use multiple `try_defer` statements to setup resource cleanup in the event of a 
failure. They are guaranteed to run in reverse order of definition.

In the next chapter, we are going to focus on `handle_newly_opened_database` and do a lot of work there, but for now, I want to focus on transactions.

=== Starting to implement transactions

You already saw the external API to create a transaction in Listing 3.4, now we want to actually build it. Before we can do that, I want to go over what transactions _are_.
A transaction in the context of Gavran is a set of changes that are applied to the database. Until they are applied, they are not visible to any other transactions.

Before we dive into the details of how a transaction is implemented in code, I want to talk about what we are trying to _do_. The attachments that we'll build in this
chapter is mostly concerned with managing a list of modified pages. The idea is that you'll always go through the transaction to modify pages, it will manage them for
you and eventually commit them.

The overall structure can be seen in Figure 4. 

.The layout of a transaction with 2 modified pages
images::{img-src}/img3-3.png

Using the Copy On Write mode tha we discussed earlier, whenever we want to modify a page, we'll copy it to the side and make the modifications. Once all the changes 
in the transaction are completed, we'll persist them all at once. Consider the implications of such aa system. How would a rollback look in this case? 
Simply free the copies, no one else had any access to them, so nothing needs to be done. 

As for commit, once all the changes has been written to the file, we are done. We saw in the previous chapter that it takes significant effort to make sure that a
change to the file metadata is persisted. Changes to the file's data are by no means easier. We are going to deal with a single facet of the ACID properties in
this chapter, and atomic commit. 


.A note about multi threading
****
We don't have any. 

Right now, the code isn't going to be able to handle multiple threads.
The atomic property that we'll have is about sequential or interleaved transactions.
Gavran _will_ have all of the ACID properties, including safe usage under multi threaded code, mind you. 
****

Listing 3.6 shows the structure of a transaction as well as its initialization.

[source]
.Listing 3.6 - impl.h & txn.c - Creating a transaction
----
include::./code/impl.h[tags=transaction_state]

include::./code/txn.c[tags=txn_create]
----


A `txn_t` is just a pointer to `txn_state_t`, which holds the details about the transactions as well as the start of a hash table to hold
the modified pages. We use the empty `entries` field to point past the end of the struct to the actual data. I'm doing it in this manner because
it means that I have to do less allocations, which greatly simplify the code.

You'll note that we are passing a `flags` parameter to the function, but not doing much of it. I'm currently ignoring that but we'll use that to
set various behaviors on the transaction, such as whatever it is read only or allowed to write.
In `txn_create`, we allocate enough space for the transaction state as well as 8 hash table entries. We'll see how they are used shortly.

[CAUTION]
.I gotta implement a hash table, argh!
====
I have to admit, given that I write databases for a living, I was highly amused by how reluctant I was to implement a hash table. I have wrote
a few, so it isn't a new task. I'm also doing that while implementing a _storage engine_, which is closely related. 

The good news here is that this hash table only need to support `put` and `get`, not `remove`. That makes the overall complexity much easier.

Given that the readers of this book are likely interested, I'm going to go for an open addressing with linear probing model. And one of the 
key features that I care about is avoiding the usual pointer caching that you'll typically see in hash tables. That is why the actual 
`entries` buffer is placed after the `txn_state_t`. Data locality. 

I'm using the actual values (`page_t`) as the hash table entries, since they already contain their key. That means that I have to deal with no 
extra space and the cost of lookup is likely very small. That might seems strange if you are coming from a language where you'll have more
separation between the data containers and the values. `Dictionary<K,V>` is a prime example of that. You have the container data and the data
held by the container and they can live very far apart. That is not the case here, we always keep them very close by.
====

One of the key responsibilities of the transaction is managing the work of `txn_modify_page`, to handle that properly, we need to store the modified
pages in the transaction and be able to access them quickly.  Listing 3.7 has the details on how we manage that.

[source]
.Listing 3.7 - Modifying a page inside a transaction
----
include::./code/txn.c[tags=txn_modify_page]
----

The `txn_modify_page` function in Listing 3.7 will first check if we already asked to modify the page in this transaction. If so, that copy of 
the page will be returned to us. If not, we'll check that the page number if valid, allocate a copy of the page, copy the data to it and
then register it in the hash table. 

This is called Copy On Write (COW) and it will serve as the basis for implementing one of the core ACID properties, Isolation.

.Memory alignment
====
In Listing 3.5 you can see that I'm using `palmem_allocate_pages` function, that make use of `posix_memaligned` under the covers. I'm asking the system
to give me 8KB buffer with 8KB alignment.

This is a match to how the operating system manage pages in memory (on 4KB alignment). It means that for the rest of my system, I can safely assume that 
the page buffer is properly align for anything that I want to do. This is important for performance and correctness reasons, especially outside of the x64/x86 world.
====

=== Managing the transaction state with a hash table

The real work in `txn_modify_page` is inside the `lookup_entry_in_tx` and `allocate_entry_in_tx`. Let's see how these work, we'll start with
the `lookup_entry_in_tx` in Listing 3.8, where we are searching through the hash table for a match.

[source]
.Listing 3.8 - txn.c -Looking up a page in the transaction's hash table
----
include::./code/txn.c[tags=lookup_entry_in_tx]
----

I'm going to skip explaining https://en.wikipedia.org/wiki/Hash_table[how hash tables work], I assume that you either have a grasp on that
or is able to learn that from other sources. I'll mention that I'm using a hash table implementation strategy called 
https://en.wikipedia.org/wiki/Linear_probing[linear probing]. This refers to how the hash table handles collisions. Instead of creating a 
linked list of values on hash collision, we'll move to the next location in the array. That means that we are going to have good locality
of reference when doing the search, the entire hash table is typically a single allocation and the `get` and `put` implementation are
straightforward.

The downside of linear probing and open addressing in general is that the deletes tend to be far more complex to implement than an 
implementation that uses chaining. The good news is that we don't need to worry about that here, so the complexity is greatly reduced.

There are a few other things to consider for this case:

* The key for the hash is the page number, these are `uint64_t` values and it is very likely that you'll get consecutive values.
  As such, we don't try to be fancy with the hash function, we simply modulus the page number with the number of buckets in the 
  hash table and start the search there.
* The rule on collision goes like this. Whenever a put is attempted that collided, we'll move to the next available spot that 
  is free. In other words, the worst case scenario for this sort of hash table is having to scan through all the buckets. 
* This is where the load factor comes into play. Given that the page numbers are likely to cluster around specific values, the
  empty buckets that remain act as backstop to avoid iterating through all the buckets. 
* A `page_hash_entry_t` is 16 bytes in size of x64, which means that we can fit 4 of them on a cache line. A sequential reading
  through the buckets is likely to generate highly predictable access pattern which can be optimized by the compiler and CPU
  significantly. 

Now that we know how to get an entry from the hash table, let's look at the other side, how can we put one in? This is the more complex
operation because we may need to resize the hash table if the load factor grows big enough. The details are in Listing 3.9.

[source]
.Listing 3.9 - txn.c - Adding an entry to the hash table, potentially resizing it
----
include::./code/txn.c[tags=allocate_entry_in_tx]
----

The code in Listing 3.9 starts out in the same manner as `lookup_entry_in_tx` does, by doing a scan on the buckets from the starting position based on the
modulus of the page number with the number of buckets. We are searching for one of two cases:

* A bucket that has the same page number, which we error on, because after the page has been allocated, there should never be any overwrites.
* An empty bucket, which we can reuse. The way I implemented this hash table, we will write to the first empty bucket. That means that if we find any
  empty bucket during a scan, we know that the page number we search can't be on a later bucket.

Once we found an empty slot of put the value in, we check the load factor on the table. If it is lower than 75%, we can assign the bucket to the page
and return successfully. But if we are over the 75% mark, we need to resize the hash table. This is done in `expand_hash_table`, which we will explore
shortly. 

There are three possible return values for `expand_hash_table`:

* Success, in which case we increased the size of the hash table and will recurse to add the page in the right location.
* Failure, the function has run into unrecoverable problem. 
* Unable to resize because of memory pressure. In this case, we have two separate behaviors. If there isn't enough memory, but there is still room in 
  the hash table we'll allow the load factor to grow as needed. We'll only raise an error when the load factor reach 100% and we cannot complete the 
  allocation at all.

The idea behind this behavior is to be robust to temporary conditions. We don't *have* to resize the hash table if there is memory pressure right now.
We can increase the load factor and gain more time. Maybe the transaction will complete and we can throw away the entire table, for example.

.Memory management in Linux
****
By default, Linux will never fail a `malloc`. In other words, when calling `malloc`, you'll never get `NULL` back. If you care to know more about 
this behavior, search for https://lwn.net/Articles/317814/[over commit and the OOM killer]. A good paper on why over commit is required is 
https://www.microsoft.com/en-us/research/publication/a-fork-in-the-road/["A fork() in the road"]. 

In essence, too much software simply assume that all memory allocations will succeed. Even the Linux kernel has this an issue with
https://lwn.net/Articles/627419/[this kind of expectation]. 

The problem with not failing `malloc` is that Linux cannot just magic some more RAM, and even with swap, you'll eventually run out. The issue
is how it _handles_ this scenario. And the way Linux work is that when you use too much memory, Linux will select a victim process and _kill_ it
to reutilize the memory it uses. The situation is worse because the triggering event that would cause the OOM killer to run and select a victim
to kill can be a totally unrelated process.

That has serious implications for people building robust software. I can't catch a `SIGKILL` and ignore it, after all. But I could change
my system behavior to handle memory allocation failure. There are configuration flag that you can use to avoid this behavior, but they 
have their own side effects and they are global. In other words, they impact the whole system. If you are running a single process, or
as we are doing now, a library, that isn't going to be useful for you.

The code I'm writing here assume that it is possible to get memory allocation failures and handle them appropriately. We are currently
allocating directly from the system, but that is likely to change as the implementation proceeds. We'll start managing the memory much
more closely and will be able to react properly to low memory events.
****

The last piece of the puzzle for the hash table is the `expand_hash_table` function, which is covered in Listing 3.10. Inside this function you
can find the reason we use the `state` pointer inside `txn_t`. 

[source]
.Listing 3.10 - txn.c - Expanding the hash table
----
include::./code/txn.c[tags=expand_hash_table]
----

The code in Listing 3.10 allocates a new bucket buffer, double the size of the old one (plus the size of the `txn_state_t`). If we fail this allocation, we let the 
caller know about this and they can decide how to handle it. You saw the two behaviors we have for this error mode in Listing 3.9.

We then copy the `txn_state_t` and then copy all the existing values from the existing table to the new one, placing them in their new location. 
I'm handling an impossible scenario as well (`located` being `false`), just to be on the safe side. The key here is that there really isn't much that is
interesting here. This function does the work, and that is it.

There is a catch here. We allocate a _new_ buffer for the hash table and the transaction state. But the caller points to the old one. That is why we use
the `txn_t` that holds a `state` field. The caller can keep using the same value, but we replace the internal state of the system. There are other
important properties that we result from this behavior:

* The memory is always allocated as a single buffer, aiding locality.
* Doubling the size of the hash table means that we'll very quickly reach the appropriate size.
* When we'll start managing memory directly, instead of using `malloc`, we'll be able to reuse this buffer across transaction calls. Meaning that allocations
  will only need to happen on the _first_ transaction.

=== Getting a page from the transaction

We focused on the process of modifying a page, what about getting one? If we don't need to mutate a page, there is no need for all this copying, right?
Checking Listing 3.12, where we show how to get a page for read only operations.

[source]
.Listing 3.12 - txn.c - Getting a page from the transaction for read only purposes
----
include::./code/txn.c[txn_get_page]
----

=== Committing and closing the transaction

Now that we have a way to keep track of the modified pages in the transaction, the next stage is to figure out how to commit those changes to disk. It turns out 
that most of the work was already done for us. Listing 3.11 shows how to "commit" a transaction.
I'm using commit in quotes here because this is not respecting any of the transaction properties, but we are laying down the foundation for actual transactions.

.Listing 3.11 - txn.c - Committing the modified pages to disk
----
include::./code/txn.c[tags=txn_commit]
----

There really isn't much to do in Listing 3.11, to be honest. We scan through the table of modified pages and write them to the file using the `palfs_write_file` function
we looked at in the previous chapter. If there is an error, we return, and that is about it. Note that we don't have any attempt to for durability, 
atomicity, etc. 

As we run through the modified pages, we are freeing them. If there is an error, nothing bad will happen, they will get cleaned up properly in `txn_close`. 
After the transaction is committed, we can close it. Alternatively, we can close the transaction without committing it to as a rollback. Listing 3.12 shows the
relevant code.

.Listing 3.12 - txn.c - Closing the transaction and freeing resources
----
include::./code/txn.c[tags=txn_close]
----

Closing the transaction in Listing 3.12 means that we iterate over the modified pages and free them, then we are done. 

A more sophisticated system will allow us to reuse the allocated memory from one transaction to the next, but I'm reserving those kind of behavior for the
future. Get it working, get it working _right_ and only then get it working fast.

=== Using the transaction API

The last thing that we'll do in this chapter is put everything together. Listing 3.12 shows how we can now write Listing 3.3 using the new API.

[source]
.Listing 3.12 - main.c - Reading and writing using the paging API
----
include::./code/main.c[tags=create_and_use_database]
----
<1> Function to read from the first page in a transaction
<2> Start a transaction and modify a page
<3> Read before committing the transaction
<4> Read after transaction commit

In Listing 3.12, you'll note that we are using _two_ transactions. One to do the writes and one for the reads. We actually got a _very_ limited form
of isolation. Until the `write_tx` is committed, the transaction in `print_msg` will not see any of its changes. I wrote it in this manner to make sure show
how we are starting to get real features. 
The reason the code in Listing 3.12 is able to create isolation between transactions is that the `write_tx` doesn't modify the file directly, it modified a 
_copy_ of the page and the act of committing the transaction make it apparent to the next transaction.

=== Unit tests

include::../pyapi/tests03.py[]