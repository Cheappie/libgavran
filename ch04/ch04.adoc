== THe paging layer and our first transactions

We have gotten to the point where we can read and write data to a file. It will take a while to understand why we didn't simply call `write` or `read` directly and call
it a day. We'll discuss that in detail when we implement transactions. Right now, I want to focus on how we are going to lay out the data on the file. 

Files are typically thought of as sequence of bytes. Consider the text file shown below. It shows a simple text file consisting of several lines. 

----
One
Three
Four
----

If I want to add `Two` to the file in the right location, what do I have to do? There is no easy way to add data in the middle of the file. I have to write out
what I want and then continue writing to the end of the file (extending it along the way). There actually _are_ ways to extend a file in such a manner, in turns
out. It isn't very useful for most scenarios, but I want to discuss that a bit so you'll understand how that works.

You can call `fallocate` with `FALLOC_FL_INSERT_RANGE` to insert a range of blocks in the middle of a file. That works because while the file system presents
you with the illusion of a sequence of bytes, the reality is very different. Take a look at <<file-layout>>, which shows the physical outline of the text file above.

[[file-layout]]
.Physical layout of a file on disk
image::file-layout.png[]

Any file that is smaller than 512 bytes will fit on a single block one block (sometimes called sector). A block is 512B - 4KB in size, in most cases and you can
assume that it will be 4KB on pretty much any modern disk. And by modern, I'm talking anything anything made in the past _decade_. 

Pretend that the file is actually large enough to reside on multiple blogs. You can use the `filefrag` command to view the blocks on the file. This is _exactly_ 
what happens when we talk about fragmentation of files. The physical layout of the file is made up of blocks, which reside on a block device (the disk drive).
It is the job of the file system to map those blocks into a file and present us with the file stream abstraction. 

With `FALLOC_FL_INSERT_RANGE`, we can tell the file system that we want to add new blocks to the file, somewhere in the middle. That is a cheap operation, because
we aren't actually moving anything. We simply change the list of blocks that are assigned to the file and write new data to the block. You can see how that looks
on <<file-layout-modified>>.

[[file-layout-modified]]
.A file's physical layout after adding a block in the middle
image::file-layout-modified.png[]

So we can add data cheaply in the middle of a file, although it requires somewhat unusual API calls. The problem is that we can do that only on block boundary. On 
most modern systems, that is 4KB in size. We can also only insert data in increments of 4KB, so this approach isn't generally usable.
At the same time, we are going to think about how we are going to manage the data we put into storage. We need to consider how we read and write data. As it 
turns out, there are quite a few common methods to handle this scenario:

* Append only - we always write at the end of the file, and the file format ensures that new values are found before old values. This has the benefit of being 
  simple to start with, since there is very little to manage, but you'll very quickly end up with most of your space being used by values that has been 
  overwritten. Getting rid of these old values require you to _compact_ the file, which may take twice as much space as the original file take and introduce
  a _lot_ of load on the I/O system.
* Fixed size records - in this model, we define the size of a record upfront (64 bytes, for example) and then we can treat the file as an array of those records.
  This is how many applications stored their data and it is a very simple method that is surprisingly powerful. It has the downside, of course, that you are 
  forced to pick a set size and use it for the life time of the data. Free space allocation is also something that you need to manage here, as you need some
  way to handle deleted records.
* Page model - the file is divided into pages (typically in the range of 4KB to 4MB) in size that are treated as independent buffers. We are always working on a 
  the page level, which is a buffer that is read and written to in an atomic fashion. In other words, we replicate exactly how the file system think of the world. 
  This is similar to the fixed size records option, but instead of storing fixed size records, we have fixed size pages and we are free to manage their internal 
  structure as we see fit. The page size is also large enough that we can pack multiple records into a single page.


[TIP]
.Always work _with_ the hardware
====
One additional factor for using pages as the manner in which we manage the data in the file is that this means that all our I/O is now going to be page based.
This in turn means that drives such as SSD or NVMe are going to have easier time, because we are never going to issue a read or a write the crosses a page
boundary. This also tend to allow you to get better lifetime from your hardware, since you are reduce the amount of work it needs to do.
Writing on page boundary has been shown to increase the longevity of the hard drives and can also allow the disk to optimize the data access better.
====

=== Getting to grips with paging

For our storage engine, we are going to use the paging model. That allows the most flexibility and is the most common choice for storage engines. I'm not going to
go too deeply into the other design choices, you might want to refer the https://www.databass.dev/[Database Internals] book for details. In particular, the terms you
are looking for are Log Structure Merge vs Page Structure. 

[TIP]
.Pages are a way to work with the data inside the storage engine
====
Our storage engine is going to divide the file into pages that are 8KB in side. Instead of thinking about the whole of a file as a stream of bytes, we are going
to cut it into pages and work on each one of them independently. This is a very common decision for databases since there are many benefits for this approach.
Modifying a single page solve the need to insert data in the middle of a file, since we can overwrite the page as a whole. 

Pages are to a database as bytes are to a file. The basic building blocks, but they aren't sufficient. In order to actually make _sense_ of the system, we have
to layer additional features and data structures. I'm building Gavran from the ground up and explaining each section independently. It all come together and
then we'll have a beautiful picture and gorgeous architecture.
====

The next question to ask is what will be the page size we'll select. The page size is of great important for the storage engine. That is the atomic unit on which
all operations are made. The page size must be a multiple of the file system block size. In practice, that means that it should be a multiple of 4KB these days. 
When building Voron (RavenDB's storage engine) we have run a whole bunch of benchmarks and the sweet spot for our needs was a page size that was 8KB. I'm going
to use that value again and maybe we'll play with the size when we get to writing benchmark code (which is still very far away).

The API that we wrote so far isn't really suitable for working with pages. That is intentional, we are now building another layer in the storage engine. On top
of our PAL code (which deals with files) we now have a paging layer. I'm going to need to jump ahead a little bit and declare a few things that will not make 
_any_ sense now. Instead of building pages and then the database infrastructure, I'm going to define the code as it should be in the end. You can see the 
relevant definitions in <<paging-api>>. 

[source]
[[paging-api]]
.`gavran/db.h` - The low level paging API
----
include::../include/gavran/db.h[tags=paging_api]
----

In <<paging-api>>, we define the `PAGE_SIZE` constant and the `page_t` struct as well as functions that we'll use to read and write those pages.
The fields that we care about at this point in `page_t` are `address`, `size` and `page_num`, we'll leave the rest for later. The `address` is used to return the 
address of the page in memory, the `size` is the size of the data that is stored on the page. This may actually be more than single page's worth of data, in some
cases. And the `page_num` is the page number, naturally. It is important to note that this is an _in memory_ structure, this is never persisted to disk.

The `pages_get()` and `pages_write()` functions uses types that we haven't seen yet, `txn_t` and `db_state_t`. For now, we'll ignore these, I'll introduce how they work
shortly.

We can use the API in <<paging-api>> to get a particular page or to write a set of pages. Let's explore how we can use this API to implement the same read & write operation as we did in 
the `read_write_io()` function in the previous chapter. 
One caveat we have to take into account is that we _can't_ modify the result of the `pages_get()` directly. This is because the memory is mapped as readonly. Attempts
to write to it will result in a segmentation fault. In order to deal with that, we'll copy the page to our own buffer and use the `pages_write()` to update its content.

We are working with pages, so we want to use system page aligned memory, which usually means that we want to make use of `posix_memalign()`. We already saw how we wrapped this 
function in `mem_alloc_page_aligned()` for our needs. With that in place, we can now write the code to modify the data. You can see how that is done in <<create_and_write_file>>.

We get the page, copy it to our own buffer, modify our own copy and then write it back. This technique is called Copy On Write and it has some highly desirable
properties. For example, until I call `pages_write()`, there has been no change to the file. That means that I can abort an operation midway through, free the 
copy of the memory I used and move on without need to write compensation logic to restore things to the way it was. <<paging-impl>> shows the implementation of
`pages_get()` and `pages_write()`.


[source]
[[create_and_write_file]]
.`test.c` - Reading and writing using the paging API
----
include::./code/test.c[tags=create_and_write_file]
----
<1> Create the file and set its size.
<2> Map the file to memory.
<3> Manual setup (for now) of the data structures we need to make things happen. 
<4> Get the page from the memory mapped file.
<5> Allocate a standalone buffer for the page.
<6> Copy the page from the data file to the new buffer and then modify the _copy_.
<7> Write the copied data back to the data file.
<8> Ensure that we can see the changes in the page after we written to the file.

[CAUTION]
.System page size vs. `PAGE_SIZE`
====
Gavran is going to use `PAGE_SIZE` (8KB) for its pages, but there is also the _system_ page size, which is something related, but different. The system page size is
going to be 4KB on pretty much all platforms. You can see this https://stackoverflow.com/questions/11543748[StackOverflow answer] for the details why this is the winning 
choice. 

When we talk about page aligned memory, we are actually talking about memory that is aligned on 4KB boundary (system page size), because that is what the API and hardware
will demand from us. When we are talking about pages in our data file, then we refer to the 8KB pages that Gavran will use. In <<paging-api>>, you can see it as `PAGE_SIZE`
vs. `PAGE_ALIGNMENT`. 
====

The code in <<create_and_write_file>> deals with creating the appropriate `db_state_t` and `txn_state_t` is pretty nasty, we are manually setting up a lot of data structured 
that I haven't introduced yet. I decided that it is  better to show you the end result, rather than iterate through the same function multiple times. You can see that for now, 
we simply need a way to pass the `span_t` that we mapped and the `handle` to the `pages_get()` and `pages_write()` functions.

[source]
[[paging-impl]]
.`pages.c` - Reading and writing using the paging API
----
include::./code/pages.c[tags=paging-impl]
----

The code for `pages_get()` and `pages_write()` right now is mostly about forwarding the calls. I created these functions because they will become much more important
down the line. 

I'm likely not going to be able to keep track of everything in a system that need to modify more than a single page.  We need some way to manage pages in a more
seamless manner, to handle the Copy On Write, the writes of all the modified pages back to the file and to free all that memory.

[IMPORTANT]
.Trust the hardware, but verify
====
One of the things that we'll need to implement at some point is checksum of the pages data. If the data was corrupted on disk, we want to learn about this as soon
as possible. We'll likely implement checksum operations in those methods. Data encryption is also very likely to be implemented at this location as well. 
====

For that matter, what happens if I need to modify the same page twice? Am I going to have multiple copies? What will end up in the file in the end?
All of that leads me to realize that we can't remain at the level of working with a single page, we need some higher scope to work with here. We need to implement
the concept of a database and a transaction.

=== A skeleton database

We aren't really ready for real transactions, because it will take time until the code will be able to deliver the appropriate transactional properties, but 
that is the right term for where we will be.

A transaction is a short lived object that is meant to do a specific operation. This is important because we don't expect a transaction to last very long or be very large.
I'm sure we'll get some of those, but we are going to optimize for small & short transactions. Long or big transactions will work, but they aren't our optimization target.

[CAUTION]
.Paying attention to the man behind the screen
==== 
C is a great low level language, but I'm really feeling the lack of data types. I want to make sure that everything that goes into the engine 
is covered here, but I'm not sure how useful it will be to have you go through a hash table implementation, for example.
 
I'm assuming that if you are reading this book, you probably are familiar with this enough so that should be of no interest to you. For now,
I want to keep putting all the code here, but I think that when I'll start using code beyond the standard library, I'm going to just point
you to that code and not put it here.
====

Look at <<txn_api>>, which shows the API we are going to expose for our users. The idea in <<txn_api>> is that we want to do _everything_ within the scope of a transaction. 
Note that if you want to get or modify a page, you must provide a 
valid transaction to do so. The functions are also named `txn_raw_get_page()`, the `raw` part of the name indicates that they are raw interface. I don't 
intend for our users to _ever_ work at that level. These are here for the internal implementation, nothing else. Building Gavran in layers like that helps to stabilize the 
architecture of the system. It also help us reduce the internal dependencies. A raw method call can be done with no setup, but higher level calls likely need more details.

We now have a `txn_t` struct that we can use to represent the transaction, but the actual implementation is hidden away in the `txn_state_t`. We'll shortly 
see why that is a _very_ useful approach for what we want to do. We also introduced the concept of a `db_t`, which similarity hold the `db_state_t` and not much else. The
idea is that `txn_t` and `db_t` are values that the user is using, while the `db_state_t` and `txn_state_t` are used by Gavran internally.

[source]
[[txn_api]]
.`gavran/db.h` - Transaction API declaration for our storage engine
----
include::../include/gavran/db.h[tags=tx_structs]

include::../include/gavran/db.h[tags=txn_api]
----

As you can imagine, the transaction API is going to be of enormous importance for our storage engine and as such, so we'll spend some time fleshing out the API.

[IMPORTANT]
.How the code is structured?
====
I thought a lot about how to present the code in this book, because I don't want to dump all the concerns that we have to deal with in building a storage engine on you all at
once. Once option was to show you a piece of code and how it changes over time. That ended up being pretty hard to work through. Similarly to trying to explain a change by
looking at the diff. 

Instead, I'm structuring the code so we are looking at the complete solution, but I've broken apart the implementation so it is composed of individual functions. I'm only going
to be discussing the relevant functions at any given point in time. If I'm not calling out a particular function, you can assume that it has no impact on the functionality we
discussed and ignore it at this point. I'll come back later and explain it in detail.
====

This is where we start to put together the skeleton of Gavran. Let's take a look at the first stage, creating a database, shown in <<create_db>>.

[source]
[[create_db]]
.`db.c` - Creating an database instance
----
include::./code/db.c[tags=db_create]
----

The code in <<create_db>> is _dense_, I'm afraid. There is quite a lot of stuff going here that would be discussed in future chapters, for now, let's talk about the parts that 
we are most interested on. We allocate the state for the database, create the file and set its minimum size, note that we _aren't_ setting the maximum size. If we were, and the 
file was larger than the maximum size, we may discard parts of the data file. There is a lot of other things that are going here, setting up the database, creating default read 
transaction, various options, etc. I'm ignoring all of these at this point.

I'm sorry for the code dump, but I assume that it is best to show everything as a single unit rather than tease and reveal them one piece at a time. 
Let's look at the actual implementation details. We'll start by looking at the `db_state_t` and `txn_state_t` structs that we are using in <<create_db>>.
There are quite a lot of fields there that we'll only need in the future. You can see the full details in <<state_structs>>.

At this point, we can ignore all but the first three fields on each of the structs in <<state_structs>>. The `db_state_t` contains the options for the database (currently we only
care for the `minimum_size` field), the file handle the the mapped memory in the `global_state`. The `txn_state_t` also has a copy of the `global_state` (and thus, the mapping). 

[source]
[[state_structs]]
.`gavran/db.h` - Internal state structures
----
include::../include/gavran/db.h[tags=db_state_t]

include::../include/gavran/db.h[tags=txn_state_t]
----

In <<create_db>> we call to `db_validate_options()` to validate the options and copy the user defined values to our own copy. I'm showing the code in <<db_validate_options>>,
note that this is a simplified version of what we'll have in the future. We'll update this function in the future. 
You can see in <<db_validate_options>> that the code validates the provided settings and copy them to our own `db_options_t` struct. 

The user can also just pass null for the `options` and we'll use the default ones. In <<db_validate_options>> you can also see that I'm using `implementation_detail` macro. This 
is just to let the compiler know that this is a hidden function which shouldn't be exported from the library.
<<db_validate_options>> also shows the default options initialization. Right now we don't have much there, but it will grow over time.

After handling the `options`, <<create_db>> then uses our PAL interface to create the file, ensure that it is a match to the minimum size we require and map it to memory. You'll note
that there is actually quite a few resources that are involved in the process. The use of `ensure` and `try_defer` allows us to dramatically reduce the amount of error handing code that
we would other need to manage. We actually have multiple `try_defer` calls and we use `done` to indicate that the function completed successfully and we should _not_ clean up those
resources. Otherwise, everything gets cleaned up. The `try_defer` calls are guaranteed to be called in reversed order of declaration, so we can be sure of the destruction order.

[source]
[[db_validate_options]]
.`db..c` - Validating the database's options and default options initialization
----
include::./code/db..c[tags=db_validate_options]

include::./code/db..c[tags=db_initialize_default_options]
----

In the next chapter, we are going to focus on `db_init()` and do a lot of work there, but for now, I want to focus on transactions.

=== Implementing transactions

You already saw the raw structs to create a transaction in <<create_db>>, now we want to actually build real transactions API. Before we can do that, I want to go over what 
transactions _are_. A transaction in the context of Gavran is a set of changes that are applied to the database. Until they are applied, they are not visible to any 
other transactions.

Before we dive into the details of how a transaction is implemented in code, I want to talk about what we are trying to _do_. The transactions that we'll build in this
chapter is mostly concerned with managing a list of modified pages. The idea is that you'll always go through the transaction to modify pages, it will manage them for
you and eventually commit them. The overall structure can be seen in <<tx-modified-pages>>. 

Using the Copy On Write mode tha we discussed earlier, whenever we want to modify a page, we'll copy it to the side and make the modifications. Once all the changes 
in the transaction are completed, we'll persist them all at once. Consider the implications of such a system. How would a rollback look in this case? 
Simply free the copies, no one else had any access to them, so nothing needs to be done. 

[[tx-modified-pages]]
.The layout of a transaction with 2 modified pages
image::tx-modified-pages.png[]


As for commit, once all the changes has been written to the file, we are done. We saw in the previous chapter that it takes significant effort to make sure that a
change to the file metadata is persisted. Changes to the file's data are by no means easier. We are going to deal with a single facet of the ACID properties in
this chapter, and atomic commit. 

<<txn_create>> shows how we create a transaction. A `txn_t` is mostly just a pointer to `txn_state_t`, which holds the details about the transaction as well as the hash table to hold
the modified pages. This is initialized by the `hash_new()` call. You'll note that we are passing a `flags` parameter to the `txn_create()`, but not doing much of it. I'm currently 
ignoring that but we'll use that to set various behaviors on the transaction, such as whatever it is read only or allowed to write.

One of the key responsibilities of the transaction is managing the work of `txn_modify_page`, to handle that properly, we need to store the modified
pages in the transaction and be able to access them quickly.  Listing 4.7 has the details on how we manage that.

[source]
[[txn_create]]
.`txn..c` - Creating a transaction
----
include::./code/txn..c[tags=txn_create]
----

<<txn_create>> shows the setup of a new transaction, but it probably wouldn't make sense without looking at the most common actions that we'll have in a transaction `txn_raw_get_page()` 
and `txn_raw_modify_page()`. Let's look at them first now.
When using a transaction, `txn_raw_get_page()` is initially very similar to using `pages_get()`, so why do we have two layers of the API? The key difference is that `pages_get()` has 
global scope, while `txn_raw_get_page()` is scoped to a transaction. Look back at <<tx-modified-pages>> and consider how that works. When we are calling `txn_raw_get_page()`, we first
need to check if this transaction has a modified copy of the page in question and only if it does will we go to the data file. You can see how that looks like in <<txn_raw_get_page>>.

[source]
[[txn_raw_get_page]]
.`txn..c` - Get a page in the scope of the transaction, either locally modified copy or directly from the data file.
----
include::./code/txn..c[tags=txn_raw_get_page]
----

.A note about multi threading
****
We don't have any at this point, we'll deal with them at a much later part of this book. Right now, the code isn't going to be able to handle multiple threads.
The atomic property that we'll have is about sequential or interleaved transactions. Gavran _will_ have all of the ACID properties, including safe usage under multi threaded code, 
mind you. I'm just building things one stage at a time, otherwise we'll drown in too much complexity, all at once.
****

With `txn_raw_get_page()` our of the way, let's look at how we are going to implement `txn_raw_modify_page()`. We are going to use a structure very similar to <<create_and_write_file>>.
The fact that we are never modifying the page directly is of enormous importance to the design of Gavran. This strategy is called Copy On Write (COW) and it will serve as the basis for
implementing one of the core ACID properties, Isolation. 

In <<txn_raw_modify_page>> `txn_raw_modify_page()` will first look in the `modified_pages` of the transaction and if we haven't already modified the page in question, we'll get it from
the data file. I'm building `txn_raw_modify_page()` on top of `txn_raw_get_page()` so it will benefit from any changes that we'll make to the `txn_raw_get_page()` in the future. Once we
have the original page, we allocate a buffer, copy the memory to the new location and register the new page in the transaction's `modified_pages`.

IN <<txn_raw_modify_page>> we are also setting up the modified page with a pointer to the _old_ version of the data. In other words, we are able to look into what this page used to be
like, before any modifications. This will become very important when the times come to do proper committing of the transaction's data.

[source]
[[txn_raw_modify_page]]
.`txn..c` - Implementing Copy on Write in the transaction for page modifications.
----
include::./code/txn..c[tags=txn_raw_modify_page]
----

It is important to note that calling `txn_raw_get_page()`, `txn_raw_modify_page()` and then `txn_raw_get_page()` on the same page is something
that you need to be aware of. The first and last `txn_raw_get_page` will return _different_ values. I haven't found this a real issue in practice, but something
that you might want to keep in mind.

=== Managing the transaction state with a hash table

The most important piece in `txn_raw_modify_page()` is the management of the `modified_pages` on the transaction. This is a pointer to `pages_hash_table_t`, which is a hash table for 
the modified pages in the transaction. 
I have to admit, given that I write databases for a living, I was highly amused by how reluctant I was to implement a hash table. I have written
a few, so it isn't a new task. I'm also doing that while implementing a _storage engine_, which is closely related. 

The good news here is that this hash table only need to support `hash_put_new` and `hash_lookup`, we don't need to handle deletions at all. That makes the overall complexity much easier.

Given that the readers of this book are likely interested, I'm going to go for an open addressing with linear probing model. And one of the 
key features that I care about is avoiding the usual pointer caching that you'll typically see in hash tables. Data locality is a huge performance boost and I want some of that. This 
is not meant to be a generic hash table, I created it specifically to hold `page_t` values indexed by their `page_num`. 
Since `page_t` already contain the key, I can simplify my implementation significantly and the cost of lookup is likely very small. That might seems strange if you are coming from a language 
where you'll have more separation between the data containers and the values. 

`Dictionary<K,V>` is a prime example of that. You have the container data and the data held by the container and they can live very far apart. That is not the case here, we always keep them
 very close by. You can see how the `pages_hash_table_t` is defined in <<pages_hash_table_t>>

[source]
[[pages_hash_table_t]]
.`gavran/internal.h` - The hash table definition
----
include::../include/gavran/internal.h[tags=pages_hash_table_t]
----

I'm going to skip explaining https://en.wikipedia.org/wiki/Hash_table[how hash tables work], I assume that you either have a grasp on that
or is able to learn that from other sources. I'll mention that I'm using a hash table implementation strategy called 
https://en.wikipedia.org/wiki/Linear_probing[linear probing]. This refers to how the hash table handles collisions. Instead of creating a 
linked list of values on hash collision, we'll move to the next location in the array. That means that we are going to have good locality
of reference when doing the search, the entire hash table is typically a single allocation and the `hash_lookup` and `hash_put_new` implementations
are straightforward.

[TIP]
.Recommended reading
====
If you want to really dig down into hash table, you might want to read https://www.amazon.com/dp/B07NKYSSR1/[The Joys of Hashing] which does a great job of covering most aspects
of hash tables design and implementation with very clear code and the background details that explain the _why_ of certain decisions.

You may also want to read http://craftinginterpreters.com/hash-tables.html[the excellent hash table chapter] in Crafting Interpreters, which is much shorter, but does a great job
explaining how hash tables work.
====

The downside of linear probing and open addressing in general is that the deletes tend to be more complex to implement than an 
implementation that uses chaining. The good news is that we don't need to worry about that here, so the complexity is greatly reduced.

There are a few other things to consider for this case:

* The key for the hash is the page number, these are `uint64_t` values and it is very likely that you'll get consecutive values.
  As such, we don't try to be fancy with the hash function, we simply modulus the page number with the number of buckets in the 
  hash table and start the search there.
* The rule on collision goes like this. Whenever a put is attempted that collided, we'll move to the next available spot that 
  is free. In other words, the worst case scenario for this sort of hash table is having to scan through all the buckets. 
* This is where the load factor comes into play. Given that the page numbers are likely to cluster around specific values, the
  empty buckets that remain act as backstop to avoid iterating through all the buckets. 

In <<hash_new_and_lookup>> you can see how we create a new hash table using `hash_new()` and looking up a single value using `hash_lookup()`. The
transaction's `modified_pages` field is setup via `hash_new()` in `txn_create()`. We are going to be using this hash table a _lot_. It is going 
to serve as one of the most fundamental pieces in the transaction's implementation. 

One thing to note about the implementation here, I have chosen to favor readability over performance. There are several low hanging
fruits that we can utilize here that will likely help performance to some degree. Using shifts instead of modulus (since our `number_of_entries`
can be assumed to be a power of two) is a simple one. We might get to that when we get to the benchmarks portion of this book, for now, simpler
code is the key.

[source]
[[hash_new_and_lookup]]
.`hashtable.h` - Creating a hash table and looking up a single value
----
include::./code/hashtable.c[tags=hash_new_and_lookup]
----

In <<hash_new_and_lookup>> you can see that creating a new hash table is simple, we allocate a single buffer, for the `pages_hash_table_t` as well
as the `entries` immediately following it. That is why we have an "empty" array at the end of `pages_hash_table_t`, that indicates to the compiler
that the value of the field is the area immediately following the struct. 

For lookups, we simply scan through the `entries` array, the starting location depends on the page that we are looking for. But if we found a value
that isn't the page we are looking for, we know that we need to continue scanning. This is what open address means, that the value can be (because
of collisions) further away from where it is meant to be. 

This might make more sense when we'll look into how we are going to implement _putting_ a value into the hash table.
This is the more complex operation because we may need to resize the hash table if the load factor grows big enough. The details are in <<hash_put_new>>.

<<hash_put_new>> starts by expanding the hash table if needed, then scans through the table for an empty location that we can place the new value in,
starting with the `page_num` modulus the size of the table. It is an error to add a value that already exists in the table. When we add a value, we 
check if the _next_ call to `hash_put_new()` should expand the table. This is done by checking if we are over 75% full. As mentioned, I'm not going to 
explain how hash tables work, if you have questions on the implementation, there are more than enough resources about hash tables available.

[source]
[[hash_put_new]]
.`hashtable.h` - Put a new value inside the hash table
----
include::./code/hashtable.c[tags=hash_put_new]
----

Let's look how expanding the table works. We are going to use simple doubling of the hash table whenever we get to 75% load factor. In other words, if we
start with 8 entries by default, we'll increase to 16 when we get to the 7th entry, to 32 when we get to 13, and 64 when we get to 25.
Note that since we are actually using `page_t` as our entries, we have to take into account it is somewhat large, on 64 bits, it is going to 
be 32 bytes.

<<hash_expand_table>> shows how we double the size of the table. We allocate a new buffer, twice as large, iterate over the old hash table using `hash_get_next()`
and add to the new hash table using `hash_put_new()`, pretty simple, overall.
We are using `mem_calloc()` to allocate the memory, but couldn't we use `mem_realloc()` instead, to reallocate the buffer without 
needing to copy the data gain? The answer is no. Part of the expansion of the hash table means that we need to hash the entries _again_, placing them in 
new locations, based on the new size.

[source]
[[hash_expand_table]]
.`hashtable.h` - Doubling the size of the hash table
----
include::./code/hashtable.c[tags=hash_expand_table]
----

There are a few interesting implications to the way we built `hash_expand_table()`. First, any `hash_put_new()` may cause the table to shift, that is why 
we send an indirect pointer to the function. 
You cannot mix `hash_get_next()` and `hash_put_new()`, iteration and mutations generally don't mix with one another.

Finally, let's look at `hash_get_next()`, shown in <<hash_get_next>>.
The implementation simple scans through the table, but we use the externally provided `state` variable to hold our state between calls.
We expect `hash_get_next()` to be used as it is used in <<hash_expand_table>>. Initialize the `iter_state` value to `0` and call `hash_get_next()` until it returns `false`. 

The hash table implementation we have here is a pretty basic one, intentionally. You can see more options on how I could have implemented it in 
https://www.amazon.com/dp/20/B07NKYSSR1[The Joys of Hashing], but the idea is to get this working properly and fix any performance issues later. That said, I don't expect to
have too many problems around the hash table implementation, but I was surprised in the past... 

[source]
[[hash_get_next]]
.`hashtable.h` - Iterating over the hash table
----
include::./code/hashtable.c[tags=hash_get_next]
----

.Memory management in Linux
****
By default, Linux will never fail a `malloc()`. In other words, when calling `malloc()`, you'll never get `NULL` back. If you care to know more about 
this behavior, search for https://lwn.net/Articles/317814/[over commit and the OOM killer]. A good paper on why over commit is required is 
https://www.microsoft.com/en-us/research/publication/a-fork-in-the-road/["A fork() in the road"]. 

In essence, too much software simply assume that all memory allocations will succeed. Even the Linux kernel has this an issue with
https://lwn.net/Articles/627419/[this kind of expectation]. 

The problem with not failing `malloc()` is that Linux cannot just magic some more RAM, and even with swap, you'll eventually run out. The issue
is how it _handles_ this scenario. And the way Linux work is that when you use too much memory, Linux will select a victim process and _kill_ it
to reutilize the memory it uses. The situation is worse because the triggering event that would cause the OOM killer to run and select a victim
to kill can be a totally unrelated process.

That has serious implications for people building robust software. I can't catch a `SIGKILL` and ignore it, after all. But I could change
my system behavior to handle memory allocation failure. There are configuration flag that you can use to avoid this behavior, but they 
have their own side effects and they are global. In other words, they impact the whole system. If you are running a single process, or
as we are doing now, a library, that isn't going to be useful for you.

The code I'm writing here assume that it is possible to get memory allocation failures and handle them appropriately. We are currently
allocating directly from the system, but that is likely to change as the implementation proceeds. We'll start managing the memory much
more closely and will be able to react properly to low memory events.
****

=== Committing and closing the transaction

Now that we have a way to keep track of the modified pages in the transaction, the next stage is to figure out how to commit those changes to disk. It turns out 
that most of the work was already done for us. Listing 4.11 shows how to "commit" a transaction.
I'm using commit in quotes here because this is not respecting any of the transaction properties, but we are laying down the foundation for actual transactions.

There really isn't much to do in <<txn_commit>>, to be honest. We scan through the table of modified pages and write them to the file using the `pal_write_file()` function
we looked at in the previous chapter. If there is an error, we return, and that is about it. Note that we don't have any attempt to for durability, atomicity, etc. 

After calling `txn_commit()`, we need to call `txn_close()` to cleanup all the resources for the transaction. If we want to rollback the transaction, we can call `txn_close()`
without calling `txn_commit()`. 

[source]
[[txn_commit]]
.`txn..c` - Implementing commit transaction by writing modified pages to disk and closing it by freeing the memory of the transaction
----
include::./code/txn..c[tags=txn_commit]

include::./code/txn..c[tags=txn_close]
----

A more sophisticated system will allow us to reuse the allocated memory from one transaction to the next, but I'm reserving those kind of behavior for the
future, when I can see benchmark results. Get it working, get it working _right_ and only then get it working fast.

=== Using the transaction API

The last thing that we'll do in this chapter is put everything together. <<create_and_write_with_tx_api>> shows how we can now write <<create_and_write_file>> using the new API.

[source]
[[create_and_write_with_tx_api]]
.`test.c` - Reading and writing using the transaction API
----
include::./code/test.c[tags=create_and_write_with_tx_api]
----

In <<create_and_write_with_tx_api>>, you'll note that we are using multiple transactions. One to do the writes and one for the reads. We actually got a _very_ limited form
of isolation. Until the `write_tx` is committed, the transactions created in `validate_message()` not see any of its changes and it is committed. I wrote it in this manner to show
how we are starting to get real features. 

The reason the code in <<create_and_write_with_tx_api>> is able to create isolation between transactions is that the `write_tx` doesn't modify the file directly, it modifies a 
_copy_ of the page and the act of committing the transaction make it apparent to the next transaction.

You can also see that I'm passing `TX_WRITE` and `TX_READ` flags to the `txn_create` function. At the moment, it does nothing, but it will become important down the line.

=== Unit tests

Finally, before we close this chapter, we need to make sure that the software _works_. 
Our next topic is going to be... managing the pages and disk space.

[source,c]
[[tests_ch04]]
.`test.c` - Testing the transactions API and copy of write
----
include::./code/test.c[tags=tests]
----
